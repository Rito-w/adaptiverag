#!/usr/bin/env python3
"""
=== ä½¿ç”¨ FlashRAG ä¸‹è½½çœŸå®æ•°æ®é›† ===

ä¸‹è½½æ ‡å‡†çš„ QA æ•°æ®é›†ç”¨äº AdaptiveRAG å®éªŒ
"""

import os
import sys
import logging
from pathlib import Path
import json

# æ·»åŠ  FlashRAG è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "FlashRAG"))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def download_flashrag_datasets():
    """ä½¿ç”¨ FlashRAG ä¸‹è½½æ ‡å‡†æ•°æ®é›†"""
    try:
        from flashrag.config import Config
        from flashrag.dataset import Dataset
        import datasets
        
        logger.info("ğŸš€ å¼€å§‹ä¸‹è½½ FlashRAG æ ‡å‡†æ•°æ®é›†")
        
        # æ•°æ®ä¿å­˜ç›®å½•
        data_dir = Path("/root/autodl-tmp/flashrag_real_data")
        data_dir.mkdir(exist_ok=True)
        
        # è¦ä¸‹è½½çš„æ•°æ®é›†ï¼ˆä½¿ç”¨æ­£ç¡®çš„åç§°ï¼‰
        dataset_configs = {
            "hotpotqa": {
                "name": "hotpotqa",
                "hf_name": "RUC-NLPIR/FlashRAG_datasets",
                "subset": "hotpotqa"
            },
            "triviaqa": {
                "name": "triviaqa",
                "hf_name": "RUC-NLPIR/FlashRAG_datasets",
                "subset": "triviaqa"
            },
            "msmarco-qa": {
                "name": "msmarco-qa",
                "hf_name": "RUC-NLPIR/FlashRAG_datasets",
                "subset": "msmarco-qa"
            },
            "squad": {
                "name": "squad",
                "hf_name": "RUC-NLPIR/FlashRAG_datasets",
                "subset": "squad"
            }
        }
        
        for dataset_key, config in dataset_configs.items():
            logger.info(f"ğŸ“Š ä¸‹è½½æ•°æ®é›†: {config['name']}")
            
            try:
                # ä¸‹è½½æ•°æ®é›†
                dataset = datasets.load_dataset(
                    config["hf_name"], 
                    config["subset"],
                    cache_dir=str(data_dir / "cache")
                )
                
                # ä¿å­˜ä¸º JSONL æ ¼å¼
                for split in ["train", "dev", "test"]:
                    if split in dataset:
                        output_file = data_dir / f"{config['name']}_{split}.jsonl"

                        logger.info(f"ğŸ“ å¤„ç† {split} æ•°æ®...")
                        items_written = 0

                        with open(output_file, 'w', encoding='utf-8') as f:
                            for i, item in enumerate(dataset[split]):
                                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                                standard_item = {
                                    "id": item.get("id", f"{config['name']}_{split}_{i}"),
                                    "question": item.get("question", ""),
                                    "golden_answers": item.get("golden_answers", []),
                                    "metadata": {
                                        "dataset": config["name"],
                                        "split": split
                                    }
                                }
                                f.write(json.dumps(standard_item, ensure_ascii=False) + '\n')
                                items_written += 1

                        logger.info(f"âœ… ä¿å­˜ {items_written} æ¡ {split} æ•°æ®åˆ°: {output_file}")
                        
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½ {config['name']} å¤±è´¥: {e}")
                continue
        
        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
        return str(data_dir)
        
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥ FlashRAG å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None

def download_huggingface_datasets():
    """ç›´æ¥ä» Hugging Face ä¸‹è½½æ ‡å‡†æ•°æ®é›†"""
    try:
        import datasets
        
        logger.info("ğŸš€ ä» Hugging Face ä¸‹è½½æ ‡å‡†æ•°æ®é›†")
        
        # æ•°æ®ä¿å­˜ç›®å½•
        data_dir = Path("/root/autodl-tmp/hf_real_data")
        data_dir.mkdir(exist_ok=True)
        
        # æ ‡å‡†æ•°æ®é›†é…ç½®ï¼ˆé¿å…å¤§æ•°æ®é›†ï¼‰
        hf_datasets = {
            "squad": {
                "path": "squad",
                "name": None
            },
            "squad_v2": {
                "path": "squad_v2",
                "name": None
            }
        }
        
        for dataset_name, config in hf_datasets.items():
            logger.info(f"ğŸ“Š ä¸‹è½½æ•°æ®é›†: {dataset_name}")
            
            try:
                # ä¸‹è½½æ•°æ®é›†
                if config["name"]:
                    dataset = datasets.load_dataset(
                        config["path"], 
                        config["name"],
                        cache_dir=str(data_dir / "cache")
                    )
                else:
                    dataset = datasets.load_dataset(
                        config["path"],
                        cache_dir=str(data_dir / "cache")
                    )
                
                # ä¿å­˜ä¸º JSONL æ ¼å¼
                for split_name, split_data in dataset.items():
                    output_file = data_dir / f"{dataset_name}_{split_name}.jsonl"
                    
                    with open(output_file, 'w', encoding='utf-8') as f:
                        for i, item in enumerate(split_data):
                            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                            if dataset_name == "natural_questions":
                                standard_item = {
                                    "id": item.get("id", f"nq_{split_name}_{i}"),
                                    "question": item.get("question", {}).get("text", ""),
                                    "golden_answers": [ans["text"] for ans in item.get("annotations", {}).get("short_answers", [])],
                                    "metadata": {
                                        "dataset": dataset_name,
                                        "split": split_name
                                    }
                                }
                            elif dataset_name == "squad":
                                standard_item = {
                                    "id": item.get("id", f"squad_{split_name}_{i}"),
                                    "question": item.get("question", ""),
                                    "golden_answers": [ans["text"] for ans in item.get("answers", {}).get("text", [])],
                                    "metadata": {
                                        "dataset": dataset_name,
                                        "split": split_name,
                                        "context": item.get("context", "")
                                    }
                                }
                            else:
                                # é€šç”¨æ ¼å¼
                                standard_item = {
                                    "id": f"{dataset_name}_{split_name}_{i}",
                                    "question": str(item.get("query", item.get("question", ""))),
                                    "golden_answers": item.get("answers", []),
                                    "metadata": {
                                        "dataset": dataset_name,
                                        "split": split_name,
                                        "original_item": item
                                    }
                                }
                            
                            f.write(json.dumps(standard_item, ensure_ascii=False) + '\n')
                    
                    logger.info(f"âœ… ä¿å­˜ {split_name} æ•°æ®åˆ°: {output_file}")
                    
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½ {dataset_name} å¤±è´¥: {e}")
                continue
        
        logger.info("ğŸ‰ Hugging Face æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
        return str(data_dir)
        
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥ datasets åº“å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹ä¸‹è½½çœŸå®æ•°æ®é›†")

    # å¯ç”¨å­¦æœ¯åŠ é€Ÿ
    logger.info("ğŸš€ å¯ç”¨å­¦æœ¯åŠ é€Ÿ...")
    os.system("source /etc/network_turbo")

    # é¦–å…ˆå°è¯• FlashRAG æ•°æ®é›†
    flashrag_path = download_flashrag_datasets()

    # ç„¶åå°è¯• Hugging Face æ•°æ®é›†
    hf_path = download_huggingface_datasets()
    
    if flashrag_path or hf_path:
        logger.info("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ!")
        if flashrag_path:
            logger.info(f"ğŸ“ FlashRAG æ•°æ®è·¯å¾„: {flashrag_path}")
        if hf_path:
            logger.info(f"ğŸ“ Hugging Face æ•°æ®è·¯å¾„: {hf_path}")
    else:
        logger.error("âŒ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½éƒ½å¤±è´¥äº†")

if __name__ == "__main__":
    main()
