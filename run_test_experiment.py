#!/usr/bin/env python3
"""
=== AdaptiveRAG æµ‹è¯•å®éªŒ ===

åœ¨å°å‹ HotpotQA æ•°æ®é›†ä¸Šæµ‹è¯• AdaptiveRAG çš„æ€§èƒ½
"""

import sys
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    test_file = Path("/root/autodl-tmp/test_data/hotpotqa_test.jsonl")
    
    if not test_file.exists():
        logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return []
    
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            test_data.append(json.loads(line.strip()))
    
    logger.info(f"ğŸ“Š åŠ è½½äº† {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
    return test_data

def load_corpus():
    """åŠ è½½è¯­æ–™åº“"""
    corpus_file = Path("/root/autodl-tmp/test_data/test_corpus.jsonl")
    
    if not corpus_file.exists():
        logger.error(f"è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_file}")
        return []
    
    corpus = []
    with open(corpus_file, 'r', encoding='utf-8') as f:
        for line in f:
            corpus.append(json.loads(line.strip()))
    
    logger.info(f"ğŸ“š åŠ è½½äº† {len(corpus)} æ¡è¯­æ–™åº“æ–‡æ¡£")
    return corpus

def simple_keyword_retrieval(query: str, corpus: List[Dict], top_k: int = 3) -> List[Dict]:
    """ç®€å•çš„å…³é”®è¯æ£€ç´¢"""
    query_words = set(query.lower().split())
    
    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°
    scored_docs = []
    for doc in corpus:
        content = (doc.get('title', '') + ' ' + doc.get('contents', '')).lower()
        content_words = set(content.split())
        
        # ç®€å•çš„è¯é‡å åˆ†æ•°
        overlap = len(query_words.intersection(content_words))
        if overlap > 0:
            scored_docs.append((overlap, doc))
    
    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    return [doc for _, doc in scored_docs[:top_k]]

def simple_rag_baseline(question: str, corpus: List[Dict]) -> Dict[str, Any]:
    """ç®€å•çš„ RAG åŸºçº¿æ–¹æ³•"""
    start_time = time.time()
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = simple_keyword_retrieval(question, corpus, top_k=3)
    retrieval_time = time.time() - start_time
    
    # ç®€å•çš„ç­”æ¡ˆç”Ÿæˆï¼ˆåŸºäºè§„åˆ™ï¼‰
    gen_start = time.time()
    
    # åˆå¹¶æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
    context = " ".join([doc.get('contents', '') for doc in retrieved_docs])
    
    # ç®€å•çš„ç­”æ¡ˆæå–é€»è¾‘
    answer = "Based on the retrieved information: " + context[:100] + "..."
    
    generation_time = time.time() - gen_start
    total_time = time.time() - start_time
    
    return {
        "answer": answer,
        "retrieved_docs": retrieved_docs,
        "retrieval_time": retrieval_time,
        "generation_time": generation_time,
        "total_time": total_time,
        "method": "simple_rag"
    }

def adaptive_rag_method(question: str, corpus: List[Dict]) -> Dict[str, Any]:
    """AdaptiveRAG æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    start_time = time.time()
    
    # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢åˆ†æ
    analysis_start = time.time()
    
    # ç®€å•çš„æŸ¥è¯¢å¤æ‚åº¦åˆ†æ
    query_words = question.lower().split()
    complexity_score = min(len(query_words) / 10.0, 1.0)  # åŸºäºé•¿åº¦çš„å¤æ‚åº¦
    
    # æ£€æµ‹æ˜¯å¦æ˜¯å¤šè·³æŸ¥è¯¢
    multi_hop_indicators = ['where', 'who', 'what', 'when', 'which', 'author', 'creator', 'founder']
    is_multi_hop = any(indicator in question.lower() for indicator in multi_hop_indicators)
    
    analysis_time = time.time() - analysis_start
    
    # ç¬¬äºŒæ­¥ï¼šè‡ªé€‚åº”æ£€ç´¢ç­–ç•¥
    retrieval_start = time.time()
    
    if is_multi_hop and complexity_score > 0.5:
        # å¤æ‚æŸ¥è¯¢ï¼šä½¿ç”¨æ›´å¤šæ–‡æ¡£
        top_k = 5
        logger.info(f"ğŸ§  æ£€æµ‹åˆ°å¤æ‚å¤šè·³æŸ¥è¯¢ï¼Œä½¿ç”¨æ‰©å±•æ£€ç´¢ (top_k={top_k})")
    else:
        # ç®€å•æŸ¥è¯¢ï¼šä½¿ç”¨è¾ƒå°‘æ–‡æ¡£
        top_k = 3
        logger.info(f"ğŸ” æ£€æµ‹åˆ°ç®€å•æŸ¥è¯¢ï¼Œä½¿ç”¨æ ‡å‡†æ£€ç´¢ (top_k={top_k})")
    
    retrieved_docs = simple_keyword_retrieval(question, corpus, top_k=top_k)
    retrieval_time = time.time() - retrieval_start
    
    # ç¬¬ä¸‰æ­¥ï¼šè‡ªé€‚åº”ç”Ÿæˆ
    gen_start = time.time()
    
    # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´ç”Ÿæˆç­–ç•¥
    context = " ".join([doc.get('contents', '') for doc in retrieved_docs])
    
    if is_multi_hop:
        answer = f"Multi-hop reasoning result: {context[:150]}..."
    else:
        answer = f"Direct answer: {context[:100]}..."
    
    generation_time = time.time() - gen_start
    total_time = time.time() - start_time
    
    return {
        "answer": answer,
        "retrieved_docs": retrieved_docs,
        "query_analysis": {
            "complexity_score": complexity_score,
            "is_multi_hop": is_multi_hop,
            "analysis_time": analysis_time
        },
        "retrieval_time": retrieval_time,
        "generation_time": generation_time,
        "total_time": total_time,
        "method": "adaptive_rag"
    }

def evaluate_answer(predicted: str, golden_answers: List[str]) -> Dict[str, float]:
    """ç®€å•çš„ç­”æ¡ˆè¯„ä¼°"""
    predicted_lower = predicted.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æ­£ç¡®ç­”æ¡ˆ
    exact_match = 0.0
    for golden in golden_answers:
        if golden.lower() in predicted_lower:
            exact_match = 1.0
            break
    
    # ç®€å•çš„F1è®¡ç®—ï¼ˆåŸºäºè¯é‡å ï¼‰
    pred_words = set(predicted_lower.split())
    best_f1 = 0.0
    
    for golden in golden_answers:
        golden_words = set(golden.lower().split())
        if len(golden_words) == 0:
            continue
            
        overlap = len(pred_words.intersection(golden_words))
        precision = overlap / len(pred_words) if len(pred_words) > 0 else 0.0
        recall = overlap / len(golden_words) if len(golden_words) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        best_f1 = max(best_f1, f1)
    
    return {
        "exact_match": exact_match,
        "f1_score": best_f1
    }

def run_experiment():
    """è¿è¡Œå®éªŒ"""
    logger.info("ğŸš€ å¼€å§‹ AdaptiveRAG æµ‹è¯•å®éªŒ")
    
    # åŠ è½½æ•°æ®
    test_data = load_test_data()
    corpus = load_corpus()
    
    if not test_data or not corpus:
        logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # å®éªŒç»“æœ
    results = {
        "simple_rag": {"predictions": [], "metrics": []},
        "adaptive_rag": {"predictions": [], "metrics": []}
    }
    
    # å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬è¿è¡Œä¸¤ç§æ–¹æ³•
    for i, item in enumerate(test_data):
        question = item["question"]
        golden_answers = item["golden_answers"]
        
        logger.info(f"\nğŸ“ æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_data)}: {question}")
        
        # ç®€å•RAGåŸºçº¿
        logger.info("ğŸ” è¿è¡Œç®€å•RAGåŸºçº¿...")
        simple_result = simple_rag_baseline(question, corpus)
        simple_metrics = evaluate_answer(simple_result["answer"], golden_answers)
        
        results["simple_rag"]["predictions"].append(simple_result)
        results["simple_rag"]["metrics"].append(simple_metrics)
        
        logger.info(f"   ğŸ“Š ç®€å•RAG - EM: {simple_metrics['exact_match']:.3f}, F1: {simple_metrics['f1_score']:.3f}")
        
        # AdaptiveRAG
        logger.info("ğŸ§  è¿è¡ŒAdaptiveRAG...")
        adaptive_result = adaptive_rag_method(question, corpus)
        adaptive_metrics = evaluate_answer(adaptive_result["answer"], golden_answers)
        
        results["adaptive_rag"]["predictions"].append(adaptive_result)
        results["adaptive_rag"]["metrics"].append(adaptive_metrics)
        
        logger.info(f"   ğŸ“Š AdaptiveRAG - EM: {adaptive_metrics['exact_match']:.3f}, F1: {adaptive_metrics['f1_score']:.3f}")
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    logger.info("\nğŸ“ˆ å®éªŒç»“æœæ±‡æ€»:")
    
    for method_name, method_results in results.items():
        metrics = method_results["metrics"]
        avg_em = sum(m["exact_match"] for m in metrics) / len(metrics)
        avg_f1 = sum(m["f1_score"] for m in metrics) / len(metrics)
        avg_time = sum(p["total_time"] for p in method_results["predictions"]) / len(method_results["predictions"])
        
        logger.info(f"\nğŸ¯ {method_name.upper()}:")
        logger.info(f"   ğŸ“Š å¹³å‡ EM: {avg_em:.3f}")
        logger.info(f"   ğŸ“Š å¹³å‡ F1: {avg_f1:.3f}")
        logger.info(f"   â±ï¸  å¹³å‡æ—¶é—´: {avg_time:.3f}s")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("/root/autodl-tmp/test_results")
    output_dir.mkdir(exist_ok=True)
    
    with open(output_dir / "experiment_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ… å®éªŒå®Œæˆï¼ç»“æœä¿å­˜åˆ°: {output_dir / 'experiment_results.json'}")

def main():
    """ä¸»å‡½æ•°"""
    run_experiment()

if __name__ == "__main__":
    main()
