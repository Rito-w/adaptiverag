# ğŸ“ AdaptiveRAG Project Structure

## ğŸ—ï¸ Directory Overview

```
adaptiverag/
â”œâ”€â”€ ğŸ“„ README.md                     # Project overview and quick start
â”œâ”€â”€ ğŸ“„ LICENSE                       # MIT License
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md               # Contribution guidelines
â”œâ”€â”€ ğŸ“„ CHANGELOG.md                  # Version history and changes
â”œâ”€â”€ ğŸ“„ setup.py                      # Package installation configuration
â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                    # Git ignore patterns
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md          # This file
â”œâ”€â”€ ğŸ“„ EXPERIMENT_FRAMEWORK_SUMMARY.md # Experimental framework summary
â”‚
â”œâ”€â”€ ğŸ§  adaptive_rag/                 # Core AdaptiveRAG implementation
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ config.py                 # Configuration system
â”‚   â”œâ”€â”€ ğŸ“ core/                     # Core components
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ flexrag_integrated_assistant.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ task_decomposer.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ retrieval_planner.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ context_reranker.py
â”‚   â”œâ”€â”€ ğŸ“ modules/                  # Individual modules
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ query_analyzer.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ multi_retriever.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ adaptive_generator.py
â”‚   â”œâ”€â”€ ğŸ“ pipeline/                 # Pipeline implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ adaptive_pipeline.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ evaluation_pipeline.py
â”‚   â”œâ”€â”€ ğŸ“ evaluation/               # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ benchmark_runner.py   # Main benchmark runner
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset_downloader.py # Dataset management
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ baseline_methods.py   # Baseline implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ablation_analyzer.py  # Ablation study tools
â”‚   â”‚   â””â”€â”€ ğŸ“„ result_analyzer.py    # Result analysis
â”‚   â”œâ”€â”€ ğŸ“ webui/                    # Web interface
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ app.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ components.py
â”‚   â””â”€â”€ ğŸ“ utils/                    # Utility functions
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ logging_utils.py
â”‚       â””â”€â”€ ğŸ“„ data_utils.py
â”‚
â”œâ”€â”€ ğŸ§ª tests/                        # Test suite
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ test_config.py            # Configuration tests
â”‚   â”œâ”€â”€ ğŸ“„ test_core.py              # Core component tests
â”‚   â”œâ”€â”€ ğŸ“„ test_evaluation.py        # Evaluation framework tests
â”‚   â””â”€â”€ ğŸ“„ test_integration.py       # Integration tests
â”‚
â”œâ”€â”€ ğŸ“Š experiments/                  # Experiment results
â”‚   â”œâ”€â”€ ğŸ“„ .gitkeep                  # Keep directory in git
â”‚   â”œâ”€â”€ ğŸ“ quick_test/               # Quick test results
â”‚   â”œâ”€â”€ ğŸ“ full_evaluation/          # Full evaluation results
â”‚   â”œâ”€â”€ ğŸ“ ablation_study/           # Ablation study results
â”‚   â””â”€â”€ ğŸ“ paper_results/            # Academic paper results
â”‚
â”œâ”€â”€ ğŸ“š docs/                         # Documentation
â”‚   â”œâ”€â”€ ğŸ“„ installation.md           # Installation guide
â”‚   â”œâ”€â”€ ğŸ“„ configuration.md          # Configuration reference
â”‚   â”œâ”€â”€ ğŸ“„ api.md                    # API documentation
â”‚   â”œâ”€â”€ ğŸ“„ experiments.md            # Experiment guide
â”‚   â””â”€â”€ ğŸ“„ architecture.md           # Architecture overview
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ ğŸ“„ setup_environment.sh      # Environment setup
â”‚   â”œâ”€â”€ ğŸ“„ run_benchmarks.sh         # Benchmark runner
â”‚   â””â”€â”€ ğŸ“„ generate_paper_results.sh # Paper result generator
â”‚
â”œâ”€â”€ ğŸ“„ run_experiments.py            # Main experiment runner
â””â”€â”€ ğŸ“„ quick_test.py                 # Quick functionality test
```

## ğŸ¯ Key Components

### 1. Core Implementation (`adaptive_rag/`)
- **Configuration System**: Flexible YAML and dictionary-based configuration
- **Core Components**: Task decomposer, retrieval planner, context reranker
- **Modules**: Query analyzer, multi-retriever, adaptive generator
- **Pipeline**: End-to-end processing pipeline
- **Evaluation**: Comprehensive evaluation framework
- **WebUI**: Interactive web interface

### 2. Evaluation Framework (`adaptive_rag/evaluation/`)
- **Benchmark Runner**: Main evaluation orchestrator
- **Dataset Downloader**: Automatic dataset management
- **Baseline Methods**: Implementation of comparison methods
- **Ablation Analyzer**: Component contribution analysis
- **Result Analyzer**: Performance analysis and visualization

### 3. Experimental Infrastructure
- **Tests**: Comprehensive test suite for reliability
- **Scripts**: Automation and setup utilities
- **Documentation**: Complete project documentation
- **Experiments**: Organized result storage

## ğŸ“‹ File Descriptions

### Core Files
- `README.md`: Project overview, installation, and usage
- `setup.py`: Python package configuration
- `requirements.txt`: Dependency specifications
- `run_experiments.py`: Main experiment execution script

### Configuration
- `adaptive_rag/config.py`: Centralized configuration management
- Support for YAML files and dictionary overrides
- Automatic environment setup and validation

### Evaluation
- `benchmark_runner.py`: Orchestrates all evaluation tasks
- `baseline_methods.py`: Naive RAG, Self-RAG, RAPTOR implementations
- `dataset_downloader.py`: FlashRAG dataset integration
- `ablation_analyzer.py`: Component contribution analysis

### Testing
- `tests/test_config.py`: Configuration system tests
- `quick_test.py`: Rapid functionality verification
- Comprehensive test coverage for all components

## ğŸš€ Usage Patterns

### Development Workflow
1. **Setup**: `scripts/setup_environment.sh`
2. **Test**: `python quick_test.py`
3. **Develop**: Edit core components
4. **Evaluate**: `python run_experiments.py`
5. **Commit**: Standard git workflow

### Experiment Workflow
1. **Configure**: Edit config files or use CLI args
2. **Run**: Execute experiment scripts
3. **Analyze**: Use built-in analysis tools
4. **Report**: Generate academic paper results

### Extension Points
- Add new retrieval methods in `modules/`
- Implement new baselines in `evaluation/baseline_methods.py`
- Create custom evaluation metrics in `evaluation/`
- Extend configuration in `config.py`

## ğŸ“Š Data Flow

```
Query â†’ Task Decomposer â†’ Retrieval Planner â†’ Multi-Retriever â†’ Context Reranker â†’ Generator â†’ Response
                                    â†“
                            Evaluation Framework
                                    â†“
                            Results & Analysis
```

## ğŸ”§ Maintenance

### Regular Tasks
- Update dependencies in `requirements.txt`
- Add new tests for new features
- Update documentation for API changes
- Run full test suite before releases

### Release Process
1. Update `CHANGELOG.md`
2. Bump version in `setup.py`
3. Run full test suite
4. Create git tag
5. Push to repository

---

**ğŸ¯ This structure provides a solid foundation for AdaptiveRAG development, experimentation, and academic research!**
