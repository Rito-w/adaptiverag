#!/usr/bin/env python3
"""
=== AdaptiveRAG è®ºæ–‡å®éªŒè„šæœ¬ ===

æ‰§è¡Œå®Œæ•´çš„è®ºæ–‡å®éªŒï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹æ¯”ã€æ¶ˆèç ”ç©¶å’Œæ•ˆç‡åˆ†æ
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from adaptive_rag.evaluation.benchmark_runner import BenchmarkRunner, BenchmarkConfig
from adaptive_rag.evaluation.ablation_analyzer import AblationAnalyzer
from adaptive_rag.config import AdaptiveRAGConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PaperExperimentRunner:
    """è®ºæ–‡å®éªŒè¿è¡Œå™¨"""

    def __init__(self, output_dir: str = "/root/autodl-tmp/adaptiverag_experiments",
                 data_dir: str = "/root/autodl-tmp/adaptiverag_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # å®éªŒé…ç½®
        self.datasets = {
            "single_hop": ["natural_questions", "trivia_qa", "ms_marco"],
            "multi_hop": ["hotpot_qa", "2wiki_multihop", "musique"],
            "conversational": ["quac", "coqa"]
        }
        
        self.baseline_methods = [
            "naive_rag",
            "self_rag", 
            "raptor",
            "rag_fusion",
            "hyde"
        ]
        
        self.ablation_methods = [
            "adaptive_rag",                    # å®Œæ•´æ–¹æ³•
            "adaptive_rag_no_decomposition",   # æ— ä»»åŠ¡åˆ†è§£
            "adaptive_rag_no_planning",        # æ— ç­–ç•¥è§„åˆ’
            "adaptive_rag_single_retriever",   # å•ä¸€æ£€ç´¢å™¨
            "adaptive_rag_no_reranking",       # æ— é‡æ’åº
            "adaptive_rag_minimal"             # æœ€å°é…ç½®
        ]
        
        self.metrics = ["exact_match", "f1_score", "rouge_l", "bert_score"]
        
    def run_main_comparison(self, sample_size: int = 1000):
        """è¿è¡Œä¸»è¦å¯¹æ¯”å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹ä¸»è¦å¯¹æ¯”å®éªŒ")
        
        results = {}
        
        for category, datasets in self.datasets.items():
            logger.info(f"ğŸ“Š å¤„ç† {category} æ•°æ®é›†")
            
            for dataset in datasets:
                logger.info(f"  ğŸ“š æ•°æ®é›†: {dataset}")
                
                # é…ç½®å®éªŒ
                config = BenchmarkConfig(
                    datasets=[dataset],
                    methods=["adaptive_rag"] + self.baseline_methods,
                    output_dir=str(self.output_dir / "main_comparison" / dataset),
                    max_samples=sample_size,
                    save_predictions=True,
                    compute_bert_score=True
                )
                
                # è¿è¡Œå®éªŒ
                runner = BenchmarkRunner(config)
                dataset_results = runner.run_benchmark()
                
                results[dataset] = dataset_results
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self._save_intermediate_results(
                    results, 
                    self.output_dir / "main_comparison" / "intermediate_results.json"
                )
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(results, "main_comparison")
        logger.info("âœ… ä¸»è¦å¯¹æ¯”å®éªŒå®Œæˆ")
        
        return results
    
    def run_ablation_study(self, sample_size: int = 500):
        """è¿è¡Œæ¶ˆèç ”ç©¶"""
        logger.info("ğŸ”¬ å¼€å§‹æ¶ˆèç ”ç©¶")
        
        # é€‰æ‹©ä»£è¡¨æ€§æ•°æ®é›†
        test_datasets = ["natural_questions", "hotpot_qa"]
        results = {}
        
        for dataset in test_datasets:
            logger.info(f"ğŸ“š æ¶ˆèç ”ç©¶æ•°æ®é›†: {dataset}")
            
            # é…ç½®æ¶ˆèå®éªŒ
            config = BenchmarkConfig(
                datasets=[dataset],
                methods=self.ablation_methods,
                output_dir=str(self.output_dir / "ablation_study" / dataset),
                max_samples=sample_size,
                save_predictions=True,
                compute_bert_score=True
            )
            
            # è¿è¡Œå®éªŒ
            runner = BenchmarkRunner(config)
            dataset_results = runner.run_benchmark()
            
            results[dataset] = dataset_results
        
        # åˆ†ææ¶ˆèç»“æœ
        analyzer = AblationAnalyzer(str(self.output_dir / "ablation_study"))
        ablation_analysis = analyzer.analyze_component_contribution()
        
        # ç”Ÿæˆå¯è§†åŒ–
        analyzer.generate_visualization(ablation_analysis)
        analyzer.generate_report(ablation_analysis)
        
        # ä¿å­˜ç»“æœ
        self._save_final_results(results, "ablation_study")
        logger.info("âœ… æ¶ˆèç ”ç©¶å®Œæˆ")
        
        return results, ablation_analysis
    
    def run_efficiency_analysis(self, sample_size: int = 200):
        """è¿è¡Œæ•ˆç‡åˆ†æ"""
        logger.info("âš¡ å¼€å§‹æ•ˆç‡åˆ†æ")
        
        # æ•ˆç‡æµ‹è¯•é…ç½®
        efficiency_methods = ["adaptive_rag", "self_rag", "naive_rag"]
        test_dataset = "natural_questions"
        
        results = {}
        
        for method in efficiency_methods:
            logger.info(f"â±ï¸ æµ‹è¯•æ–¹æ³•: {method}")
            
            config = BenchmarkConfig(
                datasets=[test_dataset],
                methods=[method],
                output_dir=str(self.output_dir / "efficiency_analysis" / method),
                max_samples=sample_size,
                save_predictions=False,  # èŠ‚çœæ—¶é—´
                compute_bert_score=False  # èŠ‚çœæ—¶é—´
            )
            
            runner = BenchmarkRunner(config)
            method_results = runner.run_benchmark()
            
            results[method] = method_results
        
        # åˆ†ææ•ˆç‡ç»“æœ
        efficiency_report = self._analyze_efficiency(results)
        
        # ä¿å­˜ç»“æœ
        self._save_final_results(results, "efficiency_analysis")
        self._save_efficiency_report(efficiency_report)
        
        logger.info("âœ… æ•ˆç‡åˆ†æå®Œæˆ")
        
        return results, efficiency_report
    
    def run_statistical_analysis(self, results: Dict[str, Any]):
        """è¿è¡Œç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ"""
        logger.info("ğŸ“ˆ å¼€å§‹ç»Ÿè®¡åˆ†æ")
        
        from adaptive_rag.evaluation.statistical_analyzer import StatisticalAnalyzer
        
        analyzer = StatisticalAnalyzer()
        statistical_results = {}
        
        for dataset, dataset_results in results.items():
            logger.info(f"ğŸ“Š åˆ†ææ•°æ®é›†: {dataset}")
            
            # æå–AdaptiveRAGå’ŒåŸºçº¿æ–¹æ³•çš„åˆ†æ•°
            adaptive_scores = self._extract_scores(dataset_results, "adaptive_rag")
            
            dataset_stats = {}
            for baseline in self.baseline_methods:
                if baseline in dataset_results:
                    baseline_scores = self._extract_scores(dataset_results, baseline)
                    
                    # tæ£€éªŒ
                    p_value = analyzer.paired_t_test(adaptive_scores, baseline_scores)
                    
                    # æ•ˆåº”é‡
                    effect_size = analyzer.cohens_d(adaptive_scores, baseline_scores)
                    
                    dataset_stats[baseline] = {
                        "p_value": p_value,
                        "effect_size": effect_size,
                        "significant": p_value < 0.05
                    }
            
            statistical_results[dataset] = dataset_stats
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        self._save_statistical_results(statistical_results)
        
        logger.info("âœ… ç»Ÿè®¡åˆ†æå®Œæˆ")
        return statistical_results
    
    def generate_paper_tables(self, results: Dict[str, Any]):
        """ç”Ÿæˆè®ºæ–‡è¡¨æ ¼"""
        logger.info("ğŸ“‹ ç”Ÿæˆè®ºæ–‡è¡¨æ ¼")
        
        # ä¸»è¦ç»“æœè¡¨æ ¼
        main_table = self._create_main_results_table(results)
        
        # æ¶ˆèç ”ç©¶è¡¨æ ¼
        ablation_table = self._create_ablation_table(results)
        
        # æ•ˆç‡å¯¹æ¯”è¡¨æ ¼
        efficiency_table = self._create_efficiency_table(results)
        
        # ä¿å­˜è¡¨æ ¼
        tables = {
            "main_results": main_table,
            "ablation_study": ablation_table,
            "efficiency_analysis": efficiency_table
        }
        
        self._save_paper_tables(tables)
        
        logger.info("âœ… è®ºæ–‡è¡¨æ ¼ç”Ÿæˆå®Œæˆ")
        return tables
    
    def _save_intermediate_results(self, results: Dict, filepath: Path):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    def _save_final_results(self, results: Dict, experiment_type: str):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        output_file = self.output_dir / f"{experiment_type}_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
    
    def _analyze_efficiency(self, results: Dict) -> Dict:
        """åˆ†ææ•ˆç‡ç»“æœ"""
        efficiency_report = {}
        
        for method, method_results in results.items():
            if method_results:
                result = method_results[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªæ•°æ®é›†
                
                efficiency_report[method] = {
                    "avg_retrieval_time": result.avg_retrieval_time,
                    "avg_generation_time": result.avg_generation_time,
                    "avg_total_time": result.avg_total_time,
                    "memory_usage_mb": result.memory_usage_mb,
                    "throughput": 60 / result.avg_total_time if result.avg_total_time > 0 else 0
                }
        
        return efficiency_report
    
    def _save_efficiency_report(self, report: Dict):
        """ä¿å­˜æ•ˆç‡æŠ¥å‘Š"""
        output_file = self.output_dir / "efficiency_report.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
    def _extract_scores(self, results: List, method: str) -> List[float]:
        """æå–æŒ‡å®šæ–¹æ³•çš„åˆ†æ•°"""
        for result in results:
            if result.method_name == method:
                return [result.exact_match, result.f1_score, result.rouge_l]
        return []
    
    def _save_statistical_results(self, results: Dict):
        """ä¿å­˜ç»Ÿè®¡ç»“æœ"""
        output_file = self.output_dir / "statistical_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    
    def _create_main_results_table(self, results: Dict) -> str:
        """åˆ›å»ºä¸»è¦ç»“æœè¡¨æ ¼"""
        # è¿™é‡Œå®ç°LaTeXè¡¨æ ¼ç”Ÿæˆé€»è¾‘
        return "LaTeX table for main results"
    
    def _create_ablation_table(self, results: Dict) -> str:
        """åˆ›å»ºæ¶ˆèç ”ç©¶è¡¨æ ¼"""
        return "LaTeX table for ablation study"
    
    def _create_efficiency_table(self, results: Dict) -> str:
        """åˆ›å»ºæ•ˆç‡å¯¹æ¯”è¡¨æ ¼"""
        return "LaTeX table for efficiency analysis"
    
    def _save_paper_tables(self, tables: Dict):
        """ä¿å­˜è®ºæ–‡è¡¨æ ¼"""
        output_file = self.output_dir / "paper_tables.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tables, f, indent=2, ensure_ascii=False)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AdaptiveRAG è®ºæ–‡å®éªŒ")
    parser.add_argument("--experiment", choices=["all", "main", "ablation", "efficiency"], 
                       default="all", help="å®éªŒç±»å‹")
    parser.add_argument("--sample_size", type=int, default=1000, help="æ ·æœ¬å¤§å°")
    parser.add_argument("--output_dir", default="./experiments/paper_results", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = PaperExperimentRunner(args.output_dir)
    
    logger.info("ğŸš€ å¼€å§‹ AdaptiveRAG è®ºæ–‡å®éªŒ")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"ğŸ“Š æ ·æœ¬å¤§å°: {args.sample_size}")
    
    results = {}
    
    if args.experiment in ["all", "main"]:
        main_results = runner.run_main_comparison(args.sample_size)
        results["main_comparison"] = main_results
    
    if args.experiment in ["all", "ablation"]:
        ablation_results, ablation_analysis = runner.run_ablation_study(args.sample_size // 2)
        results["ablation_study"] = ablation_results
    
    if args.experiment in ["all", "efficiency"]:
        efficiency_results, efficiency_report = runner.run_efficiency_analysis(args.sample_size // 5)
        results["efficiency_analysis"] = efficiency_results
    
    # ç»Ÿè®¡åˆ†æ
    if "main_comparison" in results:
        statistical_results = runner.run_statistical_analysis(results["main_comparison"])
        results["statistical_analysis"] = statistical_results
    
    # ç”Ÿæˆè®ºæ–‡è¡¨æ ¼
    paper_tables = runner.generate_paper_tables(results)
    
    logger.info("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    logger.info(f"ğŸ“„ ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    
    # ç”Ÿæˆå®éªŒæ€»ç»“
    summary = {
        "experiment_date": datetime.now().isoformat(),
        "sample_size": args.sample_size,
        "experiments_run": list(results.keys()),
        "output_directory": args.output_dir
    }
    
    summary_file = Path(args.output_dir) / "experiment_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ¯ å®éªŒæ€»ç»“å·²ä¿å­˜: {summary_file}")


if __name__ == "__main__":
    main()
