# AdaptiveRAG 论文实验配置文件

# 基础设置
experiment_name: "AdaptiveRAG_Paper_Experiments"
description: "Complete experimental evaluation for AdaptiveRAG academic paper"
version: "1.0"
date: "2024-01-01"

# 硬件配置
hardware:
  gpu_count: 2
  gpu_memory: "40GB"
  cpu_cores: 32
  ram: "256GB"
  storage: "2TB NVMe"

# 数据集配置
datasets:
  single_hop:
    natural_questions:
      train_size: 307373
      dev_size: 7830
      test_size: 7842
      difficulty: "medium"
      focus: "factual_qa"
    
    trivia_qa:
      train_size: 87622
      dev_size: 11313
      test_size: 10790
      difficulty: "medium"
      focus: "knowledge_qa"
    
    ms_marco:
      train_size: 808731
      dev_size: 101093
      test_size: null
      difficulty: "medium"
      focus: "reading_comprehension"

  multi_hop:
    hotpot_qa:
      train_size: 90447
      dev_size: 7405
      test_size: null
      difficulty: "high"
      focus: "multi_hop_reasoning"
    
    2wiki_multihop:
      train_size: 167247
      dev_size: 12576
      test_size: null
      difficulty: "high"
      focus: "structured_reasoning"
    
    musique:
      train_size: 19938
      dev_size: 2417
      test_size: null
      difficulty: "very_high"
      focus: "compositional_reasoning"

  conversational:
    quac:
      train_size: 83568
      dev_size: 7354
      test_size: null
      difficulty: "medium_high"
      focus: "contextual_qa"
    
    coqa:
      train_size: 127000
      dev_size: null
      test_size: null
      difficulty: "medium_high"
      focus: "conversational_qa"

# 方法配置
methods:
  adaptive_rag:
    description: "Complete AdaptiveRAG system"
    components:
      task_decomposition: true
      strategy_planning: true
      multi_retriever: true
      reranking: true
    config:
      retrieval_topk: 20
      final_context_count: 5
      max_tokens: 256
      temperature: 0.1

  baselines:
    naive_rag:
      description: "Simple retrieve-then-generate baseline"
      retrieval_topk: 5
      max_tokens: 256
    
    self_rag:
      description: "Self-reflective RAG with iterative refinement"
      max_iterations: 3
      reflection_threshold: 0.7
      retrieval_topk: 5
    
    raptor:
      description: "Recursive abstractive processing"
      tree_depth: 3
      cluster_size: 5
      retrieval_topk: 10
    
    rag_fusion:
      description: "Multi-query fusion approach"
      num_queries: 3
      fusion_method: "reciprocal_rank"
    
    hyde:
      description: "Hypothetical document embeddings"
      num_hypotheses: 2
      hypothesis_length: 100

  ablation_variants:
    adaptive_rag_no_decomposition:
      description: "AdaptiveRAG without task decomposition"
      base: "adaptive_rag"
      disable: ["task_decomposition"]
    
    adaptive_rag_no_planning:
      description: "AdaptiveRAG without strategy planning"
      base: "adaptive_rag"
      disable: ["strategy_planning"]
    
    adaptive_rag_single_retriever:
      description: "AdaptiveRAG with single retriever only"
      base: "adaptive_rag"
      disable: ["multi_retriever"]
      force_single_retriever: "dense"
    
    adaptive_rag_no_reranking:
      description: "AdaptiveRAG without reranking"
      base: "adaptive_rag"
      disable: ["reranking"]
    
    adaptive_rag_minimal:
      description: "Minimal AdaptiveRAG configuration"
      base: "adaptive_rag"
      disable: ["task_decomposition", "strategy_planning", "reranking"]

# 评估指标
metrics:
  primary:
    exact_match:
      description: "Exact string match with gold answer"
      weight: 0.3
    
    f1_score:
      description: "Token-level F1 score"
      weight: 0.3
    
    rouge_l:
      description: "Longest common subsequence F1"
      weight: 0.2
    
    bert_score:
      description: "Semantic similarity using BERT"
      weight: 0.2

  secondary:
    relevance_score:
      description: "Retrieved document relevance"
      compute: true
    
    coherence_score:
      description: "Response coherence and fluency"
      compute: true
    
    factual_accuracy:
      description: "Factual correctness verification"
      compute: false  # 需要额外的事实检查模型

  efficiency:
    retrieval_latency:
      description: "Average retrieval time (ms)"
      unit: "milliseconds"
    
    generation_latency:
      description: "Average generation time (ms)"
      unit: "milliseconds"
    
    total_response_time:
      description: "End-to-end response time (s)"
      unit: "seconds"
    
    memory_usage:
      description: "Peak memory consumption (MB)"
      unit: "megabytes"
    
    throughput:
      description: "Queries processed per minute"
      unit: "queries_per_minute"

# 实验设置
experiments:
  main_comparison:
    description: "Compare AdaptiveRAG with SOTA baselines"
    datasets: ["natural_questions", "hotpot_qa", "trivia_qa", "ms_marco"]
    methods: ["adaptive_rag", "self_rag", "raptor", "naive_rag", "rag_fusion"]
    sample_size: 1000
    metrics: ["exact_match", "f1_score", "rouge_l", "bert_score"]
    statistical_tests: ["paired_t_test", "wilcoxon", "bootstrap"]
    
  ablation_study:
    description: "Analyze component contributions"
    datasets: ["natural_questions", "hotpot_qa"]
    methods: 
      - "adaptive_rag"
      - "adaptive_rag_no_decomposition"
      - "adaptive_rag_no_planning"
      - "adaptive_rag_single_retriever"
      - "adaptive_rag_no_reranking"
      - "adaptive_rag_minimal"
    sample_size: 500
    metrics: ["exact_match", "f1_score", "rouge_l"]
    analysis: ["component_contribution", "interaction_effects"]
    
  efficiency_analysis:
    description: "Evaluate computational efficiency"
    datasets: ["natural_questions"]
    methods: ["adaptive_rag", "self_rag", "naive_rag"]
    sample_size: 200
    metrics: ["retrieval_latency", "generation_latency", "total_response_time", "memory_usage"]
    profiling: true
    
  scalability_test:
    description: "Test performance with varying dataset sizes"
    datasets: ["natural_questions"]
    methods: ["adaptive_rag"]
    sample_sizes: [100, 500, 1000, 2000, 5000]
    metrics: ["exact_match", "f1_score", "total_response_time"]
    
  robustness_analysis:
    description: "Evaluate robustness to different conditions"
    datasets: ["natural_questions", "hotpot_qa"]
    methods: ["adaptive_rag"]
    conditions:
      noise_levels: [0.0, 0.1, 0.2, 0.3]
      context_lengths: [3, 5, 10, 15, 20]
      temperature_values: [0.0, 0.1, 0.3, 0.5, 0.7]

# 统计分析
statistical_analysis:
  significance_level: 0.05
  correction_method: "bonferroni"
  effect_size_threshold: 0.2
  confidence_interval: 0.95
  bootstrap_samples: 1000
  
  tests:
    paired_t_test:
      description: "Paired t-test for metric comparisons"
      assumptions: ["normality", "paired_samples"]
    
    wilcoxon:
      description: "Non-parametric alternative to t-test"
      assumptions: ["paired_samples"]
    
    bootstrap:
      description: "Bootstrap confidence intervals"
      assumptions: ["none"]

# 输出配置
output:
  base_directory: "./experiments/paper_results"
  save_predictions: true
  save_intermediate_results: true
  generate_plots: true
  generate_latex_tables: true
  
  plots:
    performance_comparison:
      type: "bar_chart"
      metrics: ["exact_match", "f1_score", "rouge_l"]
      format: ["png", "pdf"]
    
    ablation_analysis:
      type: "heatmap"
      show_relative_changes: true
      format: ["png", "pdf"]
    
    efficiency_analysis:
      type: "scatter_plot"
      x_axis: "total_response_time"
      y_axis: "exact_match"
      format: ["png", "pdf"]
  
  tables:
    main_results:
      format: "latex"
      include_confidence_intervals: true
      highlight_best: true
    
    ablation_results:
      format: "latex"
      show_relative_changes: true
      include_significance: true

# 资源管理
resource_management:
  max_parallel_jobs: 4
  memory_limit_per_job: "32GB"
  timeout_per_experiment: "24h"
  checkpoint_frequency: "1h"
  
  caching:
    enable_result_caching: true
    cache_directory: "./cache"
    cache_expiry: "7d"
  
  monitoring:
    log_level: "INFO"
    log_file: "./logs/paper_experiments.log"
    progress_reporting: true
    resource_monitoring: true

# 可重现性设置
reproducibility:
  random_seed: 42
  deterministic_algorithms: true
  version_tracking: true
  environment_snapshot: true
  
  dependencies:
    python_version: "3.9+"
    pytorch_version: "2.0+"
    transformers_version: "4.30+"
    numpy_version: "1.21+"
