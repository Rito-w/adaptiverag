#!/usr/bin/env python3
"""
=== çœŸå®æ¨¡å‹å¼•æ“ ===

ä½¿ç”¨çœŸå®çš„æ¨¡å‹å’Œæ•°æ®ï¼Œè®©æ¨¡å—å¼€å…³æ•ˆæœæ˜æ˜¾å¯è§
"""

import logging
import time
import json
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = logging.getLogger(__name__)

# å¯¼å…¥æ¨¡å—ç®¡ç†å™¨
try:
    from adaptive_rag.core.module_manager import ModuleManager
    from adaptive_rag.config import (
        create_config_from_yaml, ModuleToggleConfig, 
        FlexRAGIntegratedConfig, get_enabled_modules
    )
    MODULE_MANAGER_AVAILABLE = True
except ImportError:
    MODULE_MANAGER_AVAILABLE = False
    logger.warning("æ¨¡å—ç®¡ç†å™¨ä¸å¯ç”¨")

# å¯¼å…¥çœŸå®ç»„ä»¶
try:
    import torch
    from sentence_transformers import SentenceTransformer
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorchç»„ä»¶ä¸å¯ç”¨")

try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    logger.warning("BM25ä¸å¯ç”¨")


class RealModelEngine:
    """çœŸå®æ¨¡å‹å¼•æ“ - ä½¿ç”¨çœŸå®çš„æ£€ç´¢å™¨ã€é‡æ’åºå™¨å’Œç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "adaptive_rag/config/modular_config.yaml"):
        """åˆå§‹åŒ–çœŸå®æ¨¡å‹å¼•æ“"""
        logger.info("ğŸš€ åˆå§‹åŒ–çœŸå®æ¨¡å‹å¼•æ“...")
        
        self.config_path = config_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨
        self.initialize_module_manager()
        
        # åˆå§‹åŒ–çœŸå®ç»„ä»¶
        self.initialize_real_components()
        
        # åŠ è½½çœŸå®æ•°æ®
        self.load_real_data()
        
        logger.info("âœ… çœŸå®æ¨¡å‹å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def initialize_module_manager(self):
        """åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨"""
        if MODULE_MANAGER_AVAILABLE:
            try:
                # åŠ è½½æ¨¡å—åŒ–é…ç½®
                if Path(self.config_path).exists():
                    self.modular_config = create_config_from_yaml(self.config_path, preset="performance_mode")
                else:
                    self.modular_config = FlexRAGIntegratedConfig()
                    self.modular_config.modules = ModuleToggleConfig()
                
                # åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨
                self.module_manager = ModuleManager(self.modular_config)
                self.module_manager.initialize_modules()
                
                logger.info("âœ… æ¨¡å—ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ æ¨¡å—ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.module_manager = None
                self.modular_config = None
        else:
            self.module_manager = None
            self.modular_config = None
    
    def initialize_real_components(self):
        """åˆå§‹åŒ–çœŸå®ç»„ä»¶"""
        self.components = {}
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºå¯†é›†æ£€ç´¢ï¼‰
        if TORCH_AVAILABLE:
            try:
                logger.info("ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹...")
                self.components['embedding_model'] = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.components['embedding_model'] = None
        
        # åˆå§‹åŒ–ç”Ÿæˆæ¨¡å‹
        if TORCH_AVAILABLE:
            try:
                logger.info("ğŸ“¥ åŠ è½½ç”Ÿæˆæ¨¡å‹...")
                # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœèµ„æº
                model_name = "microsoft/DialoGPT-small"
                self.components['tokenizer'] = AutoTokenizer.from_pretrained(model_name)
                self.components['generator'] = AutoModelForCausalLM.from_pretrained(model_name)
                
                # è®¾ç½®pad_token
                if self.components['tokenizer'].pad_token is None:
                    self.components['tokenizer'].pad_token = self.components['tokenizer'].eos_token
                
                logger.info("âœ… ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.components['tokenizer'] = None
                self.components['generator'] = None
    
    def load_real_data(self):
        """åŠ è½½çœŸå®æ•°æ®"""
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£åº“
        self.documents = [
            {
                "id": "doc_1",
                "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
                "content": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚è¿™åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚"
            },
            {
                "id": "doc_2", 
                "title": "æœºå™¨å­¦ä¹ æ¦‚è¿°",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚å®ƒåŸºäºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"
            },
            {
                "id": "doc_3",
                "title": "æ·±åº¦å­¦ä¹ æŠ€æœ¯",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"
            },
            {
                "id": "doc_4",
                "title": "è‡ªç„¶è¯­è¨€å¤„ç†",
                "content": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚å®ƒåŒ…æ‹¬æ–‡æœ¬åˆ†æã€è¯­è¨€ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ã€‚"
            },
            {
                "id": "doc_5",
                "title": "è®¡ç®—æœºè§†è§‰",
                "content": "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯ã€‚å®ƒæ¶‰åŠå›¾åƒå¤„ç†ã€ç‰©ä½“æ£€æµ‹ã€äººè„¸è¯†åˆ«å’Œåœºæ™¯ç†è§£ç­‰æŠ€æœ¯ã€‚"
            }
        ]
        
        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        if BM25_AVAILABLE:
            try:
                # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬ç”¨äºBM25
                doc_texts = [doc['content'] for doc in self.documents]
                tokenized_docs = [text.split() for text in doc_texts]
                self.components['bm25'] = BM25Okapi(tokenized_docs)
                logger.info("âœ… BM25æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ BM25æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.components['bm25'] = None
        
        # é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
        if self.components.get('embedding_model'):
            try:
                doc_texts = [doc['content'] for doc in self.documents]
                self.document_embeddings = self.components['embedding_model'].encode(doc_texts)
                logger.info("âœ… æ–‡æ¡£åµŒå…¥é¢„è®¡ç®—å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ æ–‡æ¡£åµŒå…¥è®¡ç®—å¤±è´¥: {e}")
                self.document_embeddings = None
    
    def process_query_with_modules(self, query: str) -> Dict[str, Any]:
        """æ ¹æ®å¯ç”¨çš„æ¨¡å—å¤„ç†æŸ¥è¯¢"""
        start_time = time.time()
        result = {
            "query": query,
            "steps": [],
            "retrieval_results": [],
            "reranked_results": [],
            "generated_answer": "",
            "total_time": 0,
            "module_usage": {}
        }
        
        logger.info(f"ğŸ” å¤„ç†æŸ¥è¯¢: {query}")
        
        # 1. ä»»åŠ¡åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("task_decomposer"):
            logger.info("ğŸ“‹ æ‰§è¡Œä»»åŠ¡åˆ†è§£...")
            subtasks = self.real_task_decomposition(query)
            result["steps"].append(f"ä»»åŠ¡åˆ†è§£: è¯†åˆ«åˆ° {len(subtasks)} ä¸ªå­ä»»åŠ¡")
            result["module_usage"]["task_decomposer"] = True
        else:
            subtasks = [query]  # ä¸åˆ†è§£ï¼Œç›´æ¥ä½¿ç”¨åŸæŸ¥è¯¢
            result["module_usage"]["task_decomposer"] = False
        
        # 2. æ£€ç´¢é˜¶æ®µ
        all_retrieved_docs = []
        
        # å…³é”®è¯æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("keyword_retriever"):
            logger.info("ğŸ” æ‰§è¡Œå…³é”®è¯æ£€ç´¢...")
            keyword_docs = self.real_keyword_retrieval(query)
            all_retrieved_docs.extend(keyword_docs)
            result["steps"].append(f"å…³é”®è¯æ£€ç´¢: æ‰¾åˆ° {len(keyword_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["keyword_retriever"] = True
        else:
            result["module_usage"]["keyword_retriever"] = False
        
        # å¯†é›†æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("dense_retriever"):
            logger.info("ğŸ§  æ‰§è¡Œå¯†é›†æ£€ç´¢...")
            dense_docs = self.real_dense_retrieval(query)
            all_retrieved_docs.extend(dense_docs)
            result["steps"].append(f"å¯†é›†æ£€ç´¢: æ‰¾åˆ° {len(dense_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["dense_retriever"] = True
        else:
            result["module_usage"]["dense_retriever"] = False
        
        # ç½‘ç»œæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("web_retriever"):
            logger.info("ğŸŒ æ‰§è¡Œç½‘ç»œæ£€ç´¢...")
            web_docs = self.simulate_web_retrieval(query)  # æ¨¡æ‹Ÿç½‘ç»œæ£€ç´¢
            all_retrieved_docs.extend(web_docs)
            result["steps"].append(f"ç½‘ç»œæ£€ç´¢: æ‰¾åˆ° {len(web_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["web_retriever"] = True
        else:
            result["module_usage"]["web_retriever"] = False
        
        result["retrieval_results"] = all_retrieved_docs
        
        # 3. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("context_reranker") and all_retrieved_docs:
            logger.info("ğŸ¯ æ‰§è¡Œä¸Šä¸‹æ–‡é‡æ’åº...")
            reranked_docs = self.real_reranking(query, all_retrieved_docs)
            result["reranked_results"] = reranked_docs
            result["steps"].append(f"é‡æ’åº: é‡æ–°æ’åº {len(reranked_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["context_reranker"] = True
        else:
            result["reranked_results"] = all_retrieved_docs[:5]  # å–å‰5ä¸ª
            result["module_usage"]["context_reranker"] = False
        
        # 4. ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("adaptive_generator"):
            logger.info("âœ¨ æ‰§è¡Œè‡ªé€‚åº”ç”Ÿæˆ...")
            answer = self.real_generation(query, result["reranked_results"])
            result["generated_answer"] = answer
            result["steps"].append("è‡ªé€‚åº”ç”Ÿæˆ: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            result["module_usage"]["adaptive_generator"] = True
        else:
            # ç®€å•æ‹¼æ¥æ£€ç´¢ç»“æœ
            contexts = [doc.get('content', '') for doc in result["reranked_results"][:3]]
            result["generated_answer"] = f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š{' '.join(contexts[:200])}..."
            result["module_usage"]["adaptive_generator"] = False
        
        result["total_time"] = time.time() - start_time
        logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶ {result['total_time']:.2f}s")
        
        return result
    
    def real_task_decomposition(self, query: str) -> List[str]:
        """çœŸå®çš„ä»»åŠ¡åˆ†è§£"""
        # ç®€å•çš„åŸºäºå…³é”®è¯çš„ä»»åŠ¡åˆ†è§£
        if "ä»€ä¹ˆæ˜¯" in query or "ä»‹ç»" in query:
            return [f"å®šä¹‰: {query}", f"ç‰¹ç‚¹: {query}", f"åº”ç”¨: {query}"]
        elif "å¦‚ä½•" in query or "æ€ä¹ˆ" in query:
            return [f"æ–¹æ³•: {query}", f"æ­¥éª¤: {query}", f"æ³¨æ„äº‹é¡¹: {query}"]
        else:
            return [query]
    
    def real_keyword_retrieval(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """çœŸå®çš„å…³é”®è¯æ£€ç´¢"""
        if not self.components.get('bm25'):
            return []
        
        try:
            # ä½¿ç”¨BM25è¿›è¡Œæ£€ç´¢
            tokenized_query = query.split()
            scores = self.components['bm25'].get_scores(tokenized_query)
            
            # è·å–top_kç»“æœ
            top_indices = scores.argsort()[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['score'] = float(scores[idx])
                    doc['retrieval_type'] = 'keyword'
                    results.append(doc)
            
            return results
        except Exception as e:
            logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def real_dense_retrieval(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """çœŸå®çš„å¯†é›†æ£€ç´¢"""
        if not self.components.get('embedding_model') or self.document_embeddings is None:
            return []
        
        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            query_embedding = self.components['embedding_model'].encode([query])
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]
            
            # è·å–top_kç»“æœ
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['score'] = float(similarities[idx])
                    doc['retrieval_type'] = 'dense'
                    results.append(doc)
            
            return results
        except Exception as e:
            logger.error(f"å¯†é›†æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def simulate_web_retrieval(self, query: str, top_k: int = 2) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿç½‘ç»œæ£€ç´¢"""
        # æ¨¡æ‹Ÿç½‘ç»œæœç´¢ç»“æœ
        web_results = [
            {
                "id": f"web_{i}",
                "title": f"ç½‘ç»œæœç´¢ç»“æœ {i}: {query}",
                "content": f"è¿™æ˜¯æ¥è‡ªç½‘ç»œçš„å…³äº'{query}'çš„æœç´¢ç»“æœ {i}ã€‚åŒ…å«æœ€æ–°çš„ç›¸å…³ä¿¡æ¯ã€‚",
                "score": 0.8 - i * 0.1,
                "retrieval_type": "web",
                "url": f"https://example.com/result_{i}"
            }
            for i in range(top_k)
        ]
        return web_results
    
    def real_reranking(self, query: str, documents: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        """çœŸå®çš„é‡æ’åº"""
        if not documents:
            return []
        
        try:
            # ç®€å•çš„åŸºäºå†…å®¹é•¿åº¦å’ŒæŸ¥è¯¢åŒ¹é…çš„é‡æ’åº
            def rerank_score(doc):
                content = doc.get('content', '')
                title = doc.get('title', '')
                
                # è®¡ç®—æŸ¥è¯¢è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°
                query_words = query.lower().split()
                content_lower = content.lower()
                title_lower = title.lower()
                
                content_matches = sum(1 for word in query_words if word in content_lower)
                title_matches = sum(1 for word in query_words if word in title_lower)
                
                # ç»¼åˆè¯„åˆ†ï¼šåŸå§‹åˆ†æ•° + åŒ¹é…åˆ†æ•°
                original_score = doc.get('score', 0)
                match_score = (title_matches * 2 + content_matches) / len(query_words)
                
                return original_score * 0.7 + match_score * 0.3
            
            # é‡æ–°æ’åº
            reranked_docs = sorted(documents, key=rerank_score, reverse=True)
            
            # æ›´æ–°åˆ†æ•°
            for i, doc in enumerate(reranked_docs[:top_k]):
                doc['rerank_score'] = rerank_score(doc)
                doc['rerank_position'] = i + 1
            
            return reranked_docs[:top_k]
            
        except Exception as e:
            logger.error(f"é‡æ’åºå¤±è´¥: {e}")
            return documents[:top_k]
    
    def real_generation(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """çœŸå®çš„ç”Ÿæˆ"""
        if not self.components.get('generator') or not self.components.get('tokenizer'):
            # å›é€€åˆ°ç®€å•æ‹¼æ¥
            if contexts:
                context_text = " ".join([ctx.get('content', '')[:100] for ctx in contexts[:3]])
                return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå…³äº'{query}'ï¼š{context_text}..."
            else:
                return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
        
        try:
            # æ„å»ºæç¤ºè¯
            context_text = "\n".join([f"- {ctx.get('content', '')[:200]}" for ctx in contexts[:3]])
            prompt = f"åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n{context_text}\n\né—®é¢˜ï¼š{query}\nå›ç­”ï¼š"
            
            # ç”Ÿæˆå›ç­”
            inputs = self.components['tokenizer'].encode(prompt, return_tensors='pt', max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = self.components['generator'].generate(
                    inputs,
                    max_length=inputs.shape[1] + 100,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.components['tokenizer'].eos_token_id
                )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = self.components['tokenizer'].decode(outputs[0], skip_special_tokens=True)
            
            # æå–å›ç­”éƒ¨åˆ†
            if "å›ç­”ï¼š" in generated_text:
                answer = generated_text.split("å›ç­”ï¼š")[-1].strip()
            else:
                answer = generated_text[len(prompt):].strip()
            
            return answer if answer else "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆåˆé€‚çš„å›ç­”ã€‚"
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ‹¼æ¥
            if contexts:
                context_text = " ".join([ctx.get('content', '')[:100] for ctx in contexts[:3]])
                return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå…³äº'{query}'ï¼š{context_text}..."
            else:
                return f"æŠ±æ­‰ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    def is_module_enabled(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ç”¨"""
        if self.module_manager:
            return self.module_manager.is_module_enabled(module_name)
        return True  # é»˜è®¤å¯ç”¨
    
    def update_module_config(self, module_config: Dict[str, bool]) -> bool:
        """æ›´æ–°æ¨¡å—é…ç½®"""
        try:
            if self.modular_config and hasattr(self.modular_config, 'modules'):
                # æ›´æ–°æ¨¡å—å¼€å…³é…ç½®
                for module_name, enabled in module_config.items():
                    if hasattr(self.modular_config.modules, module_name):
                        setattr(self.modular_config.modules, module_name, enabled)
                
                # é‡æ–°åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨
                if self.module_manager:
                    self.module_manager = ModuleManager(self.modular_config)
                    self.module_manager.initialize_modules()
                
                logger.info(f"âœ… æ¨¡å—é…ç½®å·²æ›´æ–°ï¼Œå¯ç”¨æ¨¡å—æ•°: {sum(module_config.values())}")
                return True
            else:
                logger.warning("âš ï¸ æ¨¡å—é…ç½®å¯¹è±¡ä¸å¯ç”¨")
                return False
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¨¡å—é…ç½®å¤±è´¥: {e}")
            return False
    
    def get_module_status(self) -> Dict[str, Any]:
        """è·å–æ¨¡å—çŠ¶æ€"""
        try:
            if self.module_manager:
                status = self.module_manager.get_module_status()
                enabled_modules = self.module_manager.get_enabled_modules()
                
                return {
                    "module_status": status,
                    "enabled_modules": enabled_modules,
                    "enabled_count": len(enabled_modules),
                    "total_count": len(status),
                    "status": "âœ… çœŸå®æ¨¡å‹å¼•æ“æ­£å¸¸è¿è¡Œ",
                    "components_available": {
                        "torch": TORCH_AVAILABLE,
                        "bm25": BM25_AVAILABLE,
                        "embedding_model": self.components.get('embedding_model') is not None,
                        "generator": self.components.get('generator') is not None
                    }
                }
            else:
                return {
                    "module_status": {},
                    "enabled_modules": [],
                    "enabled_count": 0,
                    "total_count": 0,
                    "status": "âš ï¸ æ¨¡å—ç®¡ç†å™¨ä¸å¯ç”¨",
                    "components_available": {
                        "torch": TORCH_AVAILABLE,
                        "bm25": BM25_AVAILABLE
                    }
                }
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å—çŠ¶æ€å¤±è´¥: {e}")
            return {
                "module_status": {},
                "enabled_modules": [],
                "enabled_count": 0,
                "total_count": 0,
                "status": f"âŒ è·å–çŠ¶æ€å¤±è´¥: {e}",
                "components_available": {}
            }
