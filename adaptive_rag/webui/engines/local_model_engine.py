#!/usr/bin/env python3
"""
=== æœ¬åœ°æ¨¡å‹å¼•æ“ ===

ä½¿ç”¨ /root/autodl-tmp ç›®å½•ä¸‹çš„çœŸå®æ¨¡å‹å’Œæ•°æ®
"""

import logging
import time
import json
import os
import pickle
from typing import Dict, List, Any, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = logging.getLogger(__name__)

# å¯¼å…¥æ¨¡å—ç®¡ç†å™¨
try:
    from adaptive_rag.core.module_manager import ModuleManager
    from adaptive_rag.config import (
        create_config_from_yaml, ModuleToggleConfig, 
        FlexRAGIntegratedConfig, get_enabled_modules
    )
    MODULE_MANAGER_AVAILABLE = True
except ImportError:
    MODULE_MANAGER_AVAILABLE = False
    logger.warning("æ¨¡å—ç®¡ç†å™¨ä¸å¯ç”¨")

# å¯¼å…¥çœŸå®ç»„ä»¶
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorchç»„ä»¶ä¸å¯ç”¨")

try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    logger.warning("BM25ä¸å¯ç”¨")

try:
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learnä¸å¯ç”¨")


class LocalModelEngine:
    """æœ¬åœ°æ¨¡å‹å¼•æ“ - ä½¿ç”¨ /root/autodl-tmp ä¸‹çš„çœŸå®æ¨¡å‹å’Œæ•°æ®"""
    
    def __init__(self, config_path: str = "adaptive_rag/config/modular_config.yaml"):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å¼•æ“"""
        logger.info("ğŸš€ åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å¼•æ“...")
        
        self.config_path = config_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½é…ç½®
        self.load_config()
        
        # åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨
        self.initialize_module_manager()
        
        # åˆå§‹åŒ–çœŸå®ç»„ä»¶
        self.initialize_local_components()
        
        # åŠ è½½çœŸå®æ•°æ®
        self.load_real_data()
        
        logger.info("âœ… æœ¬åœ°æ¨¡å‹å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            if Path(self.config_path).exists():
                self.config = create_config_from_yaml(self.config_path, preset="performance_mode")
                logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            else:
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
                self.config = FlexRAGIntegratedConfig()
                self.config.modules = ModuleToggleConfig()
        except Exception as e:
            logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
            self.config = FlexRAGIntegratedConfig()
            self.config.modules = ModuleToggleConfig()
    
    def initialize_module_manager(self):
        """åˆå§‹åŒ–æ¨¡å—ç®¡ç†å™¨"""
        if MODULE_MANAGER_AVAILABLE:
            try:
                self.module_manager = ModuleManager(self.config)
                self.module_manager.initialize_modules()
                logger.info("âœ… æ¨¡å—ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ æ¨¡å—ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.module_manager = None
        else:
            self.module_manager = None
    
    def initialize_local_components(self):
        """åˆå§‹åŒ–æœ¬åœ°ç»„ä»¶"""
        self.components = {}
        
        # è·å–è·¯å¾„é…ç½®
        paths_config = getattr(self.config, 'paths', {})
        models_dir = paths_config.get('models_dir', '/root/autodl-tmp/models')
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if TORCH_AVAILABLE:
            try:
                logger.info("ğŸ“¥ åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹...")
                embedding_model_path = f"{models_dir}/e5-base-v2"

                if os.path.exists(embedding_model_path):
                    # ä¼˜å…ˆä½¿ç”¨SentenceTransformeråŠ è½½ï¼ˆé€‚åˆe5æ¨¡å‹ï¼‰
                    try:
                        from sentence_transformers import SentenceTransformer
                        self.components['embedding_model'] = SentenceTransformer(embedding_model_path)
                        logger.info(f"âœ… æœ¬åœ°åµŒå…¥æ¨¡å‹(SentenceTransformer)åŠ è½½æˆåŠŸ: {embedding_model_path}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ SentenceTransformeråŠ è½½å¤±è´¥: {e}")
                        # å°è¯•ä½¿ç”¨æ ‡å‡†TransformersåŠ è½½
                        try:
                            self.components['embedding_tokenizer'] = AutoTokenizer.from_pretrained(embedding_model_path)
                            self.components['embedding_model'] = AutoModel.from_pretrained(
                                embedding_model_path,
                                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                trust_remote_code=True
                            ).to(self.device)
                            logger.info(f"âœ… æœ¬åœ°åµŒå…¥æ¨¡å‹(Transformers)åŠ è½½æˆåŠŸ: {embedding_model_path}")
                        except Exception as e2:
                            logger.error(f"âŒ TransformersåŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                            self.components['embedding_model'] = None
                else:
                    # å›é€€åˆ°åœ¨çº¿æ¨¡å‹
                    logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {embedding_model_path}")
                    try:
                        from sentence_transformers import SentenceTransformer
                        self.components['embedding_model'] = SentenceTransformer('intfloat/e5-base-v2')
                        logger.info("âœ… åœ¨çº¿åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"âŒ åœ¨çº¿åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        self.components['embedding_model'] = None

            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.components['embedding_model'] = None
        
        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        if TORCH_AVAILABLE:
            try:
                logger.info("ğŸ“¥ åŠ è½½æœ¬åœ°é‡æ’åºæ¨¡å‹...")
                reranker_model_path = f"{models_dir}/bge-reranker-base"
                
                if os.path.exists(reranker_model_path):
                    self.components['reranker_tokenizer'] = AutoTokenizer.from_pretrained(reranker_model_path)
                    self.components['reranker_model'] = AutoModelForCausalLM.from_pretrained(
                        reranker_model_path,
                        torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                    ).to(self.device)
                    logger.info(f"âœ… æœ¬åœ°é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: {reranker_model_path}")
                else:
                    logger.warning(f"âš ï¸ æœ¬åœ°é‡æ’åºæ¨¡å‹ä¸å­˜åœ¨: {reranker_model_path}")
                    self.components['reranker_model'] = None
                    
            except Exception as e:
                logger.error(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.components['reranker_model'] = None
        
        # åˆå§‹åŒ–ç”Ÿæˆæ¨¡å‹
        if TORCH_AVAILABLE:
            try:
                logger.info("ğŸ“¥ åŠ è½½æœ¬åœ°ç”Ÿæˆæ¨¡å‹...")
                generator_model_path = f"{models_dir}/Qwen2.5-1.5B-Instruct"
                
                if os.path.exists(generator_model_path):
                    try:
                        self.components['generator_tokenizer'] = AutoTokenizer.from_pretrained(
                            generator_model_path,
                            trust_remote_code=True
                        )
                        self.components['generator_model'] = AutoModelForCausalLM.from_pretrained(
                            generator_model_path,
                            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                            device_map="auto" if self.device == "cuda" else None,
                            trust_remote_code=True
                        )

                        # è®¾ç½®pad_token
                        if self.components['generator_tokenizer'].pad_token is None:
                            self.components['generator_tokenizer'].pad_token = self.components['generator_tokenizer'].eos_token

                        logger.info(f"âœ… æœ¬åœ°ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ: {generator_model_path}")
                    except Exception as e:
                        logger.error(f"âŒ æœ¬åœ°ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        self.components['generator_model'] = None
                else:
                    logger.warning(f"âš ï¸ æœ¬åœ°ç”Ÿæˆæ¨¡å‹ä¸å­˜åœ¨: {generator_model_path}")
                    # å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹
                    backup_models = [
                        f"{models_dir}/Qwen1.5-1.8B-Chat",
                        f"{models_dir}/Qwen2.5-7B-Instruct"
                    ]

                    for backup_path in backup_models:
                        if os.path.exists(backup_path):
                            try:
                                self.components['generator_tokenizer'] = AutoTokenizer.from_pretrained(
                                    backup_path,
                                    trust_remote_code=True
                                )
                                self.components['generator_model'] = AutoModelForCausalLM.from_pretrained(
                                    backup_path,
                                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                                    device_map="auto" if self.device == "cuda" else None,
                                    trust_remote_code=True
                                )

                                if self.components['generator_tokenizer'].pad_token is None:
                                    self.components['generator_tokenizer'].pad_token = self.components['generator_tokenizer'].eos_token

                                logger.info(f"âœ… å¤‡ç”¨ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ: {backup_path}")
                                break
                            except Exception as e:
                                logger.warning(f"âš ï¸ å¤‡ç”¨æ¨¡å‹åŠ è½½å¤±è´¥: {backup_path}, {e}")
                                continue
                    else:
                        self.components['generator_model'] = None
                        
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.components['generator_model'] = None
    
    def load_real_data(self):
        """åŠ è½½çœŸå®æ•°æ®"""
        try:
            # è·å–æ•°æ®é…ç½®
            data_config = getattr(self.config, 'data', {})
            corpus_path = data_config.get('corpus_path', '/root/autodl-tmp/flashrag_real_data/hotpotqa_dev.jsonl')
            cache_dir = data_config.get('cache_dir', '/root/autodl-tmp/flashrag_real_data/cache')
            
            logger.info(f"ğŸ“¥ åŠ è½½çœŸå®æ•°æ®: {corpus_path}")
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs(cache_dir, exist_ok=True)
            
            # åŠ è½½æ•°æ®é›†
            self.documents = []
            if os.path.exists(corpus_path):
                with open(corpus_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= 1000:  # é™åˆ¶æ•°æ®é‡ä»¥èŠ‚çœå†…å­˜
                            break
                        try:
                            data = json.loads(line.strip())
                            # é€‚é…ä¸åŒæ•°æ®æ ¼å¼
                            if 'question' in data and 'answer' in data:
                                # HotpotQAæ ¼å¼
                                doc = {
                                    "id": f"doc_{i}",
                                    "title": data.get('question', '')[:100],
                                    "content": f"é—®é¢˜: {data.get('question', '')}\nç­”æ¡ˆ: {data.get('answer', '')}",
                                    "metadata": data
                                }
                            elif 'query' in data and 'golden_answers' in data:
                                # TriviaQAæ ¼å¼
                                doc = {
                                    "id": f"doc_{i}",
                                    "title": data.get('query', '')[:100],
                                    "content": f"é—®é¢˜: {data.get('query', '')}\nç­”æ¡ˆ: {', '.join(data.get('golden_answers', []))}",
                                    "metadata": data
                                }
                            else:
                                # é€šç”¨æ ¼å¼
                                doc = {
                                    "id": f"doc_{i}",
                                    "title": str(data)[:100],
                                    "content": str(data),
                                    "metadata": data
                                }
                            self.documents.append(doc)
                        except Exception as e:
                            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®è¡Œ {i}: {e}")
                            continue
                
                logger.info(f"âœ… åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
            else:
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {corpus_path}")
                # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
                self.documents = self._create_sample_documents()
            
            # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
            if BM25_AVAILABLE and self.documents:
                try:
                    bm25_cache_path = os.path.join(cache_dir, "bm25_index.pkl")
                    
                    if os.path.exists(bm25_cache_path):
                        # åŠ è½½ç¼“å­˜çš„BM25ç´¢å¼•
                        with open(bm25_cache_path, 'rb') as f:
                            self.components['bm25'] = pickle.load(f)
                        logger.info("âœ… BM25ç´¢å¼•ä»ç¼“å­˜åŠ è½½æˆåŠŸ")
                    else:
                        # åˆ›å»ºæ–°çš„BM25ç´¢å¼•
                        doc_texts = [doc['content'] for doc in self.documents]
                        tokenized_docs = [text.split() for text in doc_texts]
                        self.components['bm25'] = BM25Okapi(tokenized_docs)
                        
                        # ä¿å­˜åˆ°ç¼“å­˜
                        with open(bm25_cache_path, 'wb') as f:
                            pickle.dump(self.components['bm25'], f)
                        logger.info("âœ… BM25ç´¢å¼•åˆ›å»ºå¹¶ç¼“å­˜æˆåŠŸ")
                        
                except Exception as e:
                    logger.error(f"âŒ BM25ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.components['bm25'] = None
            
            # é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
            if self.components.get('embedding_model') and self.documents:
                try:
                    embeddings_cache_path = os.path.join(cache_dir, "document_embeddings.npy")
                    
                    if os.path.exists(embeddings_cache_path):
                        # åŠ è½½ç¼“å­˜çš„åµŒå…¥
                        self.document_embeddings = np.load(embeddings_cache_path)
                        logger.info("âœ… æ–‡æ¡£åµŒå…¥ä»ç¼“å­˜åŠ è½½æˆåŠŸ")
                    else:
                        # è®¡ç®—æ–°çš„åµŒå…¥
                        doc_texts = [doc['content'] for doc in self.documents]
                        if hasattr(self.components['embedding_model'], 'encode'):
                            # SentenceTransformeræ¨¡å‹
                            self.document_embeddings = self.components['embedding_model'].encode(doc_texts)
                        else:
                            # Transformersæ¨¡å‹
                            self.document_embeddings = self._compute_embeddings_with_transformers(doc_texts)
                        
                        # ä¿å­˜åˆ°ç¼“å­˜
                        np.save(embeddings_cache_path, self.document_embeddings)
                        logger.info("âœ… æ–‡æ¡£åµŒå…¥è®¡ç®—å¹¶ç¼“å­˜æˆåŠŸ")
                        
                except Exception as e:
                    logger.error(f"âŒ æ–‡æ¡£åµŒå…¥è®¡ç®—å¤±è´¥: {e}")
                    self.document_embeddings = None
                    
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self.documents = self._create_sample_documents()
            self.document_embeddings = None
    
    def _create_sample_documents(self):
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        return [
            {
                "id": "sample_1",
                "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
                "content": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
                "metadata": {}
            },
            {
                "id": "sample_2",
                "title": "æœºå™¨å­¦ä¹ æ¦‚è¿°", 
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
                "metadata": {}
            },
            {
                "id": "sample_3",
                "title": "æ·±åº¦å­¦ä¹ æŠ€æœ¯",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
                "metadata": {}
            }
        ]
    
    def _compute_embeddings_with_transformers(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨Transformersæ¨¡å‹è®¡ç®—åµŒå…¥"""
        embeddings = []
        tokenizer = self.components['embedding_tokenizer']
        model = self.components['embedding_model']
        
        for text in texts:
            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = model(**inputs)
                # ä½¿ç”¨[CLS]æ ‡è®°çš„åµŒå…¥æˆ–å¹³å‡æ± åŒ–
                embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                embeddings.append(embedding[0])
        
        return np.array(embeddings)

    def process_query_with_modules(self, query: str) -> Dict[str, Any]:
        """æ ¹æ®å¯ç”¨çš„æ¨¡å—å¤„ç†æŸ¥è¯¢"""
        start_time = time.time()
        result = {
            "query": query,
            "steps": [],
            "retrieval_results": [],
            "reranked_results": [],
            "generated_answer": "",
            "total_time": 0,
            "module_usage": {}
        }

        logger.info(f"ğŸ” å¤„ç†æŸ¥è¯¢: {query}")

        # 1. ä»»åŠ¡åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("task_decomposer"):
            logger.info("ğŸ“‹ æ‰§è¡Œä»»åŠ¡åˆ†è§£...")
            subtasks = self.real_task_decomposition(query)
            result["steps"].append(f"ä»»åŠ¡åˆ†è§£: è¯†åˆ«åˆ° {len(subtasks)} ä¸ªå­ä»»åŠ¡")
            result["module_usage"]["task_decomposer"] = True
        else:
            subtasks = [query]
            result["module_usage"]["task_decomposer"] = False

        # 2. æ£€ç´¢é˜¶æ®µ
        all_retrieved_docs = []

        # å…³é”®è¯æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("keyword_retriever"):
            logger.info("ğŸ” æ‰§è¡Œå…³é”®è¯æ£€ç´¢...")
            keyword_docs = self.real_keyword_retrieval(query)
            all_retrieved_docs.extend(keyword_docs)
            result["steps"].append(f"å…³é”®è¯æ£€ç´¢: æ‰¾åˆ° {len(keyword_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["keyword_retriever"] = True
        else:
            result["module_usage"]["keyword_retriever"] = False

        # å¯†é›†æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("dense_retriever"):
            logger.info("ğŸ§  æ‰§è¡Œå¯†é›†æ£€ç´¢...")
            dense_docs = self.real_dense_retrieval(query)
            all_retrieved_docs.extend(dense_docs)
            result["steps"].append(f"å¯†é›†æ£€ç´¢: æ‰¾åˆ° {len(dense_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["dense_retriever"] = True
        else:
            result["module_usage"]["dense_retriever"] = False

        # ç½‘ç»œæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("web_retriever"):
            logger.info("ğŸŒ æ‰§è¡Œç½‘ç»œæ£€ç´¢...")
            web_docs = self.simulate_web_retrieval(query)
            all_retrieved_docs.extend(web_docs)
            result["steps"].append(f"ç½‘ç»œæ£€ç´¢: æ‰¾åˆ° {len(web_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["web_retriever"] = True
        else:
            result["module_usage"]["web_retriever"] = False

        result["retrieval_results"] = all_retrieved_docs

        # 3. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("context_reranker") and all_retrieved_docs:
            logger.info("ğŸ¯ æ‰§è¡Œä¸Šä¸‹æ–‡é‡æ’åº...")
            reranked_docs = self.real_reranking(query, all_retrieved_docs)
            result["reranked_results"] = reranked_docs
            result["steps"].append(f"é‡æ’åº: é‡æ–°æ’åº {len(reranked_docs)} ä¸ªæ–‡æ¡£")
            result["module_usage"]["context_reranker"] = True
        else:
            result["reranked_results"] = all_retrieved_docs[:5]
            result["module_usage"]["context_reranker"] = False

        # 4. ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.is_module_enabled("adaptive_generator"):
            logger.info("âœ¨ æ‰§è¡Œè‡ªé€‚åº”ç”Ÿæˆ...")
            answer = self.real_generation(query, result["reranked_results"])
            result["generated_answer"] = answer
            result["steps"].append("è‡ªé€‚åº”ç”Ÿæˆ: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            result["module_usage"]["adaptive_generator"] = True
        else:
            contexts = [doc.get('content', '') for doc in result["reranked_results"][:3]]
            result["generated_answer"] = f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š{' '.join(contexts[:200])}..."
            result["module_usage"]["adaptive_generator"] = False

        result["total_time"] = time.time() - start_time
        logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶ {result['total_time']:.2f}s")

        return result

    def real_task_decomposition(self, query: str) -> List[str]:
        """çœŸå®çš„ä»»åŠ¡åˆ†è§£"""
        if "ä»€ä¹ˆæ˜¯" in query or "ä»‹ç»" in query:
            return [f"å®šä¹‰: {query}", f"ç‰¹ç‚¹: {query}", f"åº”ç”¨: {query}"]
        elif "å¦‚ä½•" in query or "æ€ä¹ˆ" in query:
            return [f"æ–¹æ³•: {query}", f"æ­¥éª¤: {query}", f"æ³¨æ„äº‹é¡¹: {query}"]
        else:
            return [query]

    def real_keyword_retrieval(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """çœŸå®çš„å…³é”®è¯æ£€ç´¢"""
        if not self.components.get('bm25') or not self.documents:
            return []

        try:
            tokenized_query = query.split()
            scores = self.components['bm25'].get_scores(tokenized_query)

            top_indices = scores.argsort()[-top_k:][::-1]

            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['score'] = float(scores[idx])
                    doc['retrieval_type'] = 'keyword'
                    results.append(doc)

            return results
        except Exception as e:
            logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []

    def real_dense_retrieval(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """çœŸå®çš„å¯†é›†æ£€ç´¢"""
        if not self.components.get('embedding_model') or self.document_embeddings is None:
            return []

        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            if hasattr(self.components['embedding_model'], 'encode'):
                # SentenceTransformeræ¨¡å‹
                query_embedding = self.components['embedding_model'].encode([query])
            else:
                # Transformersæ¨¡å‹
                query_embedding = self._compute_embeddings_with_transformers([query])

            # è®¡ç®—ç›¸ä¼¼åº¦
            if SKLEARN_AVAILABLE:
                similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]
            else:
                # æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                query_norm = np.linalg.norm(query_embedding)
                doc_norms = np.linalg.norm(self.document_embeddings, axis=1)
                similarities = np.dot(query_embedding[0], self.document_embeddings.T) / (query_norm * doc_norms)

            top_indices = similarities.argsort()[-top_k:][::-1]

            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['score'] = float(similarities[idx])
                    doc['retrieval_type'] = 'dense'
                    results.append(doc)

            return results
        except Exception as e:
            logger.error(f"å¯†é›†æ£€ç´¢å¤±è´¥: {e}")
            return []

    def simulate_web_retrieval(self, query: str, top_k: int = 2) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿç½‘ç»œæ£€ç´¢"""
        web_results = [
            {
                "id": f"web_{i}",
                "title": f"ç½‘ç»œæœç´¢ç»“æœ {i}: {query}",
                "content": f"è¿™æ˜¯æ¥è‡ªç½‘ç»œçš„å…³äº'{query}'çš„æœç´¢ç»“æœ {i}ã€‚åŒ…å«æœ€æ–°çš„ç›¸å…³ä¿¡æ¯ã€‚",
                "score": 0.8 - i * 0.1,
                "retrieval_type": "web",
                "url": f"https://example.com/result_{i}"
            }
            for i in range(top_k)
        ]
        return web_results

    def real_reranking(self, query: str, documents: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        """çœŸå®çš„é‡æ’åº"""
        if not documents:
            return []

        try:
            # ä½¿ç”¨åŸºäºåŒ¹é…åº¦çš„é‡æ’åº
            return self._rerank_with_matching(query, documents, top_k)

        except Exception as e:
            logger.error(f"é‡æ’åºå¤±è´¥: {e}")
            return documents[:top_k]

    def _rerank_with_matching(self, query: str, documents: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """åŸºäºåŒ¹é…åº¦çš„é‡æ’åº"""
        def rerank_score(doc):
            content = doc.get('content', '')
            title = doc.get('title', '')

            query_words = query.lower().split()
            content_lower = content.lower()
            title_lower = title.lower()

            content_matches = sum(1 for word in query_words if word in content_lower)
            title_matches = sum(1 for word in query_words if word in title_lower)

            original_score = doc.get('score', 0)
            match_score = (title_matches * 2 + content_matches) / len(query_words)

            return original_score * 0.7 + match_score * 0.3

        reranked_docs = sorted(documents, key=rerank_score, reverse=True)

        for i, doc in enumerate(reranked_docs[:top_k]):
            doc['rerank_score'] = rerank_score(doc)
            doc['rerank_position'] = i + 1

        return reranked_docs[:top_k]

    def real_generation(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """çœŸå®çš„ç”Ÿæˆ"""
        if not self.components.get('generator_model') or not self.components.get('generator_tokenizer'):
            # å›é€€åˆ°ç®€å•æ‹¼æ¥
            if contexts:
                context_text = " ".join([ctx.get('content', '')[:100] for ctx in contexts[:3]])
                return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå…³äº'{query}'ï¼š{context_text}..."
            else:
                return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ã€‚"

        try:
            # æ„å»ºæç¤ºè¯
            context_text = "\n".join([f"- {ctx.get('content', '')[:200]}" for ctx in contexts[:3]])
            prompt = f"åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n{context_text}\n\né—®é¢˜ï¼š{query}\nå›ç­”ï¼š"

            # ç”Ÿæˆå›ç­”
            tokenizer = self.components['generator_tokenizer']
            model = self.components['generator_model']

            inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True)
            if self.device == "cuda":
                inputs = inputs.to(self.device)

            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # æå–å›ç­”éƒ¨åˆ†
            if "å›ç­”ï¼š" in generated_text:
                answer = generated_text.split("å›ç­”ï¼š")[-1].strip()
            else:
                answer = generated_text[len(prompt):].strip()

            return answer if answer else "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆåˆé€‚çš„å›ç­”ã€‚"

        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            if contexts:
                context_text = " ".join([ctx.get('content', '')[:100] for ctx in contexts[:3]])
                return f"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œå…³äº'{query}'ï¼š{context_text}..."
            else:
                return f"æŠ±æ­‰ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def is_module_enabled(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ç”¨"""
        if self.module_manager:
            return self.module_manager.is_module_enabled(module_name)
        return True

    def update_module_config(self, module_config: Dict[str, bool]) -> bool:
        """æ›´æ–°æ¨¡å—é…ç½®"""
        try:
            if self.config and hasattr(self.config, 'modules'):
                for module_name, enabled in module_config.items():
                    if hasattr(self.config.modules, module_name):
                        setattr(self.config.modules, module_name, enabled)

                if self.module_manager:
                    self.module_manager = ModuleManager(self.config)
                    self.module_manager.initialize_modules()

                logger.info(f"âœ… æ¨¡å—é…ç½®å·²æ›´æ–°ï¼Œå¯ç”¨æ¨¡å—æ•°: {sum(module_config.values())}")
                return True
            else:
                logger.warning("âš ï¸ æ¨¡å—é…ç½®å¯¹è±¡ä¸å¯ç”¨")
                return False
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¨¡å—é…ç½®å¤±è´¥: {e}")
            return False

    def get_module_status(self) -> Dict[str, Any]:
        """è·å–æ¨¡å—çŠ¶æ€"""
        try:
            if self.module_manager:
                status = self.module_manager.get_module_status()
                enabled_modules = self.module_manager.get_enabled_modules()

                return {
                    "module_status": status,
                    "enabled_modules": enabled_modules,
                    "enabled_count": len(enabled_modules),
                    "total_count": len(status),
                    "status": "âœ… æœ¬åœ°æ¨¡å‹å¼•æ“æ­£å¸¸è¿è¡Œ",
                    "components_available": {
                        "torch": TORCH_AVAILABLE,
                        "bm25": BM25_AVAILABLE,
                        "sklearn": SKLEARN_AVAILABLE,
                        "embedding_model": self.components.get('embedding_model') is not None,
                        "generator_model": self.components.get('generator_model') is not None,
                        "reranker_model": self.components.get('reranker_model') is not None,
                        "documents_count": len(self.documents) if hasattr(self, 'documents') else 0
                    }
                }
            else:
                return {
                    "module_status": {},
                    "enabled_modules": [],
                    "enabled_count": 0,
                    "total_count": 0,
                    "status": "âš ï¸ æ¨¡å—ç®¡ç†å™¨ä¸å¯ç”¨",
                    "components_available": {
                        "torch": TORCH_AVAILABLE,
                        "bm25": BM25_AVAILABLE,
                        "sklearn": SKLEARN_AVAILABLE
                    }
                }
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å—çŠ¶æ€å¤±è´¥: {e}")
            return {
                "module_status": {},
                "enabled_modules": [],
                "enabled_count": 0,
                "total_count": 0,
                "status": f"âŒ è·å–çŠ¶æ€å¤±è´¥: {e}",
                "components_available": {}
            }

    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        try:
            import psutil
            import torch

            # CPUå’Œå†…å­˜ä¿¡æ¯
            cpu_percent = psutil.cpu_percent()
            memory = psutil.virtual_memory()

            # GPUä¿¡æ¯
            gpu_info = {}
            if torch.cuda.is_available():
                gpu_info = {
                    "gpu_available": True,
                    "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**3,  # GB
                    "gpu_memory_reserved": torch.cuda.memory_reserved() / 1024**3,   # GB
                    "gpu_device_name": torch.cuda.get_device_name()
                }
            else:
                gpu_info = {"gpu_available": False}

            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_used_gb": memory.used / 1024**3,
                "memory_total_gb": memory.total / 1024**3,
                "documents_loaded": len(self.documents) if hasattr(self, 'documents') else 0,
                "components_loaded": sum(1 for comp in self.components.values() if comp is not None),
                **gpu_info
            }
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return {
                "cpu_percent": 0,
                "memory_percent": 0,
                "error": str(e)
            }

    def get_resource_usage(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æƒ…å†µ"""
        return self.get_performance_metrics()

    def process_query(self, query: str) -> Dict[str, Any]:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šå¤„ç†æŸ¥è¯¢"""
        return self.process_query_with_modules(query)

    def get_current_module_config(self) -> Dict[str, bool]:
        """è·å–å½“å‰æ¨¡å—é…ç½®"""
        try:
            if self.config and hasattr(self.config, 'modules'):
                return {
                    name: getattr(self.config.modules, name, False)
                    for name in dir(self.config.modules)
                    if not name.startswith('_')
                }
            else:
                return {}
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å—é…ç½®å¤±è´¥: {e}")
            return {}
