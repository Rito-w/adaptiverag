#!/usr/bin/env python3
"""
=== æ™ºèƒ½è‡ªé€‚åº” RAG é…ç½®æ¨¡å— ===

æ·±åº¦é›†æˆ FlexRAG ç»„ä»¶çš„é…ç½®ç³»ç»Ÿ
"""

import os
import yaml
import random
import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field

# å¯¼å…¥ FlexRAG ç»„ä»¶é…ç½®
try:
    from flexrag.retriever import RetrieverConfig, RETRIEVERS
    from flexrag.ranker import RankerConfig, RANKERS
    from flexrag.models import GeneratorConfig, EncoderConfig, GENERATORS, ENCODERS
    FLEXRAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ FlexRAG æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–é…ç½®")
    FLEXRAG_AVAILABLE = False


@dataclass
class ModuleToggleConfig:
    """æ¨¡å—å¼€å…³é…ç½® - ç²¾ç»†æ§åˆ¶æ¯ä¸ªæ¨¡å—çš„å¯ç”¨çŠ¶æ€"""

    # === æ ¸å¿ƒå¤„ç†æ¨¡å— ===
    task_decomposer: bool = True              # ä»»åŠ¡åˆ†è§£å™¨
    retrieval_planner: bool = True            # æ£€ç´¢è§„åˆ’å™¨
    multi_retriever: bool = True              # å¤šé‡æ£€ç´¢ç³»ç»Ÿ
    context_reranker: bool = True             # ä¸Šä¸‹æ–‡é‡æ’å™¨
    adaptive_generator: bool = True           # è‡ªé€‚åº”ç”Ÿæˆå™¨

    # === æ™ºèƒ½åˆ†ææ¨¡å— ===
    query_analyzer: bool = True               # æŸ¥è¯¢åˆ†æå™¨
    strategy_router: bool = True              # ç­–ç•¥è·¯ç”±å™¨
    performance_optimizer: bool = True        # æ€§èƒ½ä¼˜åŒ–å™¨
    intelligent_strategy_learner: bool = False # æ™ºèƒ½ç­–ç•¥å­¦ä¹ å™¨ï¼ˆå®éªŒæ€§ï¼‰
    multi_dimensional_optimizer: bool = False # å¤šç»´åº¦ä¼˜åŒ–å™¨ï¼ˆå®éªŒæ€§ï¼‰
    resource_aware_optimizer: bool = False    # èµ„æºæ„ŸçŸ¥ä¼˜åŒ–å™¨ï¼ˆå®éªŒæ€§ï¼‰

    # === æ£€ç´¢å™¨æ¨¡å— ===
    keyword_retriever: bool = True            # å…³é”®è¯æ£€ç´¢å™¨
    dense_retriever: bool = True              # å¯†é›†æ£€ç´¢å™¨
    web_retriever: bool = False               # ç½‘ç»œæ£€ç´¢å™¨ï¼ˆéœ€è¦APIï¼‰
    hybrid_retriever: bool = True             # æ··åˆæ£€ç´¢å™¨

    # === é‡æ’åºæ¨¡å— ===
    cross_encoder_ranker: bool = True         # äº¤å‰ç¼–ç å™¨é‡æ’
    colbert_ranker: bool = False              # ColBERTé‡æ’ï¼ˆéœ€è¦æ¨¡å‹ï¼‰
    gpt_ranker: bool = False                  # GPTé‡æ’ï¼ˆéœ€è¦APIï¼‰

    # === ç”Ÿæˆå™¨æ¨¡å— ===
    template_generator: bool = True           # æ¨¡æ¿ç”Ÿæˆå™¨
    freeform_generator: bool = True           # è‡ªç”±å½¢å¼ç”Ÿæˆå™¨
    dialogue_generator: bool = False          # å¯¹è¯ç”Ÿæˆå™¨ï¼ˆå®éªŒæ€§ï¼‰

    # === è¯„ä¼°æ¨¡å— ===
    fact_verification: bool = False           # äº‹å®éªŒè¯ï¼ˆå®éªŒæ€§ï¼‰
    confidence_estimation: bool = True        # ç½®ä¿¡åº¦ä¼°è®¡
    result_analyzer: bool = True              # ç»“æœåˆ†æå™¨

    # === ç¼“å­˜æ¨¡å— ===
    semantic_cache: bool = True               # è¯­ä¹‰ç¼“å­˜
    predictive_cache: bool = False            # é¢„æµ‹æ€§ç¼“å­˜ï¼ˆå®éªŒæ€§ï¼‰

    # === ç”¨æˆ·ä½“éªŒæ¨¡å— ===
    personalization: bool = False             # ä¸ªæ€§åŒ–ï¼ˆå®éªŒæ€§ï¼‰
    multimodal_support: bool = False          # å¤šæ¨¡æ€æ”¯æŒï¼ˆå®éªŒæ€§ï¼‰

    # === è°ƒè¯•å’Œç›‘æ§æ¨¡å— ===
    debug_mode: bool = False                  # è°ƒè¯•æ¨¡å¼
    performance_monitoring: bool = True       # æ€§èƒ½ç›‘æ§
    logging_enhanced: bool = True             # å¢å¼ºæ—¥å¿—


@dataclass
class SubTaskConfig:
    """å­ä»»åŠ¡é…ç½®"""
    type: str = "factual"  # factual, semantic, temporal, comparative
    priority: float = 1.0
    max_decompose_depth: int = 3
    enable_entity_extraction: bool = True
    enable_temporal_analysis: bool = True


@dataclass
class RetrievalPlanConfig:
    """æ£€ç´¢ç­–ç•¥é…ç½®"""
    # æ£€ç´¢å™¨æƒé‡é…ç½®
    default_weights: Dict[str, float] = field(default_factory=lambda: {
        "keyword": 0.33,
        "dense": 0.33, 
        "web": 0.34
    })
    
    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"keyword": 0.7, "dense": 0.2, "web": 0.1},
        "semantic": {"keyword": 0.2, "dense": 0.7, "web": 0.1},
        "temporal": {"keyword": 0.3, "dense": 0.2, "web": 0.5},
        "comparative": {"keyword": 0.4, "dense": 0.4, "web": 0.2}
    })
    
    # æ¯ä¸ªæ£€ç´¢å™¨çš„ top-k é…ç½®
    top_k_config: Dict[str, int] = field(default_factory=lambda: {
        "keyword": 10,
        "dense": 10,
        "web": 5
    })


@dataclass
class RelevanceScoreConfig:
    """ç›¸å…³åº¦è¯„åˆ†é…ç½®"""
    # è¯„åˆ†ç»´åº¦æƒé‡
    score_weights: Dict[str, float] = field(default_factory=lambda: {
        "semantic": 0.3,
        "factual": 0.3,
        "temporal": 0.1,
        "entity": 0.2,
        "diversity": 0.1
    })
    
    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_score_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"semantic": 0.3, "factual": 0.4, "temporal": 0.1, "entity": 0.15, "diversity": 0.05},
        "semantic": {"semantic": 0.5, "factual": 0.2, "temporal": 0.1, "entity": 0.1, "diversity": 0.1},
        "temporal": {"semantic": 0.2, "factual": 0.2, "temporal": 0.4, "entity": 0.1, "diversity": 0.1},
        "comparative": {"semantic": 0.25, "factual": 0.25, "temporal": 0.1, "entity": 0.2, "diversity": 0.2}
    })


@dataclass
class ContextAggregationConfig:
    """ä¸Šä¸‹æ–‡èšåˆé…ç½®"""
    max_primary_contexts: int = 3
    max_supporting_contexts: int = 5
    max_background_contexts: int = 2
    
    # ç›¸å…³åº¦é˜ˆå€¼
    primary_threshold: float = 0.8
    supporting_threshold: float = 0.5
    
    # å»é‡é…ç½®
    similarity_threshold: float = 0.85
    enable_conflict_resolution: bool = True


# é¦–å…ˆå®šä¹‰åŸºç¡€é…ç½®ç±»ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
@dataclass
class AdaptiveRAGConfig:
    """æ™ºèƒ½è‡ªé€‚åº” RAG æ€»é…ç½®ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰"""

    # åŸºç¡€é…ç½®
    device: str = "cuda"
    batch_size: int = 4
    max_input_length: int = 2048

    # æ¨¡å‹è·¯å¾„é…ç½® - ä½¿ç”¨ adaptive_rag å†…éƒ¨ç›¸å¯¹è·¯å¾„
    model_paths: Dict[str, str] = field(default_factory=lambda: {
        "keyword_retriever": "bm25",
        "dense_retriever": "./adaptive_rag/models/e5-base-v2",
        "generator": "./adaptive_rag/models/qwen1.5-1.8b",
        "reranker": "./adaptive_rag/models/bge-reranker-base"
    })

    # æ•°æ®è·¯å¾„é…ç½® - ä½¿ç”¨ adaptive_rag å†…éƒ¨æ•°æ®
    data_paths: Dict[str, str] = field(default_factory=lambda: {
        "corpus_path": "./adaptive_rag/data/general_knowledge.jsonl",
        "index_path": "./adaptive_rag/data/e5_Flat.index"
    })

    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"keyword": 0.7, "dense": 0.2, "web": 0.1},
        "semantic": {"keyword": 0.2, "dense": 0.7, "web": 0.1},
        "temporal": {"keyword": 0.3, "dense": 0.2, "web": 0.5},
        "comparative": {"keyword": 0.4, "dense": 0.4, "web": 0.2}
    })

    # ç”Ÿæˆé…ç½®
    generation_params: Dict[str, Any] = field(default_factory=lambda: {
        "do_sample": True,
        "max_tokens": 256,
        "temperature": 0.7,
        "top_p": 0.9
    })

    # è¯„ä¼°é…ç½®
    metrics: List[str] = field(default_factory=lambda: ["em", "f1", "acc"])

    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    save_intermediate_results: bool = True
    log_level: str = "INFO"


@dataclass
class FlexRAGIntegratedConfig:
    """æ·±åº¦é›†æˆ FlexRAG çš„é…ç½®ç±»"""

    # === æ¨¡å—å¼€å…³é…ç½® ===
    modules: ModuleToggleConfig = field(default_factory=ModuleToggleConfig)

    # åŸºç¡€é…ç½®
    device: str = "cuda"
    batch_size: int = 4
    max_input_length: int = 2048

    # FlexRAG æ£€ç´¢å™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸»è¦ä½¿ç”¨æ¨¡æ‹Ÿå®ç°ï¼‰
    retriever_configs: Dict[str, Any] = field(default_factory=lambda: {
        "keyword_retriever": {
            "retriever_type": "mock",
            "config": {
                "retriever_path": "./adaptive_rag/data/keyword_index"
            }
        },
        "dense_retriever": {
            "retriever_type": "mock",
            "config": {
                "retriever_path": "./adaptive_rag/data/dense_index"
            }
        },
        "web_retriever": {
            "retriever_type": "mock",
            "config": {
                "search_engine": "google"
            }
        }
    })

    # FlexRAG é‡æ’åºå™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    ranker_configs: Dict[str, Any] = field(default_factory=lambda: {
        "cross_encoder": {
            "ranker_type": "mock",
            "config": {
                "model_name": "BAAI/bge-reranker-base",
                "reserve_num": 10
            }
        },
        "colbert": {
            "ranker_type": "mock",
            "config": {
                "model_name": "colbert-ir/colbertv2.0",
                "reserve_num": 10
            }
        }
    })

    # FlexRAG ç”Ÿæˆå™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    generator_configs: Dict[str, Any] = field(default_factory=lambda: {
        "main_generator": {
            "generator_type": "mock",
            "config": {
                "model_path": "./adaptive_rag/models/qwen1.5-1.8b",
                "model_type": "causal_lm"
            }
        },
        "openai_generator": {
            "generator_type": "mock",
            "config": {
                "model_name": "gpt-3.5-turbo",
                "api_key": "${OPENAI_API_KEY}"
            }
        }
    })

    # FlexRAG ç¼–ç å™¨é…ç½®
    encoder_configs: Dict[str, Any] = field(default_factory=lambda: {
        "dense_encoder": {
            "encoder_type": "sentence_transformer",
            "sentence_transformer_config": {
                "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                "device": "cuda"
            }
        },
        "openai_encoder": {
            "encoder_type": "openai",
            "openai_config": {
                "model_name": "text-embedding-ada-002",
                "api_key": "${OPENAI_API_KEY}"
            }
        }
    })
    
    # å­æ¨¡å—é…ç½®
    subtask_config: SubTaskConfig = field(default_factory=SubTaskConfig)
    retrieval_plan_config: RetrievalPlanConfig = field(default_factory=RetrievalPlanConfig)
    relevance_score_config: RelevanceScoreConfig = field(default_factory=RelevanceScoreConfig)
    context_aggregation_config: ContextAggregationConfig = field(default_factory=ContextAggregationConfig)
    
    # ç”Ÿæˆé…ç½®
    generation_params: Dict[str, Any] = field(default_factory=lambda: {
        "do_sample": True,
        "max_tokens": 256,
        "temperature": 0.7,
        "top_p": 0.9
    })
    
    # è¯„ä¼°é…ç½®
    metrics: List[str] = field(default_factory=lambda: ["em", "f1", "acc"])
    
    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    save_intermediate_results: bool = True
    log_level: str = "INFO"


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: Optional[str] = None, config_dict: Optional[Dict] = None):
        self.config = AdaptiveRAGConfig()
        
        # ä»æ–‡ä»¶åŠ è½½é…ç½®
        if config_file and os.path.exists(config_file):
            self.load_from_file(config_file)
        
        # ä»å­—å…¸æ›´æ–°é…ç½®
        if config_dict:
            self.update_from_dict(config_dict)
    
    def load_from_file(self, config_file: str):
        """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(config_file, 'r', encoding='utf-8') as f:
            file_config = yaml.safe_load(f)
        self.update_from_dict(file_config)
    
    def update_from_dict(self, config_dict: Dict):
        """ä»å­—å…¸æ›´æ–°é…ç½®"""
        for key, value in config_dict.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
    
    def save_to_file(self, config_file: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = self.to_dict()
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        def convert_dataclass(obj):
            if hasattr(obj, '__dataclass_fields__'):
                return {k: convert_dataclass(v) for k, v in obj.__dict__.items()}
            elif isinstance(obj, dict):
                return {k: convert_dataclass(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_dataclass(item) for item in obj]
            else:
                return obj
        
        return convert_dataclass(self.config)
    
    def get_config(self) -> AdaptiveRAGConfig:
        """è·å–é…ç½®å¯¹è±¡"""
        return self.config
    
    def __getitem__(self, key):
        """æ”¯æŒå­—å…¸å¼è®¿é—®"""
        return getattr(self.config, key)
    
    def __setitem__(self, key, value):
        """æ”¯æŒå­—å…¸å¼è®¾ç½®"""
        setattr(self.config, key, value)


def get_project_root() -> str:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_dir = os.path.dirname(os.path.abspath(__file__))  # adaptive_rag ç›®å½•
    return os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•

def resolve_path(path: str) -> str:
    """è§£æè·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
    if os.path.isabs(path):
        return path
    else:
        # ç›¸å¯¹è·¯å¾„åŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼Œè§„èŒƒåŒ–è·¯å¾„
        resolved = os.path.join(get_project_root(), path)
        return os.path.normpath(resolved)

def create_default_config() -> AdaptiveRAGConfig:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    config = AdaptiveRAGConfig()

    # è§£ææ¨¡å‹è·¯å¾„
    for key, path in config.model_paths.items():
        if key != "keyword_retriever":  # bm25 ä¸éœ€è¦è·¯å¾„è§£æ
            config.model_paths[key] = resolve_path(path)

    # è§£ææ•°æ®è·¯å¾„
    for key, path in config.data_paths.items():
        config.data_paths[key] = resolve_path(path)

    return config


def load_config(config_file: Optional[str] = None, config_dict: Optional[Dict] = None) -> AdaptiveRAGConfig:
    """åŠ è½½é…ç½®çš„ä¾¿æ·å‡½æ•°"""
    manager = ConfigManager(config_file, config_dict)
    return manager.get_config()


# ===== å®éªŒé…ç½®ç³»ç»Ÿ (å€Ÿé‰´ FlashRAG) =====

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½® - å€Ÿé‰´ FlashRAG çš„å®éªŒè®¾è®¡"""

    # ç¯å¢ƒè®¾ç½®
    data_dir: str = "./data/benchmarks"
    save_dir: str = "./experiments"
    gpu_id: str = "0"
    seed: int = 2024

    # æ•°æ®é›†è®¾ç½®
    dataset_name: str = "natural_questions"
    split: List[str] = field(default_factory=lambda: ["test"])
    test_sample_num: Optional[int] = None  # None è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
    random_sample: bool = False

    # ä¿å­˜è®¾ç½®
    save_intermediate_data: bool = True
    save_predictions: bool = True
    save_note: str = "adaptive_rag_experiment"

    # è¯„ä¼°è®¾ç½®
    metrics: List[str] = field(default_factory=lambda: [
        "exact_match", "f1_score", "rouge_l", "bert_score"
    ])
    compute_bert_score: bool = True

    # æ–¹æ³•è®¾ç½®
    method_name: str = "adaptive_rag"
    baseline_methods: List[str] = field(default_factory=lambda: [
        "naive_rag", "self_rag"
    ])


@dataclass
class DatasetConfig:
    """æ•°æ®é›†é…ç½® - å…¼å®¹ FlashRAG æ•°æ®æ ¼å¼"""

    # æ”¯æŒçš„æ•°æ®é›†æ˜ å°„ (FlashRAG æ ¼å¼)
    DATASET_MAPPING = {
        # å•è·³é—®ç­”
        "natural_questions": "nq",
        "trivia_qa": "trivia",
        "ms_marco": "msmarco",

        # å¤šè·³æ¨ç†
        "hotpot_qa": "hotpot",
        "2wiki_multihop": "2wiki",
        "musique": "musique",

        # å¯¹è¯é—®ç­”
        "quac": "quac",
        "coqa": "coqa",

        # å¼€æ”¾åŸŸé—®ç­”
        "web_questions": "webq",
        "entity_questions": "entityq"
    }

    # æ•°æ®é›†è·¯å¾„é…ç½®
    dataset_path: str = "./data/benchmarks"
    corpus_path: str = "./data/corpus/wiki_2021.jsonl"
    index_path: str = "./data/indexes"

    # æ•°æ®å¤„ç†é…ç½®
    max_context_length: int = 512
    max_question_length: int = 256
    max_answer_length: int = 128


# ç¤ºä¾‹é…ç½®æ–‡ä»¶å†…å®¹ (å€Ÿé‰´ FlashRAG æ ¼å¼)
EXAMPLE_CONFIG_YAML = """
# ===== AdaptiveRAG å®éªŒé…ç½® (å€Ÿé‰´ FlashRAG) =====

# ------------------------------------------------ç¯å¢ƒè®¾ç½®------------------------------------------------#
# æ•°æ®å’Œè¾“å‡ºç›®å½•è·¯å¾„
data_dir: "data/benchmarks/"
save_dir: "experiments/"

gpu_id: "0"
dataset_name: "natural_questions"  # data_dir ä¸­çš„æ•°æ®é›†åç§°
split: ["test"]  # è¦åŠ è½½çš„æ•°æ®é›†åˆ†å‰² (ä¾‹å¦‚ train,dev,test)

# æµ‹è¯•é‡‡æ ·é…ç½®
test_sample_num: ~  # æµ‹è¯•æ ·æœ¬æ•°é‡ (ä»…åœ¨ dev/test åˆ†å‰²ä¸­æœ‰æ•ˆ), å¦‚æœä¸º None, æµ‹è¯•æ‰€æœ‰æ ·æœ¬
random_sample: false  # æ˜¯å¦éšæœºé‡‡æ ·æµ‹è¯•æ ·æœ¬

# å¯é‡ç°æ€§ç§å­
seed: 2024

# æ˜¯å¦ä¿å­˜ä¸­é—´æ•°æ®
save_intermediate_data: true
save_predictions: true
save_note: "adaptive_rag_experiment"

# ------------------------------------------------æ£€ç´¢è®¾ç½®------------------------------------------------#
# æ£€ç´¢æ–¹æ³•é…ç½®
retrieval_method: "adaptive"  # æ£€ç´¢æ–¹æ³•åç§°æˆ–è·¯å¾„
retrieval_model_path: ~  # æ£€ç´¢æ¨¡å‹è·¯å¾„
index_path: ~  # å¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨è®¾ç½®
corpus_path: ~  # è¯­æ–™åº“è·¯å¾„ï¼Œ'.jsonl' æ ¼å¼å­˜å‚¨æ–‡æ¡£

# æ£€ç´¢å‚æ•°
retrieval_topk: 20  # æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
final_context_count: 5  # æœ€ç»ˆä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ•°é‡

# ------------------------------------------------ç”Ÿæˆè®¾ç½®------------------------------------------------#
# ç”Ÿæˆå™¨é…ç½®
generator_model: "qwen1.5-1.8b"  # ç”Ÿæˆå™¨æ¨¡å‹åç§°
generator_model_path: ~  # ç”Ÿæˆå™¨æ¨¡å‹è·¯å¾„
framework: "hf"  # ä½¿ç”¨çš„æ¡†æ¶ (hf/vllm)

# ç”Ÿæˆå‚æ•°
generation_params:
  max_tokens: 256
  temperature: 0.1
  top_p: 0.9
  do_sample: false

# ------------------------------------------------è¯„ä¼°è®¾ç½®------------------------------------------------#
# è¯„ä¼°æŒ‡æ ‡
metrics: ["exact_match", "f1_score", "rouge_l", "bert_score"]
compute_bert_score: true

# ------------------------------------------------AdaptiveRAG ç‰¹å®šè®¾ç½®------------------------------------------------#
# è‡ªé€‚åº”æ£€ç´¢ç­–ç•¥
adaptive_retrieval:
  enable_task_decomposition: true
  enable_strategy_planning: true
  enable_multi_retriever: true
  enable_reranking: true

# ä»»åŠ¡åˆ†è§£é…ç½®
task_decomposition:
  max_subtasks: 5
  decomposition_threshold: 0.7

# ç­–ç•¥è§„åˆ’é…ç½®
strategy_planning:
  task_specific_weights:
    factual:
      keyword: 0.7
      dense: 0.2
      web: 0.1
    semantic:
      keyword: 0.2
      dense: 0.7
      web: 0.1
    multi_hop:
      keyword: 0.3
      dense: 0.5
      web: 0.2

# é‡æ’åºé…ç½®
reranking:
  reranker_model: "bge-reranker-base"
  reranker_model_path: ~
  rerank_topk: 10

# è°ƒè¯•é…ç½®
debug_mode: false
log_level: "INFO"
"""


# ===== é…ç½®åŠ è½½å™¨ (å€Ÿé‰´ FlashRAG Config ç±») =====

class AdaptiveRAGConfig:
    """AdaptiveRAG é…ç½®åŠ è½½å™¨ - å€Ÿé‰´ FlashRAG çš„ Config ç±»è®¾è®¡"""

    def __init__(self, config_file_path=None, config_dict=None):
        """
        åˆå§‹åŒ–é…ç½®

        Args:
            config_file_path: YAML é…ç½®æ–‡ä»¶è·¯å¾„
            config_dict: é…ç½®å­—å…¸ (ä¼˜å…ˆçº§é«˜äºæ–‡ä»¶)
        """
        if config_dict is None:
            config_dict = {}

        # åŠ è½½é…ç½®
        self.file_config = self._load_file_config(config_file_path)
        self.variable_config = config_dict
        self.external_config = self._merge_external_config()
        self.internal_config = self._get_internal_config()
        self.final_config = self._get_final_config()

        # éªŒè¯å’Œè®¾ç½®
        self._check_final_config()
        self._set_additional_keys()
        self._init_device()
        self._set_seed()
        self._prepare_directories()

    def _load_file_config(self, config_file_path):
        """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
        if config_file_path is None:
            return {}

        if not os.path.exists(config_file_path):
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file_path}")
            return {}

        with open(config_file_path, 'r', encoding='utf-8') as f:
            try:
                config = yaml.safe_load(f)
                return config if config is not None else {}
            except yaml.YAMLError as e:
                print(f"âŒ YAML é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
                return {}

    def _merge_external_config(self):
        """åˆå¹¶å¤–éƒ¨é…ç½®"""
        external_config = {}
        external_config.update(self.file_config)
        external_config.update(self.variable_config)  # å˜é‡é…ç½®ä¼˜å…ˆçº§æ›´é«˜
        return external_config

    def _get_internal_config(self):
        """è·å–å†…éƒ¨é»˜è®¤é…ç½®"""
        return {
            # åŸºç¡€è®¾ç½®
            "device": "cuda",
            "seed": 2024,
            "batch_size": 4,

            # è·¯å¾„è®¾ç½®
            "data_dir": "./data/benchmarks",
            "save_dir": "./experiments",
            "corpus_path": "./data/corpus/wiki_2021.jsonl",
            "index_path": "./data/indexes",

            # æ•°æ®é›†è®¾ç½®
            "dataset_name": "natural_questions",
            "split": ["test"],
            "test_sample_num": None,
            "random_sample": False,

            # æ£€ç´¢è®¾ç½®
            "retrieval_method": "adaptive",
            "retrieval_topk": 20,
            "final_context_count": 5,

            # ç”Ÿæˆè®¾ç½®
            "generator_model": "qwen1.5-1.8b",
            "framework": "hf",
            "generation_params": {
                "max_tokens": 256,
                "temperature": 0.1,
                "top_p": 0.9,
                "do_sample": False
            },

            # è¯„ä¼°è®¾ç½®
            "metrics": ["exact_match", "f1_score", "rouge_l"],
            "compute_bert_score": True,

            # ä¿å­˜è®¾ç½®
            "save_intermediate_data": True,
            "save_predictions": True,
            "save_note": "adaptive_rag_experiment",

            # AdaptiveRAG ç‰¹å®šè®¾ç½®
            "adaptive_retrieval": {
                "enable_task_decomposition": True,
                "enable_strategy_planning": True,
                "enable_multi_retriever": True,
                "enable_reranking": True
            },

            # è°ƒè¯•è®¾ç½®
            "debug_mode": False,
            "log_level": "INFO"
        }

    def _get_final_config(self):
        """è·å–æœ€ç»ˆé…ç½®"""
        final_config = {}
        final_config.update(self.internal_config)
        final_config.update(self.external_config)
        return final_config

    def _check_final_config(self):
        """æ£€æŸ¥å’Œä¿®æ­£æœ€ç»ˆé…ç½®"""
        # æ£€æŸ¥ split é…ç½®
        split = self.final_config.get("split")
        if split is None:
            split = ["test"]
        if isinstance(split, str):
            split = [split]
        self.final_config["split"] = split

        # æ£€æŸ¥è·¯å¾„é…ç½®
        data_dir = self.final_config.get("data_dir", "./data/benchmarks")
        save_dir = self.final_config.get("save_dir", "./experiments")

        # è®¾ç½®æ•°æ®é›†è·¯å¾„
        dataset_name = self.final_config.get("dataset_name", "natural_questions")
        dataset_path = os.path.join(data_dir, dataset_name)
        self.final_config["dataset_path"] = dataset_path

        # è®¾ç½®ä¿å­˜è·¯å¾„
        save_note = self.final_config.get("save_note", "experiment")
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = os.path.join(save_dir, f"{save_note}_{timestamp}")
        self.final_config["save_path"] = save_path

    def _set_additional_keys(self):
        """è®¾ç½®é¢å¤–çš„é”®å€¼"""
        # è®¾ç½®æ—¶é—´æˆ³
        self.final_config["timestamp"] = datetime.datetime.now().isoformat()

        # è®¾ç½®å®éªŒ ID
        import uuid
        self.final_config["experiment_id"] = str(uuid.uuid4())[:8]

    def _init_device(self):
        """åˆå§‹åŒ–è®¾å¤‡"""
        import torch

        device = self.final_config.get("device", "cuda")
        if device == "cuda" and not torch.cuda.is_available():
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° CPU")
            device = "cpu"

        self.final_config["device"] = device

        # è®¾ç½® GPU ID
        gpu_id = self.final_config.get("gpu_id", "0")
        if device == "cuda":
            os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    def _set_seed(self):
        """è®¾ç½®éšæœºç§å­"""
        seed = self.final_config.get("seed", 2024)

        import random
        import numpy as np
        import torch

        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

    def _prepare_directories(self):
        """å‡†å¤‡ç›®å½•"""
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_path = self.final_config.get("save_path")
        if save_path:
            os.makedirs(save_path, exist_ok=True)

        # åˆ›å»ºæ•°æ®ç›®å½•
        data_dir = self.final_config.get("data_dir")
        if data_dir:
            os.makedirs(data_dir, exist_ok=True)

    def __getitem__(self, key):
        """å­—å…¸å¼è®¿é—®"""
        return self.final_config[key]

    def __setitem__(self, key, value):
        """å­—å…¸å¼è®¾ç½®"""
        self.final_config[key] = value

    def get(self, key, default=None):
        """è·å–é…ç½®å€¼"""
        return self.final_config.get(key, default)

    def update(self, config_dict):
        """æ›´æ–°é…ç½®"""
        self.final_config.update(config_dict)

    def save_config(self, save_path=None):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        if save_path is None:
            save_path = os.path.join(self.final_config["save_path"], "config.yaml")

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.final_config, f, default_flow_style=False, allow_unicode=True)

        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {save_path}")


# ===== ä¾¿æ·å‡½æ•° =====

def create_experiment_config(config_file=None, **kwargs):
    """åˆ›å»ºå®éªŒé…ç½®"""
    return AdaptiveRAGConfig(config_file_path=config_file, config_dict=kwargs)


def save_example_config(save_path="./adaptive_rag_config.yaml"):
    """ä¿å­˜ç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(EXAMPLE_CONFIG_YAML)
    print(f"âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {save_path}")


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç³»ç»Ÿ
    print("ğŸ§ª æµ‹è¯• AdaptiveRAG é…ç½®ç³»ç»Ÿ")

    # åˆ›å»ºé…ç½®
    config = create_experiment_config(
        dataset_name="natural_questions",
        test_sample_num=10,
        debug_mode=True
    )

    print(f"æ•°æ®é›†: {config['dataset_name']}")
    print(f"è®¾å¤‡: {config['device']}")
    print(f"ä¿å­˜è·¯å¾„: {config['save_path']}")

    # ä¿å­˜ç¤ºä¾‹é…ç½®
    save_example_config()


def create_flexrag_integrated_config() -> FlexRAGIntegratedConfig:
    """åˆ›å»º FlexRAG æ·±åº¦é›†æˆé…ç½®"""
    config = FlexRAGIntegratedConfig()

    # è§£ææ£€ç´¢å™¨é…ç½®ä¸­çš„è·¯å¾„
    for retriever_name, retriever_config in config.retriever_configs.items():
        if "flex_config" in retriever_config:
            flex_config = retriever_config["flex_config"]
            if "retriever_path" in flex_config:
                flex_config["retriever_path"] = resolve_path(flex_config["retriever_path"])

    # è§£æç”Ÿæˆå™¨é…ç½®ä¸­çš„è·¯å¾„
    for generator_name, generator_config in config.generator_configs.items():
        if "hf_config" in generator_config:
            hf_config = generator_config["hf_config"]
            if "model_path" in hf_config:
                hf_config["model_path"] = resolve_path(hf_config["model_path"])

    return config


def get_config_for_mode(mode: str = "adaptive"):
    """æ ¹æ®æ¨¡å¼è·å–é…ç½®

    Args:
        mode: é…ç½®æ¨¡å¼
            - "adaptive": åŸå§‹è‡ªé€‚åº”é…ç½®
            - "flexrag": FlexRAG æ·±åº¦é›†æˆé…ç½®
            - "hybrid": æ··åˆé…ç½®
    """
    if mode == "flexrag":
        return create_flexrag_integrated_config()
    elif mode == "hybrid":
        # æ··åˆé…ç½®ï¼šç»“åˆä¸¤ç§é…ç½®çš„ä¼˜ç‚¹
        base_config = create_default_config()
        flexrag_config = create_flexrag_integrated_config()

        # è¿™é‡Œå¯ä»¥å®ç°é…ç½®åˆå¹¶é€»è¾‘
        return flexrag_config  # æš‚æ—¶è¿”å› FlexRAG é…ç½®
    else:
        return create_default_config()


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç³»ç»Ÿ
    print("ğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ")

    # æµ‹è¯•åŸå§‹é…ç½®
    config = create_default_config()
    print("âœ… é»˜è®¤é…ç½®åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯• FlexRAG é›†æˆé…ç½®
    if FLEXRAG_AVAILABLE:
        flexrag_config = create_flexrag_integrated_config()
        print("âœ… FlexRAG é›†æˆé…ç½®åˆ›å»ºæˆåŠŸ")
    else:
        print("âš ï¸ FlexRAG æœªå®‰è£…ï¼Œè·³è¿‡é›†æˆé…ç½®æµ‹è¯•")

    # æµ‹è¯•é…ç½®æ¨¡å¼é€‰æ‹©
    for mode in ["adaptive", "flexrag", "hybrid"]:
        test_config = get_config_for_mode(mode)
        print(f"âœ… {mode} æ¨¡å¼é…ç½®åˆ›å»ºæˆåŠŸ")

    # ä¿å­˜ç¤ºä¾‹é…ç½®
    with open("adaptive_rag_config.yaml", "w") as f:
        f.write(EXAMPLE_CONFIG_YAML)
    print("âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²ä¿å­˜")


# ===== æ¨¡å—åŒ–é…ç½®åŠ è½½å‡½æ•° =====

def load_modular_config_from_yaml(yaml_path: str) -> FlexRAGIntegratedConfig:
    """ä»æ¨¡å—åŒ–YAMLé…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        yaml_config = yaml.safe_load(f)

    config = FlexRAGIntegratedConfig()

    # åŠ è½½æ¨¡å—å¼€å…³é…ç½®
    if 'modules' in yaml_config:
        modules_config = yaml_config['modules']
        config.modules = ModuleToggleConfig(**modules_config)

    # åŠ è½½åŸºç¡€é…ç½®
    if 'basic' in yaml_config:
        basic_config = yaml_config['basic']
        config.device = basic_config.get('device', config.device)
        config.batch_size = basic_config.get('batch_size', config.batch_size)
        config.max_input_length = basic_config.get('max_input_length', config.max_input_length)

    return config


def apply_preset_config(config: FlexRAGIntegratedConfig, preset_name: str, yaml_config: dict) -> FlexRAGIntegratedConfig:
    """åº”ç”¨é¢„è®¾é…ç½®æ¨¡å¼"""
    if 'presets' in yaml_config and preset_name in yaml_config['presets']:
        preset = yaml_config['presets'][preset_name]

        if 'modules' in preset:
            # æ›´æ–°æ¨¡å—å¼€å…³é…ç½®
            for module_name, enabled in preset['modules'].items():
                if hasattr(config.modules, module_name):
                    setattr(config.modules, module_name, enabled)

    return config


def create_config_from_yaml(yaml_path: str, preset: str = None) -> FlexRAGIntegratedConfig:
    """ä» YAML æ–‡ä»¶åˆ›å»ºé…ç½®ï¼Œæ”¯æŒé¢„è®¾æ¨¡å¼"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        yaml_config = yaml.safe_load(f)

    # å¦‚æœæ˜¯æ¨¡å—åŒ–é…ç½®æ–‡ä»¶
    if 'modules' in yaml_config:
        config = load_modular_config_from_yaml(yaml_path)

        # åº”ç”¨é¢„è®¾é…ç½®
        if preset:
            config = apply_preset_config(config, preset, yaml_config)

        return config

    # å…¼å®¹æ—§ç‰ˆé…ç½®æ–‡ä»¶
    config = FlexRAGIntegratedConfig()
    return config


def get_enabled_modules(config: FlexRAGIntegratedConfig) -> Dict[str, bool]:
    """è·å–å¯ç”¨çš„æ¨¡å—åˆ—è¡¨"""
    if hasattr(config, 'modules'):
        return {
            name: getattr(config.modules, name)
            for name in dir(config.modules)
            if not name.startswith('_')
        }
    return {}


def print_module_status(config: FlexRAGIntegratedConfig):
    """æ‰“å°æ¨¡å—å¯ç”¨çŠ¶æ€"""
    enabled_modules = get_enabled_modules(config)

    print("ğŸ”§ AdaptiveRAG æ¨¡å—çŠ¶æ€:")
    print("=" * 50)

    categories = {
        "æ ¸å¿ƒå¤„ç†æ¨¡å—": ["task_decomposer", "retrieval_planner", "multi_retriever", "context_reranker", "adaptive_generator"],
        "æ™ºèƒ½åˆ†ææ¨¡å—": ["query_analyzer", "strategy_router", "performance_optimizer", "intelligent_strategy_learner"],
        "æ£€ç´¢å™¨æ¨¡å—": ["keyword_retriever", "dense_retriever", "web_retriever", "hybrid_retriever"],
        "é‡æ’åºæ¨¡å—": ["cross_encoder_ranker", "colbert_ranker", "gpt_ranker"],
        "ç”Ÿæˆå™¨æ¨¡å—": ["template_generator", "freeform_generator", "dialogue_generator"],
        "è¯„ä¼°æ¨¡å—": ["fact_verification", "confidence_estimation", "result_analyzer"],
        "ç¼“å­˜æ¨¡å—": ["semantic_cache", "predictive_cache"],
        "ç”¨æˆ·ä½“éªŒæ¨¡å—": ["personalization", "multimodal_support"],
        "è°ƒè¯•ç›‘æ§æ¨¡å—": ["debug_mode", "performance_monitoring", "logging_enhanced"]
    }

    for category, modules in categories.items():
        print(f"\nğŸ“‚ {category}:")
        for module in modules:
            if module in enabled_modules:
                status = "âœ… å¯ç”¨" if enabled_modules[module] else "âŒ ç¦ç”¨"
                print(f"  {module}: {status}")
