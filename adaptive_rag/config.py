#!/usr/bin/env python3
"""
=== æ™ºèƒ½è‡ªé€‚åº” RAG é…ç½®æ¨¡å— ===

æ·±åº¦é›†æˆ FlexRAG ç»„ä»¶çš„é…ç½®ç³»ç»Ÿ
"""

import os
import yaml
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field

# å¯¼å…¥ FlexRAG ç»„ä»¶é…ç½®
try:
    from flexrag.retriever import RetrieverConfig, RETRIEVERS
    from flexrag.ranker import RankerConfig, RANKERS
    from flexrag.models import GeneratorConfig, EncoderConfig, GENERATORS, ENCODERS
    FLEXRAG_AVAILABLE = True
except ImportError:
    print("âš ï¸ FlexRAG æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–é…ç½®")
    FLEXRAG_AVAILABLE = False


@dataclass
class SubTaskConfig:
    """å­ä»»åŠ¡é…ç½®"""
    type: str = "factual"  # factual, semantic, temporal, comparative
    priority: float = 1.0
    max_decompose_depth: int = 3
    enable_entity_extraction: bool = True
    enable_temporal_analysis: bool = True


@dataclass
class RetrievalPlanConfig:
    """æ£€ç´¢ç­–ç•¥é…ç½®"""
    # æ£€ç´¢å™¨æƒé‡é…ç½®
    default_weights: Dict[str, float] = field(default_factory=lambda: {
        "keyword": 0.33,
        "dense": 0.33, 
        "web": 0.34
    })
    
    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"keyword": 0.7, "dense": 0.2, "web": 0.1},
        "semantic": {"keyword": 0.2, "dense": 0.7, "web": 0.1},
        "temporal": {"keyword": 0.3, "dense": 0.2, "web": 0.5},
        "comparative": {"keyword": 0.4, "dense": 0.4, "web": 0.2}
    })
    
    # æ¯ä¸ªæ£€ç´¢å™¨çš„ top-k é…ç½®
    top_k_config: Dict[str, int] = field(default_factory=lambda: {
        "keyword": 10,
        "dense": 10,
        "web": 5
    })


@dataclass
class RelevanceScoreConfig:
    """ç›¸å…³åº¦è¯„åˆ†é…ç½®"""
    # è¯„åˆ†ç»´åº¦æƒé‡
    score_weights: Dict[str, float] = field(default_factory=lambda: {
        "semantic": 0.3,
        "factual": 0.3,
        "temporal": 0.1,
        "entity": 0.2,
        "diversity": 0.1
    })
    
    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_score_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"semantic": 0.3, "factual": 0.4, "temporal": 0.1, "entity": 0.15, "diversity": 0.05},
        "semantic": {"semantic": 0.5, "factual": 0.2, "temporal": 0.1, "entity": 0.1, "diversity": 0.1},
        "temporal": {"semantic": 0.2, "factual": 0.2, "temporal": 0.4, "entity": 0.1, "diversity": 0.1},
        "comparative": {"semantic": 0.25, "factual": 0.25, "temporal": 0.1, "entity": 0.2, "diversity": 0.2}
    })


@dataclass
class ContextAggregationConfig:
    """ä¸Šä¸‹æ–‡èšåˆé…ç½®"""
    max_primary_contexts: int = 3
    max_supporting_contexts: int = 5
    max_background_contexts: int = 2
    
    # ç›¸å…³åº¦é˜ˆå€¼
    primary_threshold: float = 0.8
    supporting_threshold: float = 0.5
    
    # å»é‡é…ç½®
    similarity_threshold: float = 0.85
    enable_conflict_resolution: bool = True


# é¦–å…ˆå®šä¹‰åŸºç¡€é…ç½®ç±»ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
@dataclass
class AdaptiveRAGConfig:
    """æ™ºèƒ½è‡ªé€‚åº” RAG æ€»é…ç½®ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰"""

    # åŸºç¡€é…ç½®
    device: str = "cuda"
    batch_size: int = 4
    max_input_length: int = 2048

    # æ¨¡å‹è·¯å¾„é…ç½® - ä½¿ç”¨ adaptive_rag å†…éƒ¨ç›¸å¯¹è·¯å¾„
    model_paths: Dict[str, str] = field(default_factory=lambda: {
        "keyword_retriever": "bm25",
        "dense_retriever": "./adaptive_rag/models/e5-base-v2",
        "generator": "./adaptive_rag/models/qwen1.5-1.8b",
        "reranker": "./adaptive_rag/models/bge-reranker-base"
    })

    # æ•°æ®è·¯å¾„é…ç½® - ä½¿ç”¨ adaptive_rag å†…éƒ¨æ•°æ®
    data_paths: Dict[str, str] = field(default_factory=lambda: {
        "corpus_path": "./adaptive_rag/data/general_knowledge.jsonl",
        "index_path": "./adaptive_rag/data/e5_Flat.index"
    })

    # ä»»åŠ¡ç±»å‹ç‰¹å®šæƒé‡
    task_specific_weights: Dict[str, Dict[str, float]] = field(default_factory=lambda: {
        "factual": {"keyword": 0.7, "dense": 0.2, "web": 0.1},
        "semantic": {"keyword": 0.2, "dense": 0.7, "web": 0.1},
        "temporal": {"keyword": 0.3, "dense": 0.2, "web": 0.5},
        "comparative": {"keyword": 0.4, "dense": 0.4, "web": 0.2}
    })

    # ç”Ÿæˆé…ç½®
    generation_params: Dict[str, Any] = field(default_factory=lambda: {
        "do_sample": True,
        "max_tokens": 256,
        "temperature": 0.7,
        "top_p": 0.9
    })

    # è¯„ä¼°é…ç½®
    metrics: List[str] = field(default_factory=lambda: ["em", "f1", "acc"])

    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    save_intermediate_results: bool = True
    log_level: str = "INFO"


@dataclass
class FlexRAGIntegratedConfig:
    """æ·±åº¦é›†æˆ FlexRAG çš„é…ç½®ç±»"""

    # åŸºç¡€é…ç½®
    device: str = "cuda"
    batch_size: int = 4
    max_input_length: int = 2048

    # FlexRAG æ£€ç´¢å™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸»è¦ä½¿ç”¨æ¨¡æ‹Ÿå®ç°ï¼‰
    retriever_configs: Dict[str, Any] = field(default_factory=lambda: {
        "keyword_retriever": {
            "retriever_type": "mock",
            "config": {
                "retriever_path": "./adaptive_rag/data/keyword_index"
            }
        },
        "dense_retriever": {
            "retriever_type": "mock",
            "config": {
                "retriever_path": "./adaptive_rag/data/dense_index"
            }
        },
        "web_retriever": {
            "retriever_type": "mock",
            "config": {
                "search_engine": "google"
            }
        }
    })

    # FlexRAG é‡æ’åºå™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    ranker_configs: Dict[str, Any] = field(default_factory=lambda: {
        "cross_encoder": {
            "ranker_type": "mock",
            "config": {
                "model_name": "BAAI/bge-reranker-base",
                "reserve_num": 10
            }
        },
        "colbert": {
            "ranker_type": "mock",
            "config": {
                "model_name": "colbert-ir/colbertv2.0",
                "reserve_num": 10
            }
        }
    })

    # FlexRAG ç”Ÿæˆå™¨é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    generator_configs: Dict[str, Any] = field(default_factory=lambda: {
        "main_generator": {
            "generator_type": "mock",
            "config": {
                "model_path": "./adaptive_rag/models/qwen1.5-1.8b",
                "model_type": "causal_lm"
            }
        },
        "openai_generator": {
            "generator_type": "mock",
            "config": {
                "model_name": "gpt-3.5-turbo",
                "api_key": "${OPENAI_API_KEY}"
            }
        }
    })

    # FlexRAG ç¼–ç å™¨é…ç½®
    encoder_configs: Dict[str, Any] = field(default_factory=lambda: {
        "dense_encoder": {
            "encoder_type": "sentence_transformer",
            "sentence_transformer_config": {
                "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                "device": "cuda"
            }
        },
        "openai_encoder": {
            "encoder_type": "openai",
            "openai_config": {
                "model_name": "text-embedding-ada-002",
                "api_key": "${OPENAI_API_KEY}"
            }
        }
    })
    
    # å­æ¨¡å—é…ç½®
    subtask_config: SubTaskConfig = field(default_factory=SubTaskConfig)
    retrieval_plan_config: RetrievalPlanConfig = field(default_factory=RetrievalPlanConfig)
    relevance_score_config: RelevanceScoreConfig = field(default_factory=RelevanceScoreConfig)
    context_aggregation_config: ContextAggregationConfig = field(default_factory=ContextAggregationConfig)
    
    # ç”Ÿæˆé…ç½®
    generation_params: Dict[str, Any] = field(default_factory=lambda: {
        "do_sample": True,
        "max_tokens": 256,
        "temperature": 0.7,
        "top_p": 0.9
    })
    
    # è¯„ä¼°é…ç½®
    metrics: List[str] = field(default_factory=lambda: ["em", "f1", "acc"])
    
    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    save_intermediate_results: bool = True
    log_level: str = "INFO"


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: Optional[str] = None, config_dict: Optional[Dict] = None):
        self.config = AdaptiveRAGConfig()
        
        # ä»æ–‡ä»¶åŠ è½½é…ç½®
        if config_file and os.path.exists(config_file):
            self.load_from_file(config_file)
        
        # ä»å­—å…¸æ›´æ–°é…ç½®
        if config_dict:
            self.update_from_dict(config_dict)
    
    def load_from_file(self, config_file: str):
        """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(config_file, 'r', encoding='utf-8') as f:
            file_config = yaml.safe_load(f)
        self.update_from_dict(file_config)
    
    def update_from_dict(self, config_dict: Dict):
        """ä»å­—å…¸æ›´æ–°é…ç½®"""
        for key, value in config_dict.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
    
    def save_to_file(self, config_file: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = self.to_dict()
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        def convert_dataclass(obj):
            if hasattr(obj, '__dataclass_fields__'):
                return {k: convert_dataclass(v) for k, v in obj.__dict__.items()}
            elif isinstance(obj, dict):
                return {k: convert_dataclass(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_dataclass(item) for item in obj]
            else:
                return obj
        
        return convert_dataclass(self.config)
    
    def get_config(self) -> AdaptiveRAGConfig:
        """è·å–é…ç½®å¯¹è±¡"""
        return self.config
    
    def __getitem__(self, key):
        """æ”¯æŒå­—å…¸å¼è®¿é—®"""
        return getattr(self.config, key)
    
    def __setitem__(self, key, value):
        """æ”¯æŒå­—å…¸å¼è®¾ç½®"""
        setattr(self.config, key, value)


def get_project_root() -> str:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_dir = os.path.dirname(os.path.abspath(__file__))  # adaptive_rag ç›®å½•
    return os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•

def resolve_path(path: str) -> str:
    """è§£æè·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
    if os.path.isabs(path):
        return path
    else:
        # ç›¸å¯¹è·¯å¾„åŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼Œè§„èŒƒåŒ–è·¯å¾„
        resolved = os.path.join(get_project_root(), path)
        return os.path.normpath(resolved)

def create_default_config() -> AdaptiveRAGConfig:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    config = AdaptiveRAGConfig()

    # è§£ææ¨¡å‹è·¯å¾„
    for key, path in config.model_paths.items():
        if key != "keyword_retriever":  # bm25 ä¸éœ€è¦è·¯å¾„è§£æ
            config.model_paths[key] = resolve_path(path)

    # è§£ææ•°æ®è·¯å¾„
    for key, path in config.data_paths.items():
        config.data_paths[key] = resolve_path(path)

    return config


def load_config(config_file: Optional[str] = None, config_dict: Optional[Dict] = None) -> AdaptiveRAGConfig:
    """åŠ è½½é…ç½®çš„ä¾¿æ·å‡½æ•°"""
    manager = ConfigManager(config_file, config_dict)
    return manager.get_config()


# ç¤ºä¾‹é…ç½®æ–‡ä»¶å†…å®¹
EXAMPLE_CONFIG_YAML = """
# æ™ºèƒ½è‡ªé€‚åº” RAG é…ç½®ç¤ºä¾‹

# åŸºç¡€é…ç½®
device: "cuda"
batch_size: 4
max_input_length: 2048

# æ¨¡å‹è·¯å¾„
model_paths:
  dense_retriever: "/root/autodl-tmp/models/e5-base-v2"
  generator: "/root/autodl-tmp/models/qwen1.5-1.8b"
  reranker: "/root/autodl-tmp/models/bge-reranker-v2-m3"

# æ£€ç´¢ç­–ç•¥é…ç½®
retrieval_plan_config:
  task_specific_weights:
    factual:
      keyword: 0.7
      dense: 0.2
      web: 0.1
    semantic:
      keyword: 0.2
      dense: 0.7
      web: 0.1

# ç›¸å…³åº¦è¯„åˆ†é…ç½®
relevance_score_config:
  score_weights:
    semantic: 0.3
    factual: 0.3
    temporal: 0.1
    entity: 0.2
    diversity: 0.1

# è°ƒè¯•é…ç½®
debug_mode: true
save_intermediate_results: true
log_level: "DEBUG"
"""


def create_flexrag_integrated_config() -> FlexRAGIntegratedConfig:
    """åˆ›å»º FlexRAG æ·±åº¦é›†æˆé…ç½®"""
    config = FlexRAGIntegratedConfig()

    # è§£ææ£€ç´¢å™¨é…ç½®ä¸­çš„è·¯å¾„
    for retriever_name, retriever_config in config.retriever_configs.items():
        if "flex_config" in retriever_config:
            flex_config = retriever_config["flex_config"]
            if "retriever_path" in flex_config:
                flex_config["retriever_path"] = resolve_path(flex_config["retriever_path"])

    # è§£æç”Ÿæˆå™¨é…ç½®ä¸­çš„è·¯å¾„
    for generator_name, generator_config in config.generator_configs.items():
        if "hf_config" in generator_config:
            hf_config = generator_config["hf_config"]
            if "model_path" in hf_config:
                hf_config["model_path"] = resolve_path(hf_config["model_path"])

    return config


def get_config_for_mode(mode: str = "adaptive"):
    """æ ¹æ®æ¨¡å¼è·å–é…ç½®

    Args:
        mode: é…ç½®æ¨¡å¼
            - "adaptive": åŸå§‹è‡ªé€‚åº”é…ç½®
            - "flexrag": FlexRAG æ·±åº¦é›†æˆé…ç½®
            - "hybrid": æ··åˆé…ç½®
    """
    if mode == "flexrag":
        return create_flexrag_integrated_config()
    elif mode == "hybrid":
        # æ··åˆé…ç½®ï¼šç»“åˆä¸¤ç§é…ç½®çš„ä¼˜ç‚¹
        base_config = create_default_config()
        flexrag_config = create_flexrag_integrated_config()

        # è¿™é‡Œå¯ä»¥å®ç°é…ç½®åˆå¹¶é€»è¾‘
        return flexrag_config  # æš‚æ—¶è¿”å› FlexRAG é…ç½®
    else:
        return create_default_config()


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç³»ç»Ÿ
    print("ğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ")

    # æµ‹è¯•åŸå§‹é…ç½®
    config = create_default_config()
    print("âœ… é»˜è®¤é…ç½®åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯• FlexRAG é›†æˆé…ç½®
    if FLEXRAG_AVAILABLE:
        flexrag_config = create_flexrag_integrated_config()
        print("âœ… FlexRAG é›†æˆé…ç½®åˆ›å»ºæˆåŠŸ")
    else:
        print("âš ï¸ FlexRAG æœªå®‰è£…ï¼Œè·³è¿‡é›†æˆé…ç½®æµ‹è¯•")

    # æµ‹è¯•é…ç½®æ¨¡å¼é€‰æ‹©
    for mode in ["adaptive", "flexrag", "hybrid"]:
        test_config = get_config_for_mode(mode)
        print(f"âœ… {mode} æ¨¡å¼é…ç½®åˆ›å»ºæˆåŠŸ")

    # ä¿å­˜ç¤ºä¾‹é…ç½®
    with open("adaptive_rag_config.yaml", "w") as f:
        f.write(EXAMPLE_CONFIG_YAML)
    print("âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²ä¿å­˜")
