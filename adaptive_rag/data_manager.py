#!/usr/bin/env python3
"""
=== æ•°æ®ç®¡ç†å™¨ ===

å€Ÿé‰´ FlashRAG çš„æ•°æ®é›†å¤„ç†å’Œ LightRAG çš„æ–‡æ¡£ç®¡ç†
æä¾›ç»Ÿä¸€çš„æ•°æ®æ¥å£å’Œå¤„ç†æµç¨‹

è®¾è®¡ç†å¿µï¼š
1. å€Ÿé‰´ FlashRAG çš„æ•°æ®é›†æ ‡å‡†åŒ–å¤„ç†
2. å‚è€ƒ LightRAG çš„æ–‡æ¡£åˆ†å—å’Œç´¢å¼•
3. èåˆ GraphRAG çš„ç»“æ„åŒ–æ•°æ®å¤„ç†
4. æ”¯æŒå¤šç§æ•°æ®æºå’Œæ ¼å¼
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Iterator, Union
from dataclasses import dataclass, field
from pathlib import Path
import hashlib

logger = logging.getLogger(__name__)


@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç»“æ„ - å€Ÿé‰´ FlashRAG çš„è®¾è®¡"""
    id: str
    content: str
    title: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = ""
    timestamp: Optional[str] = None
    
    def __post_init__(self):
        if not self.id:
            # è‡ªåŠ¨ç”Ÿæˆ ID
            self.id = self._generate_id()
    
    def _generate_id(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£ ID"""
        content_hash = hashlib.md5(self.content.encode()).hexdigest()
        return f"doc_{content_hash[:8]}"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "content": self.content,
            "title": self.title,
            "metadata": self.metadata,
            "source": self.source,
            "timestamp": self.timestamp
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Document":
        """ä»å­—å…¸åˆ›å»ºæ–‡æ¡£"""
        return cls(
            id=data.get("id", ""),
            content=data.get("content", ""),
            title=data.get("title", ""),
            metadata=data.get("metadata", {}),
            source=data.get("source", ""),
            timestamp=data.get("timestamp")
        )


@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœæ•°æ®ç»“æ„"""
    query: str
    documents: List[Document]
    scores: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query": self.query,
            "documents": [doc.to_dict() for doc in self.documents],
            "scores": self.scores,
            "metadata": self.metadata,
            "processing_time": self.processing_time
        }


class DataManager:
    """æ•°æ®ç®¡ç†å™¨ - å€Ÿé‰´ FlashRAG çš„æ•°æ®å¤„ç†æ¶æ„"""
    
    def __init__(self, config):
        self.config = config
        self.documents: Dict[str, Document] = {}

        # æ•°æ®è·¯å¾„ä¼˜å…ˆçº§ï¼šadaptive_rag å†…éƒ¨æ•°æ® > é¡¹ç›®æ ¹ç›®å½•æ•°æ® > å¤–éƒ¨æ•°æ® > é…ç½®è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))  # adaptive_rag ç›®å½•
        project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•

        # 1. ä¼˜å…ˆä½¿ç”¨ adaptive_rag å†…éƒ¨çš„æ•°æ®
        adaptive_rag_data = os.path.join(current_dir, "data", "general_knowledge.jsonl")
        if os.path.exists(adaptive_rag_data):
            self.corpus_path = adaptive_rag_data
            logger.info(f"âœ… ä½¿ç”¨ adaptive_rag å†…éƒ¨æ•°æ®: {adaptive_rag_data}")

        # 2. ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„æ•°æ®
        elif os.path.exists(os.path.join(project_root, "data", "general_knowledge.jsonl")):
            self.corpus_path = os.path.join(project_root, "data", "general_knowledge.jsonl")
            logger.info(f"ğŸ“‚ ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•æ•°æ®: {self.corpus_path}")

        # 3. å›é€€åˆ°å¤–éƒ¨ FlashRAG æ•°æ®
        elif os.path.exists("/root/autodl-tmp/rag_project/FlashRAG/examples/quick_start/indexes/general_knowledge.jsonl"):
            self.corpus_path = "/root/autodl-tmp/rag_project/FlashRAG/examples/quick_start/indexes/general_knowledge.jsonl"
            logger.info(f"ğŸ”— ä½¿ç”¨å¤–éƒ¨ FlashRAG æ•°æ®: {self.corpus_path}")

        # 4. ä½¿ç”¨ adaptive_rag å†…éƒ¨ç¤ºä¾‹æ•°æ®
        elif os.path.exists(os.path.join(current_dir, "data", "sample_corpus.jsonl")):
            self.corpus_path = os.path.join(current_dir, "data", "sample_corpus.jsonl")
            logger.info(f"ğŸ“„ ä½¿ç”¨ adaptive_rag ç¤ºä¾‹æ•°æ®: {self.corpus_path}")

        # 5. ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ç¤ºä¾‹æ•°æ®
        elif os.path.exists(os.path.join(project_root, "data", "sample_corpus.jsonl")):
            self.corpus_path = os.path.join(project_root, "data", "sample_corpus.jsonl")
            logger.info(f"ğŸ“„ ä½¿ç”¨é¡¹ç›®ç¤ºä¾‹æ•°æ®: {self.corpus_path}")

        # 6. æœ€åå›é€€åˆ°é…ç½®è·¯å¾„
        else:
            config_path = getattr(config, 'corpus_path', './data/general_knowledge.jsonl')
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
            if not os.path.isabs(config_path):
                self.corpus_path = os.path.join(project_root, config_path)
            else:
                self.corpus_path = config_path
            logger.info(f"âš™ï¸ ä½¿ç”¨é…ç½®æ•°æ®è·¯å¾„: {self.corpus_path}")
        
        logger.info("DataManager åˆå§‹åŒ–å®Œæˆ")
    
    def load_corpus(self) -> int:
        """åŠ è½½è¯­æ–™åº“"""
        if not os.path.exists(self.corpus_path):
            logger.warning(f"è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.corpus_path}")
            return 0

        count = 0
        try:
            logger.info(f"å¼€å§‹åŠ è½½è¯­æ–™åº“: {self.corpus_path}")
            with open(self.corpus_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data = json.loads(line.strip())
                            doc = self._parse_document(data)
                            if doc:
                                self.documents[doc.id] = doc
                                count += 1

                                # æ¯1000ä¸ªæ–‡æ¡£æ‰“å°ä¸€æ¬¡è¿›åº¦
                                if count % 1000 == 0:
                                    logger.info(f"å·²åŠ è½½ {count} ä¸ªæ–‡æ¡£...")

                        except json.JSONDecodeError as e:
                            logger.warning(f"ç¬¬ {line_num} è¡Œè§£æ JSON å¤±è´¥: {e}")
                            continue
                        except Exception as e:
                            logger.warning(f"ç¬¬ {line_num} è¡Œå¤„ç†å¤±è´¥: {e}")
                            continue

            logger.info(f"âœ… æˆåŠŸåŠ è½½ {count} ä¸ªæ–‡æ¡£")
            return count

        except Exception as e:
            logger.error(f"åŠ è½½è¯­æ–™åº“å¤±è´¥: {e}")
            return 0
    
    def _parse_document(self, data: Dict[str, Any]) -> Optional[Document]:
        """è§£ææ–‡æ¡£æ•°æ®"""
        try:
            # FlashRAG æ ¼å¼
            if 'contents' in data:
                doc_id = data.get('id', '')
                if not doc_id:
                    # å¦‚æœæ²¡æœ‰ IDï¼Œç”Ÿæˆä¸€ä¸ª
                    import hashlib
                    doc_id = hashlib.md5(data['contents'].encode()).hexdigest()[:8]

                return Document(
                    id=doc_id,
                    content=data['contents'],
                    title=data.get('title', ''),
                    metadata=data.get('metadata', {}),
                    source='flashrag'
                )

            # æ ‡å‡†æ ¼å¼
            elif 'content' in data:
                return Document(
                    id=data.get('id', ''),
                    content=data['content'],
                    title=data.get('title', ''),
                    metadata=data.get('metadata', {}),
                    source=data.get('source', '')
                )

            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
                content_field = None
                for field in ['text', 'body', 'document', 'passage']:
                    if field in data:
                        content_field = field
                        break

                if content_field:
                    doc_id = data.get('id', '')
                    if not doc_id:
                        import hashlib
                        doc_id = hashlib.md5(data[content_field].encode()).hexdigest()[:8]

                    return Document(
                        id=doc_id,
                        content=data[content_field],
                        title=data.get('title', ''),
                        metadata=data.get('metadata', {}),
                        source='auto_detected'
                    )
                else:
                    logger.warning(f"æœªçŸ¥çš„æ–‡æ¡£æ ¼å¼: {list(data.keys())}")
                    return None

        except Exception as e:
            logger.error(f"è§£ææ–‡æ¡£å¤±è´¥: {e}")
            return None
    
    def search_documents(self, query: str, top_k: int = 10) -> List[Document]:
        """æœç´¢æ–‡æ¡£ - ç®€å•çš„å…³é”®è¯åŒ¹é…"""
        if not self.documents:
            logger.warning("æ²¡æœ‰åŠ è½½çš„æ–‡æ¡£")
            return []
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        query_lower = query.lower()
        scored_docs = []
        
        for doc in self.documents.values():
            score = 0.0
            content_lower = doc.content.lower()
            title_lower = doc.title.lower()
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            for word in query_lower.split():
                if word in content_lower:
                    score += content_lower.count(word) * 1.0
                if word in title_lower:
                    score += title_lower.count(word) * 2.0  # æ ‡é¢˜æƒé‡æ›´é«˜
            
            if score > 0:
                scored_docs.append((doc, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›å‰ top_k ä¸ªæ–‡æ¡£
        return [doc for doc, score in scored_docs[:top_k]]
    
    def get_document(self, doc_id: str) -> Optional[Document]:
        """è·å–æŒ‡å®šæ–‡æ¡£"""
        return self.documents.get(doc_id)
    
    def add_document(self, document: Document) -> bool:
        """æ·»åŠ æ–‡æ¡£"""
        try:
            self.documents[document.id] = document
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.get_corpus_stats()

    def get_corpus_stats(self) -> Dict[str, Any]:
        """è·å–è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯"""
        total_docs = len(self.documents)
        total_chars = sum(len(doc.content) for doc in self.documents.values())
        avg_length = total_chars / total_docs if total_docs > 0 else 0
        
        sources = {}
        for doc in self.documents.values():
            source = doc.source or "unknown"
            sources[source] = sources.get(source, 0) + 1
        
        return {
            "total_documents": total_docs,
            "total_characters": total_chars,
            "average_length": avg_length,
            "sources": sources,
            "corpus_path": self.corpus_path
        }
    
    def export_documents(self, output_path: str, format: str = "jsonl") -> bool:
        """å¯¼å‡ºæ–‡æ¡£"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                if format == "jsonl":
                    for doc in self.documents.values():
                        f.write(json.dumps(doc.to_dict(), ensure_ascii=False) + '\n')
                elif format == "json":
                    docs_list = [doc.to_dict() for doc in self.documents.values()]
                    json.dump(docs_list, f, ensure_ascii=False, indent=2)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            
            logger.info(f"æˆåŠŸå¯¼å‡º {len(self.documents)} ä¸ªæ–‡æ¡£åˆ° {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ–‡æ¡£å¤±è´¥: {e}")
            return False


def create_sample_corpus(output_path: str, num_docs: int = 100):
    """åˆ›å»ºç¤ºä¾‹è¯­æ–™åº“"""
    sample_docs = [
        {
            "id": f"sample_{i}",
            "content": f"è¿™æ˜¯ç¬¬ {i} ä¸ªç¤ºä¾‹æ–‡æ¡£ã€‚å®ƒåŒ…å«äº†å…³äºäººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å†…å®¹ã€‚",
            "title": f"ç¤ºä¾‹æ–‡æ¡£ {i}",
            "metadata": {"category": "AI", "length": "short"},
            "source": "sample"
        }
        for i in range(num_docs)
    ]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for doc in sample_docs:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    print(f"åˆ›å»ºäº†åŒ…å« {num_docs} ä¸ªæ–‡æ¡£çš„ç¤ºä¾‹è¯­æ–™åº“: {output_path}")


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®ç®¡ç†å™¨
    from config import create_default_config
    
    config = create_default_config()
    data_manager = DataManager(config)
    
    # åŠ è½½æ•°æ®
    count = data_manager.load_corpus()
    print(f"åŠ è½½äº† {count} ä¸ªæ–‡æ¡£")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = data_manager.get_statistics()
    print("ç»Ÿè®¡ä¿¡æ¯:", stats)
    
    # æµ‹è¯•æœç´¢
    results = data_manager.search_documents("machine learning", top_k=3)
    print(f"æœç´¢ç»“æœ: {len(results)} ä¸ªæ–‡æ¡£")
