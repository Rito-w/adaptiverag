#!/usr/bin/env python3
"""
=== AdaptiveRAG åŸºå‡†æµ‹è¯•è¿è¡Œå™¨ ===

å€Ÿé‰´ FlashRAG çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸º AdaptiveRAG è®¾è®¡çš„ç»¼åˆè¯„ä¼°ç³»ç»Ÿ
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

import pandas as pd
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    method_name: str
    dataset_name: str
    exact_match: float
    f1_score: float
    rouge_l: float
    bert_score: float
    avg_retrieval_time: float
    avg_generation_time: float
    avg_total_time: float
    memory_usage_mb: float
    num_samples: int
    timestamp: str


@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    datasets: List[str]
    methods: List[str]
    output_dir: str
    max_samples: Optional[int] = None
    save_predictions: bool = True
    compute_bert_score: bool = True

    def get(self, key: str, default=None):
        """å­—å…¸å¼è®¿é—®æ–¹æ³•"""
        return getattr(self, key, default)


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ - å€Ÿé‰´ FlashRAG çš„æ•°æ®é›†æ¥å£"""
    
    SUPPORTED_DATASETS = {
        # å•è·³é—®ç­”
        "natural_questions": "nq",
        "trivia_qa": "trivia",
        "ms_marco": "msmarco",
        
        # å¤šè·³æ¨ç†
        "hotpot_qa": "hotpot",
        "2wiki_multihop": "2wiki",
        "musique": "musique",
        
        # å¯¹è¯é—®ç­”
        "quac": "quac",
        "coqa": "coqa",
        
        # å¼€æ”¾åŸŸé—®ç­”
        "web_questions": "webq",
        "entity_questions": "entityq"
    }
    
    def __init__(self, data_dir: str = "./data/benchmarks"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def load_dataset(self, dataset_name: str, split: str = "test") -> List[Dict[str, Any]]:
        """åŠ è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.SUPPORTED_DATASETS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
        
        dataset_file = self.data_dir / f"{dataset_name}_{split}.jsonl"
        
        if not dataset_file.exists():
            logger.warning(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
            return self._create_mock_dataset(dataset_name)
        
        data = []
        with open(dataset_file, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line.strip()))
        
        logger.info(f"åŠ è½½æ•°æ®é›† {dataset_name}: {len(data)} ä¸ªæ ·æœ¬")
        return data
    
    def _create_mock_dataset(self, dataset_name: str) -> List[Dict[str, Any]]:
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºæµ‹è¯•"""
        logger.info(f"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†: {dataset_name}")
        
        if "hotpot" in dataset_name:
            # å¤šè·³æ¨ç†æ ·æœ¬
            return [
                {
                    "id": f"mock_hotpot_{i}",
                    "question": f"What is the connection between entity A and entity B in context {i}?",
                    "answer": f"The connection is through intermediate entity C{i}.",
                    "supporting_facts": [f"fact_{i}_1", f"fact_{i}_2"],
                    "type": "multi_hop"
                }
                for i in range(50)
            ]
        elif "natural" in dataset_name:
            # å•è·³é—®ç­”æ ·æœ¬
            return [
                {
                    "id": f"mock_nq_{i}",
                    "question": f"What is the capital of country {i}?",
                    "answer": f"Capital{i}",
                    "type": "single_hop"
                }
                for i in range(50)
            ]
        else:
            # é€šç”¨æ ·æœ¬
            return [
                {
                    "id": f"mock_{dataset_name}_{i}",
                    "question": f"Sample question {i} for {dataset_name}",
                    "answer": f"Sample answer {i}",
                    "type": "general"
                }
                for i in range(50)
            ]


class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨ - é›†æˆ FlashRAG çš„è¯„ä¼°æŒ‡æ ‡"""

    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
        # å°è¯•å¯¼å…¥ FlashRAG è¯„ä¼°æŒ‡æ ‡
        self.flashrag_available = False
        try:
            from flashrag.evaluator.metrics import EM, F1_Score, Rouge_L
            self.flashrag_em = EM({})
            self.flashrag_f1 = F1_Score({})
            self.flashrag_rouge = Rouge_L({})
            self.flashrag_available = True
            logger.info("âœ… FlashRAG è¯„ä¼°æŒ‡æ ‡å¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ FlashRAG è¯„ä¼°æŒ‡æ ‡ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…ç½®å®ç°")

        # å°è¯•å¯¼å…¥ BERTScore
        self.bertscore_available = False
        try:
            from bert_score import score
            self.bertscore_available = True
            logger.info("âœ… BERTScore å¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ BERTScore ä¸å¯ç”¨ï¼Œä½¿ç”¨è¿‘ä¼¼å®ç°")
    
    def calculate_exact_match(self, predictions: List[str], references: List[str]) -> float:
        """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡"""
        if self.flashrag_available:
            try:
                # ä½¿ç”¨ FlashRAG çš„ EM è®¡ç®—
                mock_data = self._create_mock_data(predictions, references)
                result, _ = self.flashrag_em.calculate_metric(mock_data)
                return result.get('exact_match', 0.0)
            except Exception as e:
                logger.warning(f"FlashRAG EM è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®å®ç°: {e}")

        # å†…ç½®å®ç°
        if len(predictions) != len(references):
            raise ValueError("é¢„æµ‹å’Œå‚è€ƒç­”æ¡ˆæ•°é‡ä¸åŒ¹é…")

        exact_matches = 0
        for pred, ref in zip(predictions, references):
            ref_list = [ref] if isinstance(ref, str) else ref
            if any(self._normalize_answer(pred) == self._normalize_answer(r) for r in ref_list):
                exact_matches += 1
        return exact_matches / len(predictions)

    def _create_mock_data(self, predictions: List[str], references: List[str]):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç»“æ„ç”¨äº FlashRAG è¯„ä¼°"""
        mock_data = []
        for pred, ref in zip(predictions, references):
            mock_item = type('MockItem', (), {
                'pred': pred,
                'golden_answers': [ref] if isinstance(ref, str) else ref
            })()
            mock_data.append(mock_item)

        return type('MockDataset', (), {'data': mock_data})()

    def _normalize_answer(self, answer: str) -> str:
        """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬ - å€Ÿé‰´ FlashRAG çš„æ ‡å‡†åŒ–æ–¹æ³•"""
        import re
        import string

        # è½¬å°å†™
        answer = answer.lower()

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        answer = answer.translate(str.maketrans('', '', string.punctuation))

        # ç§»é™¤å† è¯
        answer = re.sub(r'\b(a|an|the)\b', ' ', answer)

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        answer = re.sub(r'\s+', ' ', answer).strip()

        return answer

    def calculate_f1_score(self, predictions: List[str], references: List[str]) -> float:
        """è®¡ç®— F1 åˆ†æ•°"""
        if self.flashrag_available:
            try:
                # ä½¿ç”¨ FlashRAG çš„ F1 è®¡ç®—
                mock_data = self._create_mock_data(predictions, references)
                result, _ = self.flashrag_f1.calculate_metric(mock_data)
                return result.get('f1_score', 0.0)
            except Exception as e:
                logger.warning(f"FlashRAG F1 è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®å®ç°: {e}")

        # å†…ç½®å®ç°
        total_f1 = 0.0

        for pred, ref in zip(predictions, references):
            pred_tokens = set(pred.lower().split())
            ref_tokens = set(ref.lower().split())

            if len(pred_tokens) == 0 and len(ref_tokens) == 0:
                f1 = 1.0
            elif len(pred_tokens) == 0 or len(ref_tokens) == 0:
                f1 = 0.0
            else:
                common = len(pred_tokens & ref_tokens)
                precision = common / len(pred_tokens)
                recall = common / len(ref_tokens)
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

            total_f1 += f1

        return total_f1 / len(predictions)
    
    def calculate_rouge_l(self, predictions: List[str], references: List[str]) -> float:
        """è®¡ç®— ROUGE-L åˆ†æ•°"""
        if self.flashrag_available:
            try:
                # ä½¿ç”¨ FlashRAG çš„ ROUGE-L è®¡ç®—
                mock_data = self._create_mock_data(predictions, references)
                result, _ = self.flashrag_rouge.calculate_metric(mock_data)
                return result.get('rouge-l', 0.0)
            except Exception as e:
                logger.warning(f"FlashRAG ROUGE-L è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®å®ç°: {e}")

        # å†…ç½®å®ç° - ç®€åŒ–çš„ ROUGE-L è®¡ç®—
        total_rouge = 0.0

        for pred, ref in zip(predictions, references):
            pred_words = pred.lower().split()
            ref_words = ref.lower().split()

            # ç®€åŒ–çš„æœ€é•¿å…¬å…±å­åºåˆ—
            lcs_length = self._lcs_length(pred_words, ref_words)

            if len(ref_words) == 0:
                rouge_l = 0.0
            else:
                recall = lcs_length / len(ref_words)
                precision = lcs_length / len(pred_words) if len(pred_words) > 0 else 0.0
                rouge_l = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

            total_rouge += rouge_l

        return total_rouge / len(predictions)
    
    def _lcs_length(self, seq1: List[str], seq2: List[str]) -> int:
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    def calculate_bert_score(self, predictions: List[str], references: List[str]) -> float:
        """è®¡ç®— BERTScore"""
        if self.bertscore_available:
            try:
                from bert_score import score
                # ä½¿ç”¨çœŸå®çš„ BERTScore
                P, R, F1 = score(predictions, references, lang="en", verbose=False)
                return F1.mean().item()
            except Exception as e:
                logger.warning(f"BERTScore è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨è¿‘ä¼¼å®ç°: {e}")

        # è¿‘ä¼¼å®ç° - åŸºäºè¯æ±‡é‡å 
        total_score = 0.0

        for pred, ref in zip(predictions, references):
            pred_tokens = set(pred.lower().split())
            ref_tokens = set(ref.lower().split())

            if len(ref_tokens) == 0:
                score = 0.0
            else:
                overlap = len(pred_tokens & ref_tokens)
                score = overlap / max(len(pred_tokens), len(ref_tokens))

            total_score += score

        return total_score / len(predictions)


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨ - é›†æˆ FlashRAG æ•°æ®é›†å’Œè¯„ä¼°"""

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.dataset_loader = DatasetLoader()
        self.metrics_calculator = MetricsCalculator()
        self.results = []

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ•°æ®é›†ä¸‹è½½å™¨
        from .dataset_downloader import DatasetDownloader
        self.dataset_downloader = DatasetDownloader(str(self.dataset_loader.data_dir))
    
    def run_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œ AdaptiveRAG åŸºå‡†æµ‹è¯•")
        logger.info(f"æ•°æ®é›†: {self.config.datasets}")
        logger.info(f"æ–¹æ³•: {self.config.methods}")
        
        for dataset_name in self.config.datasets:
            for method_name in self.config.methods:
                logger.info(f"\nğŸ“Š è¯„ä¼° {method_name} åœ¨ {dataset_name} ä¸Šçš„æ€§èƒ½")
                
                try:
                    result = self._evaluate_method_on_dataset(method_name, dataset_name)
                    self.results.append(result)
                    logger.info(f"âœ… å®Œæˆè¯„ä¼°: EM={result.exact_match:.3f}, F1={result.f1_score:.3f}")
                    
                except Exception as e:
                    logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
                    continue
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        logger.info(f"ğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")
    
    def _evaluate_method_on_dataset(self, method_name: str, dataset_name: str) -> EvaluationResult:
        """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¯„ä¼°æŒ‡å®šæ–¹æ³•"""
        # å°è¯•ä» FlashRAG åŠ è½½æ•°æ®é›†
        dataset = self._load_dataset_with_fallback(dataset_name)

        if self.config.max_samples:
            dataset = dataset[:self.config.max_samples]

        # åˆå§‹åŒ–æ–¹æ³•
        method = self._load_method(method_name)

        # è¿è¡Œè¯„ä¼°
        predictions = []
        retrieval_times = []
        generation_times = []
        total_times = []

        for sample in tqdm(dataset, desc=f"è¯„ä¼° {method_name}"):
            start_time = time.time()

            # è¿è¡Œæ–¹æ³•
            if hasattr(method, 'answer'):
                # FlexRAG é›†æˆåŠ©æ‰‹ä½¿ç”¨ answer æ–¹æ³•
                assistant_result = method.answer(sample["question"])
                result = {
                    "answer": assistant_result.answer,
                    "retrieval_time": sum(r.retrieval_time for r in assistant_result.retrieval_results),
                    "generation_time": assistant_result.generation_result.generation_time if assistant_result.generation_result else 0.0
                }
            else:
                # å…¶ä»–æ–¹æ³•ä½¿ç”¨ process_query
                result = method.process_query(sample["question"])

            end_time = time.time()

            predictions.append(result["answer"])
            retrieval_times.append(result.get("retrieval_time", 0.0))
            generation_times.append(result.get("generation_time", 0.0))
            total_times.append(end_time - start_time)

        # è®¡ç®—æŒ‡æ ‡
        references = [sample["answer"] for sample in dataset]

        exact_match = self.metrics_calculator.calculate_exact_match(predictions, references)
        f1_score = self.metrics_calculator.calculate_f1_score(predictions, references)
        rouge_l = self.metrics_calculator.calculate_rouge_l(predictions, references)

        bert_score = 0.0
        if self.config.compute_bert_score:
            bert_score = self.metrics_calculator.calculate_bert_score(predictions, references)

        # åˆ›å»ºç»“æœ
        result = EvaluationResult(
            method_name=method_name,
            dataset_name=dataset_name,
            exact_match=exact_match,
            f1_score=f1_score,
            rouge_l=rouge_l,
            bert_score=bert_score,
            avg_retrieval_time=sum(retrieval_times) / len(retrieval_times),
            avg_generation_time=sum(generation_times) / len(generation_times),
            avg_total_time=sum(total_times) / len(total_times),
            memory_usage_mb=0.0,  # TODO: å®ç°å†…å­˜ç›‘æ§
            num_samples=len(dataset),
            timestamp=datetime.now().isoformat()
        )

        # ä¿å­˜é¢„æµ‹ç»“æœ
        if self.config.save_predictions:
            self._save_predictions(method_name, dataset_name, predictions, references)

        return result

    def _load_dataset_with_fallback(self, dataset_name: str) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®é›†ï¼Œä¼˜å…ˆä½¿ç”¨ FlashRAGï¼Œå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°æ•°æ®"""
        # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°æ•°æ®é›†
        try:
            dataset = self.dataset_loader.load_dataset(dataset_name)
            if dataset:
                logger.info(f"âœ… ä»æœ¬åœ°åŠ è½½æ•°æ®é›†: {dataset_name}")
                return dataset
        except Exception as e:
            logger.warning(f"æœ¬åœ°æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")

        # å°è¯•ä» FlashRAG ä¸‹è½½æ•°æ®é›†
        logger.info(f"ğŸ“¥ å°è¯•ä» FlashRAG ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        if self.dataset_downloader.download_flashrag_dataset(dataset_name, "test"):
            try:
                dataset = self.dataset_loader.load_dataset(dataset_name)
                if dataset:
                    logger.info(f"âœ… ä» FlashRAG åŠ è½½æ•°æ®é›†: {dataset_name}")
                    return dataset
            except Exception as e:
                logger.error(f"FlashRAG æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")

        # æœ€åä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†
        logger.warning(f"âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†: {dataset_name}")
        return self.dataset_loader._create_mock_dataset(dataset_name)
        
        # åˆå§‹åŒ–æ–¹æ³•
        method = self._load_method(method_name)
        
        # è¿è¡Œè¯„ä¼°
        predictions = []
        retrieval_times = []
        generation_times = []
        total_times = []
        
        for sample in tqdm(dataset, desc=f"è¯„ä¼° {method_name}"):
            start_time = time.time()
            
            # è¿è¡Œæ–¹æ³•
            if hasattr(method, 'answer'):
                # FlexRAG é›†æˆåŠ©æ‰‹ä½¿ç”¨ answer æ–¹æ³•
                assistant_result = method.answer(sample["question"])
                result = {
                    "answer": assistant_result.answer,
                    "retrieval_time": sum(r.retrieval_time for r in assistant_result.retrieval_results),
                    "generation_time": assistant_result.generation_result.generation_time if assistant_result.generation_result else 0.0
                }
            else:
                # å…¶ä»–æ–¹æ³•ä½¿ç”¨ process_query
                result = method.process_query(sample["question"])
            
            end_time = time.time()
            
            predictions.append(result["answer"])
            retrieval_times.append(result.get("retrieval_time", 0.0))
            generation_times.append(result.get("generation_time", 0.0))
            total_times.append(end_time - start_time)
        
        # è®¡ç®—æŒ‡æ ‡
        references = [sample["answer"] for sample in dataset]
        
        exact_match = self.metrics_calculator.calculate_exact_match(predictions, references)
        f1_score = self.metrics_calculator.calculate_f1_score(predictions, references)
        rouge_l = self.metrics_calculator.calculate_rouge_l(predictions, references)
        
        bert_score = 0.0
        if self.config.compute_bert_score:
            bert_score = self.metrics_calculator.calculate_bert_score(predictions, references)
        
        # åˆ›å»ºç»“æœ
        result = EvaluationResult(
            method_name=method_name,
            dataset_name=dataset_name,
            exact_match=exact_match,
            f1_score=f1_score,
            rouge_l=rouge_l,
            bert_score=bert_score,
            avg_retrieval_time=sum(retrieval_times) / len(retrieval_times),
            avg_generation_time=sum(generation_times) / len(generation_times),
            avg_total_time=sum(total_times) / len(total_times),
            memory_usage_mb=0.0,  # TODO: å®ç°å†…å­˜ç›‘æ§
            num_samples=len(dataset),
            timestamp=datetime.now().isoformat()
        )
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if self.config.save_predictions:
            self._save_predictions(method_name, dataset_name, predictions, references)
        
        return result
    
    def _load_method(self, method_name: str):
        """åŠ è½½æŒ‡å®šçš„æ–¹æ³•"""
        if method_name == "adaptive_rag":
            # åŠ è½½ AdaptiveRAG æ–¹æ³•
            try:
                from ..core.flexrag_integrated_assistant import FlexRAGIntegratedAssistant
                from ..config import create_flexrag_integrated_config

                config = create_flexrag_integrated_config()
                return FlexRAGIntegratedAssistant(config)
            except ImportError as e:
                logger.error(f"âŒ æ— æ³•åŠ è½½ AdaptiveRAG: {e}")
                raise

        elif method_name in ["naive_rag", "self_rag", "raptor"]:
            # åŠ è½½åŸºçº¿æ–¹æ³•
            from .baseline_methods import create_baseline_method

            # åˆ›å»ºåŸºçº¿æ–¹æ³•é…ç½®
            baseline_config = {
                "retrieval_topk": self.config.get("retrieval_topk", 5),
                "max_tokens": self.config.get("max_tokens", 256),
                "max_iterations": 3,
                "tree_depth": 3,
                "reflection_threshold": 0.7,
                "cluster_size": 5
            }

            return create_baseline_method(method_name, baseline_config)

        elif method_name.startswith("adaptive_rag_"):
            # æ¶ˆèç ”ç©¶æ–¹æ³•
            return self._load_ablation_method(method_name)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method_name}")

    def _load_ablation_method(self, method_name: str):
        """åŠ è½½æ¶ˆèç ”ç©¶æ–¹æ³•"""
        # è§£ææ¶ˆèæ–¹æ³•åç§°
        ablation_type = method_name.replace("adaptive_rag_", "")

        try:
            from ..core.flexrag_integrated_assistant import FlexRAGIntegratedAssistant
            from ..config import create_flexrag_integrated_config

            # åˆ›å»ºå¸¦æœ‰æ¶ˆèè®¾ç½®çš„é…ç½®
            config = create_flexrag_integrated_config()

            # æ ¹æ®æ¶ˆèç±»å‹ä¿®æ”¹é…ç½®
            if ablation_type == "no_decomposition":
                config.adaptive_retrieval["enable_task_decomposition"] = False
            elif ablation_type == "no_planning":
                config.adaptive_retrieval["enable_strategy_planning"] = False
            elif ablation_type == "single_retriever":
                config.adaptive_retrieval["enable_multi_retriever"] = False
            elif ablation_type == "no_reranking":
                config.adaptive_retrieval["enable_reranking"] = False

            return FlexRAGIntegratedAssistant(config)

        except ImportError as e:
            logger.error(f"âŒ æ— æ³•åŠ è½½æ¶ˆèæ–¹æ³• {method_name}: {e}")
            raise

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method_name}")
    
    def _create_mock_method(self, method_name: str):
        """åˆ›å»ºæ¨¡æ‹Ÿæ–¹æ³•ç”¨äºæµ‹è¯•"""
        class MockMethod:
            def __init__(self, name):
                self.name = name

            def process_query(self, query: str) -> Dict[str, Any]:
                # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                time.sleep(0.1)

                return {
                    "answer": f"Mock answer from {self.name} for: {query[:50]}...",
                    "retrieval_time": 0.05,
                    "generation_time": 0.05
                }

        return MockMethod(method_name)
    
    def _save_predictions(self, method_name: str, dataset_name: str, 
                         predictions: List[str], references: List[str]):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        predictions_file = self.output_dir / f"{method_name}_{dataset_name}_predictions.jsonl"
        
        with open(predictions_file, 'w', encoding='utf-8') as f:
            for i, (pred, ref) in enumerate(zip(predictions, references)):
                json.dump({
                    "id": i,
                    "prediction": pred,
                    "reference": ref
                }, f, ensure_ascii=False)
                f.write('\n')
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ")

        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = [result for result in self.results if result is not None]

        if not valid_results:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆç»“æœå¯ä¿å­˜")
            return

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.output_dir / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(result) for result in valid_results], f,
                     ensure_ascii=False, indent=2)

        # ä¿å­˜æ±‡æ€»è¡¨æ ¼
        df = pd.DataFrame([asdict(result) for result in valid_results])
        summary_file = self.output_dir / "evaluation_summary.csv"
        df.to_csv(summary_file, index=False)
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
        self._create_performance_table()
    
    def _create_performance_table(self):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨"""
        df = pd.DataFrame([asdict(result) for result in self.results])
        
        # æŒ‰æ–¹æ³•å’Œæ•°æ®é›†åˆ†ç»„
        pivot_table = df.pivot_table(
            index='method_name',
            columns='dataset_name',
            values=['exact_match', 'f1_score', 'avg_total_time'],
            aggfunc='mean'
        )
        
        # ä¿å­˜å¯¹æ¯”è¡¨
        comparison_file = self.output_dir / "method_comparison.csv"
        pivot_table.to_csv(comparison_file)
        
        logger.info(f"æ€§èƒ½å¯¹æ¯”è¡¨ä¿å­˜åœ¨: {comparison_file}")


def main():
    """ä¸»å‡½æ•° - è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    config = BenchmarkConfig(
        datasets=["natural_questions", "hotpot_qa", "trivia_qa"],
        methods=["adaptive_rag", "naive_rag", "self_rag"],
        output_dir="./evaluation_results",
        max_samples=100,  # æµ‹è¯•æ—¶ä½¿ç”¨å°æ ·æœ¬
        save_predictions=True,
        compute_bert_score=True
    )
    
    runner = BenchmarkRunner(config)
    runner.run_benchmark()


if __name__ == "__main__":
    main()
