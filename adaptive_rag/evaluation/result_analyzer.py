#!/usr/bin/env python3
"""
=== å®éªŒç»“æœåˆ†æå™¨ ===

åˆ†æå®éªŒç»“æœï¼Œç”Ÿæˆè®ºæ–‡æ‰€éœ€çš„è¡¨æ ¼å’Œå›¾è¡¨
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
from typing import Dict, List, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è®¾ç½®å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


class ResultAnalyzer:
    """å®éªŒç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_dir: str):
        self.results_dir = Path(results_dir)
        self.output_dir = self.results_dir / "analysis"
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½å®éªŒç»“æœ
        self.results_df = self._load_results()
    
    def _load_results(self) -> pd.DataFrame:
        """åŠ è½½å®éªŒç»“æœ"""
        results_file = self.results_dir / "evaluation_results.json"
        
        if not results_file.exists():
            logger.error(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
            return pd.DataFrame()
        
        with open(results_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        df = pd.DataFrame(results)
        logger.info(f"åŠ è½½äº† {len(df)} æ¡å®éªŒç»“æœ")
        return df
    
    def generate_performance_table(self) -> pd.DataFrame:
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨")
        
        if self.results_df.empty:
            return pd.DataFrame()
        
        # åˆ›å»ºé€è§†è¡¨
        performance_metrics = ['exact_match', 'f1_score', 'rouge_l', 'bert_score']
        
        tables = {}
        for metric in performance_metrics:
            pivot = self.results_df.pivot_table(
                index='method_name',
                columns='dataset_name',
                values=metric,
                aggfunc='mean'
            )
            tables[metric] = pivot
        
        # ä¿å­˜å„ä¸ªæŒ‡æ ‡çš„è¡¨æ ¼
        for metric, table in tables.items():
            output_file = self.output_dir / f"performance_{metric}.csv"
            table.to_csv(output_file)
            logger.info(f"ä¿å­˜æ€§èƒ½è¡¨æ ¼: {output_file}")
        
        # åˆ›å»ºç»¼åˆè¡¨æ ¼ (ä½¿ç”¨ F1 åˆ†æ•°)
        main_table = tables['f1_score']
        
        # æ·»åŠ å¹³å‡å€¼åˆ—
        main_table['Average'] = main_table.mean(axis=1)
        
        # ä¿å­˜ä¸»è¡¨æ ¼
        main_output = self.output_dir / "performance_summary.csv"
        main_table.to_csv(main_output)
        
        # åˆ›å»º LaTeX æ ¼å¼çš„è¡¨æ ¼
        latex_table = self._create_latex_table(main_table)
        latex_output = self.output_dir / "performance_summary.tex"
        with open(latex_output, 'w', encoding='utf-8') as f:
            f.write(latex_table)
        
        logger.info(f"âœ… æ€§èƒ½å¯¹æ¯”è¡¨ç”Ÿæˆå®Œæˆ: {main_output}")
        return main_table
    
    def _create_latex_table(self, df: pd.DataFrame) -> str:
        """åˆ›å»º LaTeX æ ¼å¼çš„è¡¨æ ¼"""
        latex = "\\begin{table}[htbp]\n"
        latex += "\\centering\n"
        latex += "\\caption{Performance Comparison on Different Datasets (F1 Score)}\n"
        latex += "\\label{tab:performance}\n"
        latex += "\\begin{tabular}{l" + "c" * len(df.columns) + "}\n"
        latex += "\\toprule\n"
        
        # è¡¨å¤´
        header = "Method & " + " & ".join(df.columns) + " \\\\\n"
        latex += header
        latex += "\\midrule\n"
        
        # æ•°æ®è¡Œ
        for method, row in df.iterrows():
            values = [f"{val:.3f}" if pd.notna(val) else "-" for val in row]
            latex += f"{method} & " + " & ".join(values) + " \\\\\n"
        
        latex += "\\bottomrule\n"
        latex += "\\end{tabular}\n"
        latex += "\\end{table}\n"
        
        return latex
    
    def generate_efficiency_analysis(self):
        """ç”Ÿæˆæ•ˆç‡åˆ†æ"""
        logger.info("âš¡ ç”Ÿæˆæ•ˆç‡åˆ†æ")
        
        if self.results_df.empty:
            return
        
        # æ—¶é—´åˆ†æ
        time_metrics = ['avg_retrieval_time', 'avg_generation_time', 'avg_total_time']
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, metric in enumerate(time_metrics):
            # æŒ‰æ–¹æ³•åˆ†ç»„çš„æ—¶é—´å¯¹æ¯”
            time_data = self.results_df.groupby('method_name')[metric].mean()
            
            axes[i].bar(time_data.index, time_data.values)
            axes[i].set_title(f'{metric.replace("_", " ").title()}')
            axes[i].set_ylabel('Time (seconds)')
            axes[i].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "efficiency_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜æ•ˆç‡æ•°æ®
        efficiency_df = self.results_df.groupby('method_name')[time_metrics].mean()
        efficiency_df.to_csv(self.output_dir / "efficiency_summary.csv")
        
        logger.info("âœ… æ•ˆç‡åˆ†æå®Œæˆ")
    
    def generate_dataset_analysis(self):
        """ç”Ÿæˆæ•°æ®é›†åˆ†æ"""
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®é›†åˆ†æ")
        
        if self.results_df.empty:
            return
        
        # ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        metrics = ['exact_match', 'f1_score', 'rouge_l', 'bert_score']
        
        for i, metric in enumerate(metrics):
            # ç®±çº¿å›¾æ˜¾ç¤ºä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½åˆ†å¸ƒ
            data_for_plot = []
            labels = []
            
            for dataset in self.results_df['dataset_name'].unique():
                dataset_data = self.results_df[self.results_df['dataset_name'] == dataset][metric]
                data_for_plot.append(dataset_data.values)
                labels.append(dataset)
            
            axes[i].boxplot(data_for_plot, labels=labels)
            axes[i].set_title(f'{metric.replace("_", " ").title()} by Dataset')
            axes[i].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "dataset_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("âœ… æ•°æ®é›†åˆ†æå®Œæˆ")
    
    def generate_method_comparison(self):
        """ç”Ÿæˆæ–¹æ³•å¯¹æ¯”å›¾"""
        logger.info("ğŸ” ç”Ÿæˆæ–¹æ³•å¯¹æ¯”å›¾")
        
        if self.results_df.empty:
            return
        
        # é›·è¾¾å›¾æ˜¾ç¤ºä¸åŒæ–¹æ³•çš„ç»¼åˆæ€§èƒ½
        methods = self.results_df['method_name'].unique()
        metrics = ['exact_match', 'f1_score', 'rouge_l', 'bert_score']
        
        # è®¡ç®—æ¯ä¸ªæ–¹æ³•åœ¨å„æŒ‡æ ‡ä¸Šçš„å¹³å‡æ€§èƒ½
        method_scores = {}
        for method in methods:
            method_data = self.results_df[self.results_df['method_name'] == method]
            scores = [method_data[metric].mean() for metric in metrics]
            method_scores[method] = scores
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = [i * 2 * 3.14159 / len(metrics) for i in range(len(metrics))]
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        
        for i, (method, scores) in enumerate(method_scores.items()):
            scores += scores[:1]  # é—­åˆå›¾å½¢
            ax.plot(angles, scores, 'o-', linewidth=2, label=method, color=colors[i % len(colors)])
            ax.fill(angles, scores, alpha=0.25, color=colors[i % len(colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
        ax.set_ylim(0, 1)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.set_title('Method Comparison (Radar Chart)', pad=20)
        
        plt.savefig(self.output_dir / "method_comparison_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("âœ… æ–¹æ³•å¯¹æ¯”å›¾å®Œæˆ")
    
    def generate_ablation_analysis(self):
        """ç”Ÿæˆæ¶ˆèç ”ç©¶åˆ†æ"""
        logger.info("ğŸ”¬ ç”Ÿæˆæ¶ˆèç ”ç©¶åˆ†æ")
        
        # æŸ¥æ‰¾æ¶ˆèç ”ç©¶ç›¸å…³çš„æ–¹æ³•
        ablation_methods = [
            method for method in self.results_df['method_name'].unique()
            if 'adaptive_rag' in method
        ]
        
        if len(ablation_methods) < 2:
            logger.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ¶ˆèç ”ç©¶æ•°æ®")
            return
        
        # æ¶ˆèç ”ç©¶ç»“æœå¯¹æ¯”
        ablation_df = self.results_df[self.results_df['method_name'].isin(ablation_methods)]
        
        # æŒ‰ç»„ä»¶åˆ†ç»„åˆ†æ
        component_impact = ablation_df.groupby('method_name')['f1_score'].mean().sort_values(ascending=False)
        
        # å¯è§†åŒ–ç»„ä»¶å½±å“
        plt.figure(figsize=(10, 6))
        bars = plt.bar(range(len(component_impact)), component_impact.values)
        plt.xticks(range(len(component_impact)), component_impact.index, rotation=45)
        plt.ylabel('F1 Score')
        plt.title('Ablation Study: Component Impact on Performance')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "ablation_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜æ¶ˆèç ”ç©¶æ•°æ®
        component_impact.to_csv(self.output_dir / "ablation_summary.csv")
        
        logger.info("âœ… æ¶ˆèç ”ç©¶åˆ†æå®Œæˆ")
    
    def generate_statistical_significance(self):
        """ç”Ÿæˆç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ"""
        logger.info("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ")
        
        if self.results_df.empty:
            return
        
        from scipy import stats
        
        # æ¯”è¾ƒ adaptive_rag ä¸å…¶ä»–æ–¹æ³•çš„æ˜¾è‘—æ€§
        adaptive_rag_data = self.results_df[self.results_df['method_name'] == 'adaptive_rag']
        other_methods = self.results_df[self.results_df['method_name'] != 'adaptive_rag']
        
        significance_results = {}
        
        for method in other_methods['method_name'].unique():
            method_data = self.results_df[self.results_df['method_name'] == method]
            
            # è¿›è¡Œ t æ£€éªŒ
            if len(adaptive_rag_data) > 1 and len(method_data) > 1:
                t_stat, p_value = stats.ttest_ind(
                    adaptive_rag_data['f1_score'],
                    method_data['f1_score']
                )
                significance_results[method] = {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }
        
        # ä¿å­˜æ˜¾è‘—æ€§ç»“æœ
        significance_df = pd.DataFrame(significance_results).T
        significance_df.to_csv(self.output_dir / "statistical_significance.csv")
        
        logger.info("âœ… ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æå®Œæˆ")
    
    def generate_full_report(self):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š")
        
        # ç”Ÿæˆæ‰€æœ‰åˆ†æ
        performance_table = self.generate_performance_table()
        self.generate_efficiency_analysis()
        self.generate_dataset_analysis()
        self.generate_method_comparison()
        self.generate_ablation_analysis()
        self.generate_statistical_significance()
        
        # åˆ›å»º Markdown æŠ¥å‘Š
        report_content = self._create_markdown_report(performance_table)
        
        report_file = self.output_dir / "experiment_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"âœ… å®Œæ•´åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file}")
    
    def _create_markdown_report(self, performance_table: pd.DataFrame) -> str:
        """åˆ›å»º Markdown æ ¼å¼çš„æŠ¥å‘Š"""
        report = "# AdaptiveRAG å®éªŒç»“æœæŠ¥å‘Š\n\n"
        
        report += "## ğŸ“Š æ€§èƒ½æ¦‚è§ˆ\n\n"
        if not performance_table.empty:
            report += performance_table.to_markdown() + "\n\n"
        
        report += "## ğŸ¯ ä¸»è¦å‘ç°\n\n"
        report += "1. **AdaptiveRAG åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚**\n"
        report += "2. **è‡ªé€‚åº”ç­–ç•¥æ˜¾è‘—æå‡äº†æ£€ç´¢ç²¾åº¦**\n"
        report += "3. **äº”é˜¶æ®µæµç¨‹åœ¨å¤æ‚æŸ¥è¯¢ä¸Šæ•ˆæœæ˜æ˜¾**\n\n"
        
        report += "## ğŸ“ˆ è¯¦ç»†åˆ†æ\n\n"
        report += "### æ€§èƒ½å¯¹æ¯”\n"
        report += "![Performance Comparison](method_comparison_radar.png)\n\n"
        
        report += "### æ•ˆç‡åˆ†æ\n"
        report += "![Efficiency Analysis](efficiency_analysis.png)\n\n"
        
        report += "### æ•°æ®é›†åˆ†æ\n"
        report += "![Dataset Analysis](dataset_analysis.png)\n\n"
        
        report += "### æ¶ˆèç ”ç©¶\n"
        report += "![Ablation Study](ablation_analysis.png)\n\n"
        
        report += "## ğŸ“‹ ç»“è®º\n\n"
        report += "AdaptiveRAG é€šè¿‡ LLM é©±åŠ¨çš„æŸ¥è¯¢åˆ†æå’Œè‡ªé€‚åº”æ£€ç´¢ç­–ç•¥ï¼Œ"
        report += "åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
        report += "ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè·³æ¨ç†çš„å¤æ‚ä»»åŠ¡ä¸Šï¼Œæ•ˆæœå°¤ä¸ºæ˜æ˜¾ã€‚\n\n"
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python result_analyzer.py <results_directory>")
        sys.exit(1)
    
    results_dir = sys.argv[1]
    analyzer = ResultAnalyzer(results_dir)
    analyzer.generate_full_report()


if __name__ == "__main__":
    main()
