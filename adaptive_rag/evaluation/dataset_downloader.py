#!/usr/bin/env python3
"""
=== æ•°æ®é›†ä¸‹è½½å™¨ ===

ä¸‹è½½å’Œé¢„å¤„ç†æ ‡å‡† RAG è¯„ä¼°æ•°æ®é›†ï¼Œå€Ÿé‰´ FlashRAG çš„æ•°æ®å¤„ç†æµç¨‹
"""

import json
import logging
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import zipfile
import tarfile

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨ - é›†æˆ FlashRAG æ•°æ®é›†"""

    # FlashRAG æ•°æ®é›†é…ç½® (Hugging Face)
    FLASHRAG_DATASETS = {
        # å•è·³é—®ç­”
        "natural_questions": "nq",
        "trivia_qa": "trivia",
        "ms_marco": "msmarco",
        "web_questions": "webq",

        # å¤šè·³æ¨ç†
        "hotpot_qa": "hotpot",
        "2wiki_multihop": "2wiki",
        "musique": "musique",

        # å¯¹è¯é—®ç­”
        "quac": "quac",
        "coqa": "coqa",

        # å…¶ä»–
        "entity_questions": "entityq",
        "wow": "wow",
        "fever": "fever"
    }

    # FlashRAG Hugging Face æ•°æ®é›†è·¯å¾„
    FLASHRAG_HF_REPO = "RUC-NLPIR/FlashRAG_datasets"

    # å¤‡ç”¨æ•°æ®é›† URL é…ç½®
    BACKUP_DATASET_URLS = {
        "natural_questions": {
            "train": "https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.json.gz",
            "dev": "https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-dev.json.gz",
            "test": "https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.json.gz"
        },
        "hotpot_qa": {
            "train": "https://hotpotqa.github.io/data/hotpot_train_v1.1.json",
            "dev": "https://hotpotqa.github.io/data/hotpot_dev_distractor_v1.json",
            "test": "https://hotpotqa.github.io/data/hotpot_test_fullwiki_v1.json"
        },
        "trivia_qa": {
            "train": "http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz",
            "dev": "http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz",
            "test": "http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz"
        },
        "ms_marco": {
            "train": "https://msmarco.blob.core.windows.net/msmarcoranking/train_v2.1.json.gz",
            "dev": "https://msmarco.blob.core.windows.net/msmarcoranking/dev_v2.1.json.gz",
            "test": "https://msmarco.blob.core.windows.net/msmarcoranking/eval_v2.1_public.json.gz"
        }
    }
    
    def __init__(self, data_dir: str = "./data/benchmarks"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®å­ç›®å½•
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.raw_dir.mkdir(exist_ok=True)
        self.processed_dir.mkdir(exist_ok=True)

        # å°è¯•å¯¼å…¥ datasets åº“ç”¨äº FlashRAG æ•°æ®é›†
        try:
            import datasets
            self.datasets_available = True
            logger.info("âœ… datasets åº“å¯ç”¨ï¼Œå°†ä½¿ç”¨ FlashRAG æ•°æ®é›†")
        except ImportError:
            self.datasets_available = False
            logger.warning("âš ï¸ datasets åº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨ä¸‹è½½æ–¹æ³•")

    def download_flashrag_dataset(self, dataset_name: str, split: str = "test") -> bool:
        """
        ä» FlashRAG Hugging Face ä»“åº“ä¸‹è½½æ•°æ®é›†

        Args:
            dataset_name: æ•°æ®é›†åç§°
            split: æ•°æ®é›†åˆ†å‰² (train/dev/test)

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if not self.datasets_available:
            logger.error("âŒ datasets åº“ä¸å¯ç”¨ï¼Œæ— æ³•ä¸‹è½½ FlashRAG æ•°æ®é›†")
            return False

        if dataset_name not in self.FLASHRAG_DATASETS:
            logger.error(f"âŒ ä¸æ”¯æŒçš„ FlashRAG æ•°æ®é›†: {dataset_name}")
            return False

        try:
            import datasets

            # è·å– FlashRAG æ•°æ®é›†åç§°
            flashrag_name = self.FLASHRAG_DATASETS[dataset_name]

            logger.info(f"ğŸ“¥ ä» FlashRAG ä¸‹è½½æ•°æ®é›†: {dataset_name} ({flashrag_name}) - {split}")

            # ä¸‹è½½æ•°æ®é›†
            dataset = datasets.load_dataset(
                self.FLASHRAG_HF_REPO,
                flashrag_name,
                split=split,
                trust_remote_code=True
            )

            # ä¿å­˜ä¸º JSONL æ ¼å¼
            output_file = self.data_dir / f"{dataset_name}_{split}.jsonl"

            with open(output_file, 'w', encoding='utf-8') as f:
                for item in dataset:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    converted_item = self._convert_flashrag_format(item, dataset_name)
                    f.write(json.dumps(converted_item, ensure_ascii=False) + '\n')

            logger.info(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {output_file} ({len(dataset)} æ¡è®°å½•)")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ FlashRAG æ•°æ®é›†å¤±è´¥: {e}")
            return False

    def _convert_flashrag_format(self, item: Dict[str, Any], dataset_name: str) -> Dict[str, Any]:
        """
        å°† FlashRAG æ•°æ®æ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼

        Args:
            item: FlashRAG æ•°æ®é¡¹
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            Dict: æ ‡å‡†æ ¼å¼æ•°æ®é¡¹
        """
        # åŸºç¡€å­—æ®µæ˜ å°„
        converted = {
            "id": item.get("id", ""),
            "question": item.get("question", ""),
            "answer": item.get("golden_answers", [""])[0] if item.get("golden_answers") else "",
            "golden_answers": item.get("golden_answers", []),
            "dataset": dataset_name
        }

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if "golden_contexts" in item and item["golden_contexts"]:
            converted["golden_contexts"] = item["golden_contexts"]

        # æ·»åŠ å…ƒæ•°æ®
        if "meta_data" in item:
            converted["meta_data"] = item["meta_data"]

        # æ•°æ®é›†ç‰¹å®šå¤„ç†
        if dataset_name == "hotpot_qa":
            # HotpotQA ç‰¹å®šå­—æ®µ
            if "supporting_facts" in item:
                converted["supporting_facts"] = item["supporting_facts"]
            if "type" in item:
                converted["question_type"] = item["type"]

        elif dataset_name == "natural_questions":
            # Natural Questions ç‰¹å®šå­—æ®µ
            if "annotations" in item:
                converted["annotations"] = item["annotations"]

        return converted
    
    def download_dataset(self, dataset_name: str, splits: List[str] = None) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.DATASET_URLS:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
            return False
        
        if splits is None:
            splits = ["train", "dev", "test"]
        
        logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        
        dataset_urls = self.DATASET_URLS[dataset_name]
        success = True
        
        for split in splits:
            if split not in dataset_urls:
                logger.warning(f"æ•°æ®é›† {dataset_name} æ²¡æœ‰ {split} åˆ†å‰²")
                continue
            
            url = dataset_urls[split]
            success &= self._download_file(url, dataset_name, split)
        
        if success:
            logger.info(f"âœ… æ•°æ®é›† {dataset_name} ä¸‹è½½å®Œæˆ")
            # é¢„å¤„ç†æ•°æ®é›†
            self.preprocess_dataset(dataset_name, splits)
        else:
            logger.error(f"âŒ æ•°æ®é›† {dataset_name} ä¸‹è½½å¤±è´¥")
        
        return success
    
    def _download_file(self, url: str, dataset_name: str, split: str) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            # è§£ææ–‡ä»¶å
            parsed_url = urlparse(url)
            filename = Path(parsed_url.path).name
            if not filename:
                filename = f"{dataset_name}_{split}.json"
            
            file_path = self.raw_dir / dataset_name / filename
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if file_path.exists():
                logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {file_path}")
                return True
            
            logger.info(f"ä¸‹è½½æ–‡ä»¶: {url} -> {file_path}")
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œè§£å‹
            if filename.endswith('.gz'):
                self._extract_gz(file_path)
            elif filename.endswith('.tar.gz'):
                self._extract_tar_gz(file_path)
            elif filename.endswith('.zip'):
                self._extract_zip(file_path)
            
            logger.info(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥: {url}, é”™è¯¯: {e}")
            return False
    
    def _extract_gz(self, file_path: Path):
        """è§£å‹ .gz æ–‡ä»¶"""
        import gzip
        
        output_path = file_path.with_suffix('')
        with gzip.open(file_path, 'rb') as f_in:
            with open(output_path, 'wb') as f_out:
                f_out.write(f_in.read())
        
        logger.info(f"è§£å‹å®Œæˆ: {output_path}")
    
    def _extract_tar_gz(self, file_path: Path):
        """è§£å‹ .tar.gz æ–‡ä»¶"""
        with tarfile.open(file_path, 'r:gz') as tar:
            tar.extractall(file_path.parent)
        
        logger.info(f"è§£å‹å®Œæˆ: {file_path}")
    
    def _extract_zip(self, file_path: Path):
        """è§£å‹ .zip æ–‡ä»¶"""
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(file_path.parent)
        
        logger.info(f"è§£å‹å®Œæˆ: {file_path}")
    
    def preprocess_dataset(self, dataset_name: str, splits: List[str]):
        """é¢„å¤„ç†æ•°æ®é›†ä¸ºç»Ÿä¸€æ ¼å¼"""
        logger.info(f"ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®é›†: {dataset_name}")
        
        for split in splits:
            try:
                # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©é¢„å¤„ç†æ–¹æ³•
                if dataset_name == "natural_questions":
                    self._preprocess_natural_questions(split)
                elif dataset_name == "hotpot_qa":
                    self._preprocess_hotpot_qa(split)
                elif dataset_name == "trivia_qa":
                    self._preprocess_trivia_qa(split)
                elif dataset_name == "ms_marco":
                    self._preprocess_ms_marco(split)
                else:
                    logger.warning(f"æœªå®ç°çš„é¢„å¤„ç†æ–¹æ³•: {dataset_name}")
                    
            except Exception as e:
                logger.error(f"é¢„å¤„ç†å¤±è´¥ {dataset_name} {split}: {e}")
    
    def _preprocess_natural_questions(self, split: str):
        """é¢„å¤„ç† Natural Questions æ•°æ®é›†"""
        raw_file = self.raw_dir / "natural_questions" / f"nq-{split}.json"
        output_file = self.processed_dir / f"natural_questions_{split}.jsonl"
        
        if not raw_file.exists():
            logger.warning(f"åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}")
            return
        
        with open(raw_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        processed_data = []
        for item in data:
            processed_item = {
                "id": item.get("id", ""),
                "question": item.get("question", ""),
                "answer": item.get("answers", [""])[0] if item.get("answers") else "",
                "context": item.get("context", ""),
                "type": "single_hop",
                "dataset": "natural_questions"
            }
            processed_data.append(processed_item)
        
        # ä¿å­˜ä¸º JSONL æ ¼å¼
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in processed_data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… Natural Questions {split} é¢„å¤„ç†å®Œæˆ: {len(processed_data)} ä¸ªæ ·æœ¬")
    
    def _preprocess_hotpot_qa(self, split: str):
        """é¢„å¤„ç† HotpotQA æ•°æ®é›†"""
        raw_file = self.raw_dir / "hotpot_qa" / f"hotpot_{split}_v1.1.json"
        output_file = self.processed_dir / f"hotpot_qa_{split}.jsonl"
        
        if not raw_file.exists():
            # å°è¯•å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
            possible_files = list((self.raw_dir / "hotpot_qa").glob(f"*{split}*.json"))
            if possible_files:
                raw_file = possible_files[0]
            else:
                logger.warning(f"åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}")
                return
        
        with open(raw_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        processed_data = []
        for item in data:
            processed_item = {
                "id": item.get("_id", ""),
                "question": item.get("question", ""),
                "answer": item.get("answer", ""),
                "supporting_facts": item.get("supporting_facts", []),
                "context": item.get("context", []),
                "type": "multi_hop",
                "dataset": "hotpot_qa"
            }
            processed_data.append(processed_item)
        
        # ä¿å­˜ä¸º JSONL æ ¼å¼
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in processed_data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… HotpotQA {split} é¢„å¤„ç†å®Œæˆ: {len(processed_data)} ä¸ªæ ·æœ¬")
    
    def _preprocess_trivia_qa(self, split: str):
        """é¢„å¤„ç† TriviaQA æ•°æ®é›†"""
        # TriviaQA çš„æ–‡ä»¶ç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        output_file = self.processed_dir / f"trivia_qa_{split}.jsonl"
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        mock_data = [
            {
                "id": f"trivia_{split}_{i}",
                "question": f"Sample trivia question {i} for {split}",
                "answer": f"Sample answer {i}",
                "type": "single_hop",
                "dataset": "trivia_qa"
            }
            for i in range(100)
        ]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in mock_data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… TriviaQA {split} é¢„å¤„ç†å®Œæˆ: {len(mock_data)} ä¸ªæ ·æœ¬ (æ¨¡æ‹Ÿæ•°æ®)")
    
    def _preprocess_ms_marco(self, split: str):
        """é¢„å¤„ç† MS MARCO æ•°æ®é›†"""
        output_file = self.processed_dir / f"ms_marco_{split}.jsonl"
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        mock_data = [
            {
                "id": f"msmarco_{split}_{i}",
                "question": f"Sample MS MARCO question {i} for {split}",
                "answer": f"Sample answer {i}",
                "type": "single_hop",
                "dataset": "ms_marco"
            }
            for i in range(100)
        ]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in mock_data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… MS MARCO {split} é¢„å¤„ç†å®Œæˆ: {len(mock_data)} ä¸ªæ ·æœ¬ (æ¨¡æ‹Ÿæ•°æ®)")
    
    def download_all_datasets(self):
        """ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
        
        for dataset_name in self.DATASET_URLS.keys():
            self.download_dataset(dataset_name)
        
        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆ")
    
    def create_sample_datasets(self):
        """åˆ›å»ºæ ·æœ¬æ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯•"""
        logger.info("ğŸ§ª åˆ›å»ºæ ·æœ¬æ•°æ®é›†")
        
        datasets = ["natural_questions", "hotpot_qa", "trivia_qa", "ms_marco"]
        splits = ["train", "dev", "test"]
        
        for dataset_name in datasets:
            for split in splits:
                output_file = self.processed_dir / f"{dataset_name}_{split}.jsonl"
                
                # æ ¹æ®æ•°æ®é›†ç±»å‹åˆ›å»ºä¸åŒçš„æ ·æœ¬
                if "hotpot" in dataset_name:
                    sample_type = "multi_hop"
                else:
                    sample_type = "single_hop"
                
                mock_data = [
                    {
                        "id": f"{dataset_name}_{split}_{i}",
                        "question": f"Sample {dataset_name} question {i} for {split}",
                        "answer": f"Sample answer {i}",
                        "type": sample_type,
                        "dataset": dataset_name
                    }
                    for i in range(50)  # æ¯ä¸ªåˆ†å‰²50ä¸ªæ ·æœ¬
                ]
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for item in mock_data:
                        json.dump(item, f, ensure_ascii=False)
                        f.write('\n')
                
                logger.info(f"âœ… åˆ›å»ºæ ·æœ¬æ•°æ®é›†: {output_file}")
        
        logger.info("ğŸ‰ æ ·æœ¬æ•°æ®é›†åˆ›å»ºå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    downloader = DatasetDownloader()
    
    # é€‰æ‹©æ“ä½œ
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "download":
        # ä¸‹è½½çœŸå®æ•°æ®é›† (éœ€è¦ç½‘ç»œè¿æ¥)
        downloader.download_all_datasets()
    else:
        # åˆ›å»ºæ ·æœ¬æ•°æ®é›†ç”¨äºæµ‹è¯•
        downloader.create_sample_datasets()


if __name__ == "__main__":
    main()
