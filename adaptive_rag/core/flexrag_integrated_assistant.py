#!/usr/bin/env python3
"""
=== FlexRAG æ·±åº¦é›†æˆåŠ©æ‰‹ ===

ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ FlexRAG é›†æˆç»„ä»¶çš„ä¸»åŠ©æ‰‹ç±»
"""

import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)

# å¯¼å…¥é›†æˆç»„ä»¶
from ..modules.retriever.flexrag_integrated_retriever import FlexRAGIntegratedRetriever
from ..modules.refiner.flexrag_integrated_ranker import FlexRAGIntegratedRanker
from ..modules.generator.flexrag_integrated_generator import FlexRAGIntegratedGenerator
from ..task_decomposer import TaskDecomposer
from ..retrieval_planner import RetrievalPlanner

# å¯¼å…¥ç»Ÿä¸€çš„æ•°æ®ç»“æ„
from ..modules.retriever.flexrag_integrated_retriever import RetrievedContext

# æ£€æŸ¥ FlexRAG å¯ç”¨æ€§
try:
    import flexrag
    FLEXRAG_AVAILABLE = True
except ImportError:
    FLEXRAG_AVAILABLE = False


@dataclass
class AdaptiveRAGResult:
    """è‡ªé€‚åº” RAG å®Œæ•´ç»“æœ"""
    query: str
    answer: str
    subtasks: List[Any]
    retrieval_results: List[Any]
    ranking_results: List[Any]
    generation_result: Any
    total_time: float
    metadata: Dict[str, Any] = None


class FlexRAGIntegratedAssistant:
    """
    FlexRAG æ·±åº¦é›†æˆåŠ©æ‰‹
    
    æ•´åˆæ‰€æœ‰ FlexRAG ç»„ä»¶ï¼Œæä¾›å®Œæ•´çš„è‡ªé€‚åº” RAG æµç¨‹ï¼š
    1. ä»»åŠ¡åˆ†è§£ (Task Decomposition)
    2. æ£€ç´¢ç­–ç•¥è§„åˆ’ (Retrieval Planning)  
    3. å¤šæ¨¡æ€æ£€ç´¢ (Multi-modal Retrieval)
    4. æ™ºèƒ½é‡æ’åº (Intelligent Reranking)
    5. è‡ªé€‚åº”ç”Ÿæˆ (Adaptive Generation)
    """
    
    def __init__(self, config):
        self.config = config
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        logger.info("ğŸš€ åˆå§‹åŒ– FlexRAG æ·±åº¦é›†æˆåŠ©æ‰‹...")
        
        # ä»»åŠ¡åˆ†è§£å’Œè§„åˆ’ç»„ä»¶
        self.task_decomposer = TaskDecomposer(config)
        self.retrieval_planner = RetrievalPlanner(config)
        
        # FlexRAG é›†æˆç»„ä»¶
        self.retriever = FlexRAGIntegratedRetriever(config)
        self.ranker = FlexRAGIntegratedRanker(config)
        self.generator = FlexRAGIntegratedGenerator(config)
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_initialized = True
        self.component_status = self._check_component_status()
        
        logger.info("âœ… FlexRAG æ·±åº¦é›†æˆåŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š ç»„ä»¶çŠ¶æ€: {self.component_status}")
    
    def _check_component_status(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ‰€æœ‰ç»„ä»¶çŠ¶æ€"""
        return {
            "flexrag_available": FLEXRAG_AVAILABLE,
            "retriever_info": self.retriever.get_retriever_info(),
            "ranker_info": self.ranker.get_ranker_info(),
            "generator_info": self.generator.get_generator_info(),
            "task_decomposer": "active",
            "retrieval_planner": "active"
        }
    
    def answer(
        self,
        query: str,
        strategy_config: Optional[Dict[str, Any]] = None
    ) -> AdaptiveRAGResult:
        """
        ä¸»è¦çš„é—®ç­”æ–¹æ³• - å®Œæ•´çš„è‡ªé€‚åº” RAG æµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            strategy_config: å¯é€‰çš„ç­–ç•¥é…ç½®
            
        Returns:
            AdaptiveRAGResult: å®Œæ•´çš„ RAG ç»“æœ
        """
        start_time = time.time()
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        
        # ä½¿ç”¨é»˜è®¤ç­–ç•¥é…ç½®
        if strategy_config is None:
            strategy_config = self._get_default_strategy()
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šä»»åŠ¡åˆ†è§£å’Œç­–ç•¥è§„åˆ’ ===
            logger.info("ğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šä»»åŠ¡åˆ†è§£å’Œç­–ç•¥è§„åˆ’")
            
            # 1. ä»»åŠ¡åˆ†è§£
            subtasks = self.task_decomposer.decompose_query(query)
            logger.info(f"   åˆ†è§£ä¸º {len(subtasks)} ä¸ªå­ä»»åŠ¡")
            
            # 2. æ£€ç´¢ç­–ç•¥è§„åˆ’
            retrieval_plans = self.retrieval_planner.plan_retrieval_strategy(subtasks)
            logger.info(f"   ç”Ÿæˆ {len(retrieval_plans)} ä¸ªæ£€ç´¢è®¡åˆ’")
            
            # === ç¬¬äºŒé˜¶æ®µï¼šå¤šæ¨¡æ€æ£€ç´¢ ===
            logger.info("ğŸ” ç¬¬äºŒé˜¶æ®µï¼šå¤šæ¨¡æ€æ£€ç´¢")
            
            retrieval_results = []
            all_contexts = []
            
            for subtask in subtasks:
                plan = retrieval_plans[subtask.id]
                
                # è½¬æ¢è®¡åˆ’ä¸ºæ£€ç´¢ç­–ç•¥
                retrieval_strategy = {
                    "weights": plan.weights,
                    "top_k_per_retriever": plan.top_k_per_retriever,
                    "fusion_method": plan.fusion_method
                }
                
                # æ‰§è¡Œæ£€ç´¢
                result = self.retriever.adaptive_retrieve(
                    query=subtask.content,
                    strategy=retrieval_strategy,
                    top_k=strategy_config.get("retrieval_top_k", 10)
                )
                
                retrieval_results.append(result)
                all_contexts.extend(result.contexts)
                
                logger.info(f"   å­ä»»åŠ¡ '{subtask.content}' æ£€ç´¢åˆ° {len(result.contexts)} ä¸ªæ–‡æ¡£")
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½é‡æ’åº ===
            logger.info("ğŸ¯ ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½é‡æ’åº")
            
            if strategy_config.get("enable_reranking", True) and all_contexts:
                ranking_strategy = strategy_config.get("ranking_strategy", {
                    "ranker": "cross_encoder",
                    "enable_multi_ranker": False,
                    "final_top_k": 10
                })
                
                ranking_result = self.ranker.adaptive_rank(
                    query=query,
                    contexts=all_contexts,
                    strategy=ranking_strategy
                )
                
                final_contexts = ranking_result.ranked_contexts
                ranking_results = [ranking_result]
                
                logger.info(f"   é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆ {len(final_contexts)} ä¸ªæ–‡æ¡£")
            else:
                # ä¸ä½¿ç”¨é‡æ’åºï¼Œç›´æ¥æŒ‰åˆ†æ•°æ’åº
                final_contexts = sorted(all_contexts, key=lambda x: x.score, reverse=True)
                final_contexts = final_contexts[:strategy_config.get("final_context_count", 10)]
                ranking_results = []
                
                logger.info(f"   è·³è¿‡é‡æ’åºï¼Œç›´æ¥ä½¿ç”¨ {len(final_contexts)} ä¸ªæ–‡æ¡£")
            
            # === ç¬¬å››é˜¶æ®µï¼šè‡ªé€‚åº”ç”Ÿæˆ ===
            logger.info("âœ¨ ç¬¬å››é˜¶æ®µï¼šè‡ªé€‚åº”ç”Ÿæˆ")
            
            generation_strategy = strategy_config.get("generation_strategy", {
                "generator": "main_generator",
                "prompt_template": "default",
                "max_tokens": 256,
                "temperature": 0.7
            })
            
            generation_result = self.generator.adaptive_generate(
                query=query,
                contexts=final_contexts,
                strategy=generation_strategy
            )
            
            logger.info(f"   ç”Ÿæˆå®Œæˆï¼Œç­”æ¡ˆé•¿åº¦: {len(generation_result.answer)} å­—ç¬¦")
            
            # === æ„å»ºæœ€ç»ˆç»“æœ ===
            total_time = time.time() - start_time
            
            result = AdaptiveRAGResult(
                query=query,
                answer=generation_result.answer,
                subtasks=subtasks,
                retrieval_results=retrieval_results,
                ranking_results=ranking_results,
                generation_result=generation_result,
                total_time=total_time,
                metadata={
                    "strategy_config": strategy_config,
                    "component_status": self.component_status,
                    "stage_times": {
                        "total": total_time,
                        "generation": generation_result.generation_time,
                        "ranking": ranking_results[0].ranking_time if ranking_results else 0,
                        "retrieval": sum(r.retrieval_time for r in retrieval_results)
                    },
                    "document_counts": {
                        "total_retrieved": len(all_contexts),
                        "final_contexts": len(final_contexts),
                        "subtasks": len(subtasks)
                    }
                }
            )
            
            logger.info(f"ğŸ‰ æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é”™è¯¯ç»“æœ
            return AdaptiveRAGResult(
                query=query,
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                subtasks=[],
                retrieval_results=[],
                ranking_results=[],
                generation_result=None,
                total_time=time.time() - start_time,
                metadata={"error": str(e)}
            )
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤ç­–ç•¥é…ç½®"""
        return {
            "retrieval_top_k": 10,
            "enable_reranking": True,
            "final_context_count": 5,
            "ranking_strategy": {
                "ranker": "cross_encoder",
                "enable_multi_ranker": False,
                "final_top_k": 5
            },
            "generation_strategy": {
                "generator": "main_generator",
                "prompt_template": "default",
                "max_tokens": 256,
                "temperature": 0.7,
                "max_context_length": 2000
            }
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "assistant_type": "FlexRAG Integrated Assistant",
            "flexrag_available": FLEXRAG_AVAILABLE,
            "is_initialized": self.is_initialized,
            "component_status": self.component_status,
            "supported_features": [
                "Task Decomposition",
                "Retrieval Planning", 
                "Multi-modal Retrieval",
                "Intelligent Reranking",
                "Adaptive Generation"
            ]
        }
    
    def quick_answer(self, query: str) -> str:
        """å¿«é€Ÿå›ç­”ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
        result = self.answer(query)
        return result.answer
    
    def explain_process(self, query: str) -> Dict[str, Any]:
        """è§£é‡Šå¤„ç†è¿‡ç¨‹ï¼ˆç”¨äºè°ƒè¯•å’Œå¯è§†åŒ–ï¼‰"""
        result = self.answer(query)
        
        return {
            "query": query,
            "process_explanation": {
                "step1_decomposition": {
                    "description": "å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå­ä»»åŠ¡",
                    "subtasks": [
                        {
                            "content": st.content,
                            "type": st.task_type.value,
                            "priority": st.priority
                        }
                        for st in result.subtasks
                    ]
                },
                "step2_retrieval": {
                    "description": "å¤šæ¨¡æ€æ£€ç´¢ç›¸å…³æ–‡æ¡£",
                    "results": [
                        {
                            "query": r.query,
                            "document_count": len(r.contexts),
                            "retrieval_time": r.retrieval_time
                        }
                        for r in result.retrieval_results
                    ]
                },
                "step3_ranking": {
                    "description": "æ™ºèƒ½é‡æ’åºä¼˜åŒ–ç»“æœ",
                    "enabled": len(result.ranking_results) > 0,
                    "final_count": len(result.ranking_results[0].ranked_contexts) if result.ranking_results else 0
                },
                "step4_generation": {
                    "description": "åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ",
                    "generator_type": result.generation_result.generator_type if result.generation_result else "none",
                    "answer_length": len(result.answer)
                }
            },
            "performance": result.metadata.get("stage_times", {}),
            "final_answer": result.answer
        }


if __name__ == "__main__":
    # æµ‹è¯• FlexRAG é›†æˆåŠ©æ‰‹
    from ..config import FlexRAGIntegratedConfig
    
    config = FlexRAGIntegratedConfig()
    assistant = FlexRAGIntegratedAssistant(config)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "What is the difference between machine learning and deep learning?"
    
    print("ğŸ§ª æµ‹è¯• FlexRAG é›†æˆåŠ©æ‰‹")
    print(f"æŸ¥è¯¢: {test_query}")
    print("=" * 60)
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    system_info = assistant.get_system_info()
    print(f"ç³»ç»Ÿä¿¡æ¯: {system_info}")
    print()
    
    # æ‰§è¡Œå®Œæ•´æµç¨‹
    result = assistant.answer(test_query)
    
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"- æ€»è€—æ—¶: {result.total_time:.3f}s")
    print(f"- å­ä»»åŠ¡æ•°: {len(result.subtasks)}")
    print(f"- æ£€ç´¢ç»“æœæ•°: {sum(len(r.contexts) for r in result.retrieval_results)}")
    print(f"- æœ€ç»ˆç­”æ¡ˆ: {result.answer[:200]}...")
    
    # è§£é‡Šå¤„ç†è¿‡ç¨‹
    explanation = assistant.explain_process(test_query)
    print(f"\nğŸ” å¤„ç†è¿‡ç¨‹è§£é‡Š:")
    for step, details in explanation["process_explanation"].items():
        print(f"- {step}: {details['description']}")
